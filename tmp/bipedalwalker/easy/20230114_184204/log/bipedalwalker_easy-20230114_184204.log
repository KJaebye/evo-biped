[32m[20230114 18:42:04 @logger.py:106][0m Log file set to ./tmp/bipedalwalker/easy/20230114_184204/log/bipedalwalker_easy-20230114_184204.log
[32m[20230114 18:42:04 @agent_ppo2.py:125][0m #------------------------ Iteration 0 --------------------------#
[32m[20230114 18:42:05 @agent_ppo2.py:131][0m Sampling time: 0.44 s by 4 slaves
[32m[20230114 18:42:05 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:42:05 @agent_ppo2.py:189][0m |          -0.0008 |         112.6510 |           1.8742 |
[32m[20230114 18:42:05 @agent_ppo2.py:189][0m |          -0.0019 |         105.3064 |           1.8758 |
[32m[20230114 18:42:05 @agent_ppo2.py:189][0m |          -0.0024 |         101.0952 |           1.8767 |
[32m[20230114 18:42:05 @agent_ppo2.py:189][0m |          -0.0032 |          98.4338 |           1.8782 |
[32m[20230114 18:42:05 @agent_ppo2.py:189][0m |          -0.0030 |          96.6674 |           1.8787 |
[32m[20230114 18:42:05 @agent_ppo2.py:189][0m |          -0.0046 |          95.1259 |           1.8799 |
[32m[20230114 18:42:06 @agent_ppo2.py:189][0m |          -0.0028 |          93.4184 |           1.8814 |
[32m[20230114 18:42:06 @agent_ppo2.py:189][0m |          -0.0052 |          91.5235 |           1.8821 |
[32m[20230114 18:42:06 @agent_ppo2.py:189][0m |          -0.0063 |          89.2934 |           1.8828 |
[32m[20230114 18:42:06 @agent_ppo2.py:189][0m |          -0.0048 |          86.9193 |           1.8838 |
[32m[20230114 18:42:06 @agent_ppo2.py:134][0m Policy update time: 1.28 s
[32m[20230114 18:42:06 @agent_ppo2.py:142][0m Average TRAINING episode reward: -110.00
[32m[20230114 18:42:06 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: -100.39
[32m[20230114 18:42:06 @agent_ppo2.py:144][0m Average EVALUATION episode reward: -92.27
[32m[20230114 18:42:06 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: -92.27
[32m[20230114 18:42:06 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards -92.27
[32m[20230114 18:42:06 @agent_ppo2.py:147][0m Total time:       0.03 min
[32m[20230114 18:42:06 @agent_ppo2.py:149][0m 2048 total steps have happened
[32m[20230114 18:42:06 @agent_ppo2.py:125][0m #------------------------ Iteration 1 --------------------------#
[32m[20230114 18:42:06 @agent_ppo2.py:131][0m Sampling time: 0.36 s by 4 slaves
[32m[20230114 18:42:06 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:42:07 @agent_ppo2.py:189][0m |          -0.0001 |          84.7502 |           1.8808 |
[32m[20230114 18:42:07 @agent_ppo2.py:189][0m |          -0.0014 |          77.5576 |           1.8814 |
[32m[20230114 18:42:07 @agent_ppo2.py:189][0m |          -0.0027 |          74.2840 |           1.8814 |
[32m[20230114 18:42:07 @agent_ppo2.py:189][0m |          -0.0039 |          71.1955 |           1.8810 |
[32m[20230114 18:42:07 @agent_ppo2.py:189][0m |          -0.0050 |          67.9791 |           1.8811 |
[32m[20230114 18:42:07 @agent_ppo2.py:189][0m |          -0.0060 |          64.7816 |           1.8805 |
[32m[20230114 18:42:07 @agent_ppo2.py:189][0m |          -0.0068 |          61.6444 |           1.8798 |
[32m[20230114 18:42:07 @agent_ppo2.py:189][0m |          -0.0074 |          58.6013 |           1.8796 |
[32m[20230114 18:42:07 @agent_ppo2.py:189][0m |          -0.0080 |          55.5441 |           1.8787 |
[32m[20230114 18:42:08 @agent_ppo2.py:189][0m |          -0.0083 |          52.8598 |           1.8777 |
[32m[20230114 18:42:08 @agent_ppo2.py:134][0m Policy update time: 1.20 s
[32m[20230114 18:42:08 @agent_ppo2.py:142][0m Average TRAINING episode reward: -111.38
[32m[20230114 18:42:08 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: -98.02
[32m[20230114 18:42:08 @agent_ppo2.py:144][0m Average EVALUATION episode reward: -92.52
[32m[20230114 18:42:08 @agent_ppo2.py:147][0m Total time:       0.06 min
[32m[20230114 18:42:08 @agent_ppo2.py:149][0m 4096 total steps have happened
[32m[20230114 18:42:08 @agent_ppo2.py:125][0m #------------------------ Iteration 2 --------------------------#
[32m[20230114 18:42:08 @agent_ppo2.py:131][0m Sampling time: 0.34 s by 4 slaves
[32m[20230114 18:42:08 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:42:08 @agent_ppo2.py:189][0m |           0.0009 |          27.8688 |           1.9508 |
[32m[20230114 18:42:08 @agent_ppo2.py:189][0m |          -0.0025 |          24.8891 |           1.9505 |
[32m[20230114 18:42:08 @agent_ppo2.py:189][0m |          -0.0022 |          23.8910 |           1.9501 |
[32m[20230114 18:42:09 @agent_ppo2.py:189][0m |          -0.0029 |          22.9804 |           1.9505 |
[32m[20230114 18:42:09 @agent_ppo2.py:189][0m |          -0.0047 |          21.6940 |           1.9496 |
[32m[20230114 18:42:09 @agent_ppo2.py:189][0m |          -0.0053 |          20.8218 |           1.9486 |
[32m[20230114 18:42:09 @agent_ppo2.py:189][0m |          -0.0052 |          20.0933 |           1.9479 |
[32m[20230114 18:42:09 @agent_ppo2.py:189][0m |          -0.0064 |          19.2041 |           1.9472 |
[32m[20230114 18:42:09 @agent_ppo2.py:189][0m |          -0.0065 |          18.4298 |           1.9467 |
[32m[20230114 18:42:09 @agent_ppo2.py:189][0m |          -0.0075 |          17.6589 |           1.9459 |
[32m[20230114 18:42:09 @agent_ppo2.py:134][0m Policy update time: 1.19 s
[32m[20230114 18:42:09 @agent_ppo2.py:142][0m Average TRAINING episode reward: -105.24
[32m[20230114 18:42:09 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: -100.00
[32m[20230114 18:42:09 @agent_ppo2.py:144][0m Average EVALUATION episode reward: -93.51
[32m[20230114 18:42:09 @agent_ppo2.py:147][0m Total time:       0.09 min
[32m[20230114 18:42:09 @agent_ppo2.py:149][0m 6144 total steps have happened
[32m[20230114 18:42:09 @agent_ppo2.py:125][0m #------------------------ Iteration 3 --------------------------#
[32m[20230114 18:42:10 @agent_ppo2.py:131][0m Sampling time: 0.36 s by 4 slaves
[32m[20230114 18:42:10 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:42:10 @agent_ppo2.py:189][0m |          -0.0003 |          54.8764 |           1.9181 |
[32m[20230114 18:42:10 @agent_ppo2.py:189][0m |          -0.0013 |          45.0776 |           1.9179 |
[32m[20230114 18:42:10 @agent_ppo2.py:189][0m |          -0.0013 |          40.8261 |           1.9169 |
[32m[20230114 18:42:10 @agent_ppo2.py:189][0m |          -0.0034 |          37.4027 |           1.9163 |
[32m[20230114 18:42:10 @agent_ppo2.py:189][0m |          -0.0037 |          34.5214 |           1.9158 |
[32m[20230114 18:42:11 @agent_ppo2.py:189][0m |          -0.0052 |          31.8451 |           1.9144 |
[32m[20230114 18:42:11 @agent_ppo2.py:189][0m |          -0.0058 |          29.5906 |           1.9135 |
[32m[20230114 18:42:11 @agent_ppo2.py:189][0m |          -0.0060 |          27.5400 |           1.9125 |
[32m[20230114 18:42:11 @agent_ppo2.py:189][0m |          -0.0066 |          25.7792 |           1.9118 |
[32m[20230114 18:42:11 @agent_ppo2.py:189][0m |          -0.0058 |          24.2276 |           1.9107 |
[32m[20230114 18:42:11 @agent_ppo2.py:134][0m Policy update time: 1.21 s
[32m[20230114 18:42:11 @agent_ppo2.py:142][0m Average TRAINING episode reward: -106.02
[32m[20230114 18:42:11 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: -98.72
[32m[20230114 18:42:11 @agent_ppo2.py:144][0m Average EVALUATION episode reward: -93.91
[32m[20230114 18:42:11 @agent_ppo2.py:147][0m Total time:       0.12 min
[32m[20230114 18:42:11 @agent_ppo2.py:149][0m 8192 total steps have happened
[32m[20230114 18:42:11 @agent_ppo2.py:125][0m #------------------------ Iteration 4 --------------------------#
[32m[20230114 18:42:11 @agent_ppo2.py:131][0m Sampling time: 0.40 s by 4 slaves
[32m[20230114 18:42:12 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:42:12 @agent_ppo2.py:189][0m |          -0.0004 |          52.4850 |           1.9043 |
[32m[20230114 18:42:12 @agent_ppo2.py:189][0m |          -0.0021 |          43.3598 |           1.9030 |
[32m[20230114 18:42:12 @agent_ppo2.py:189][0m |          -0.0040 |          38.8474 |           1.9016 |
[32m[20230114 18:42:12 @agent_ppo2.py:189][0m |          -0.0053 |          35.2321 |           1.9002 |
[32m[20230114 18:42:12 @agent_ppo2.py:189][0m |          -0.0064 |          32.2758 |           1.8983 |
[32m[20230114 18:42:12 @agent_ppo2.py:189][0m |          -0.0071 |          29.7057 |           1.8963 |
[32m[20230114 18:42:12 @agent_ppo2.py:189][0m |          -0.0075 |          27.4084 |           1.8949 |
[32m[20230114 18:42:12 @agent_ppo2.py:189][0m |          -0.0074 |          25.3928 |           1.8937 |
[32m[20230114 18:42:13 @agent_ppo2.py:189][0m |          -0.0078 |          23.5310 |           1.8928 |
[32m[20230114 18:42:13 @agent_ppo2.py:189][0m |          -0.0083 |          21.9537 |           1.8914 |
[32m[20230114 18:42:13 @agent_ppo2.py:134][0m Policy update time: 1.24 s
[32m[20230114 18:42:13 @agent_ppo2.py:142][0m Average TRAINING episode reward: -108.76
[32m[20230114 18:42:13 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: -93.06
[32m[20230114 18:42:13 @agent_ppo2.py:144][0m Average EVALUATION episode reward: -29.68
[32m[20230114 18:42:13 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: -29.68
[32m[20230114 18:42:13 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards -29.68
[32m[20230114 18:42:13 @agent_ppo2.py:147][0m Total time:       0.15 min
[32m[20230114 18:42:13 @agent_ppo2.py:149][0m 10240 total steps have happened
[32m[20230114 18:42:13 @agent_ppo2.py:125][0m #------------------------ Iteration 5 --------------------------#
[32m[20230114 18:42:13 @agent_ppo2.py:131][0m Sampling time: 0.36 s by 4 slaves
[32m[20230114 18:42:14 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:42:14 @agent_ppo2.py:189][0m |          -0.0008 |          19.1359 |           1.8948 |
[32m[20230114 18:42:14 @agent_ppo2.py:189][0m |          -0.0037 |          14.7116 |           1.8953 |
[32m[20230114 18:42:14 @agent_ppo2.py:189][0m |          -0.0050 |          12.9094 |           1.8947 |
[32m[20230114 18:42:14 @agent_ppo2.py:189][0m |          -0.0065 |          11.7292 |           1.8944 |
[32m[20230114 18:42:14 @agent_ppo2.py:189][0m |          -0.0076 |          10.5398 |           1.8940 |
[32m[20230114 18:42:14 @agent_ppo2.py:189][0m |          -0.0096 |           9.9732 |           1.8941 |
[32m[20230114 18:42:14 @agent_ppo2.py:189][0m |          -0.0086 |           9.2813 |           1.8945 |
[32m[20230114 18:42:14 @agent_ppo2.py:189][0m |          -0.0088 |           8.7730 |           1.8948 |
[32m[20230114 18:42:14 @agent_ppo2.py:189][0m |          -0.0102 |           8.3785 |           1.8954 |
[32m[20230114 18:42:15 @agent_ppo2.py:189][0m |          -0.0091 |           7.8546 |           1.8961 |
[32m[20230114 18:42:15 @agent_ppo2.py:134][0m Policy update time: 1.14 s
[32m[20230114 18:42:15 @agent_ppo2.py:142][0m Average TRAINING episode reward: -110.52
[32m[20230114 18:42:15 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: -101.78
[32m[20230114 18:42:15 @agent_ppo2.py:144][0m Average EVALUATION episode reward: -112.49
[32m[20230114 18:42:15 @agent_ppo2.py:147][0m Total time:       0.18 min
[32m[20230114 18:42:15 @agent_ppo2.py:149][0m 12288 total steps have happened
[32m[20230114 18:42:15 @agent_ppo2.py:125][0m #------------------------ Iteration 6 --------------------------#
[32m[20230114 18:42:15 @agent_ppo2.py:131][0m Sampling time: 0.38 s by 4 slaves
[32m[20230114 18:42:15 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:42:15 @agent_ppo2.py:189][0m |           0.0001 |           5.3323 |           1.8774 |
[32m[20230114 18:42:15 @agent_ppo2.py:189][0m |          -0.0010 |           3.8504 |           1.8771 |
[32m[20230114 18:42:15 @agent_ppo2.py:189][0m |          -0.0028 |           3.6353 |           1.8767 |
[32m[20230114 18:42:16 @agent_ppo2.py:189][0m |          -0.0033 |           3.4922 |           1.8762 |
[32m[20230114 18:42:16 @agent_ppo2.py:189][0m |          -0.0045 |           3.4121 |           1.8759 |
[32m[20230114 18:42:16 @agent_ppo2.py:189][0m |          -0.0042 |           3.3271 |           1.8758 |
[32m[20230114 18:42:16 @agent_ppo2.py:189][0m |          -0.0051 |           3.3101 |           1.8746 |
[32m[20230114 18:42:16 @agent_ppo2.py:189][0m |          -0.0070 |           3.1723 |           1.8744 |
[32m[20230114 18:42:16 @agent_ppo2.py:189][0m |          -0.0066 |           3.1070 |           1.8748 |
[32m[20230114 18:42:16 @agent_ppo2.py:189][0m |          -0.0071 |           3.0509 |           1.8747 |
[32m[20230114 18:42:16 @agent_ppo2.py:134][0m Policy update time: 1.13 s
[32m[20230114 18:42:16 @agent_ppo2.py:142][0m Average TRAINING episode reward: -98.36
[32m[20230114 18:42:16 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: -88.40
[32m[20230114 18:42:16 @agent_ppo2.py:144][0m Average EVALUATION episode reward: -98.95
[32m[20230114 18:42:16 @agent_ppo2.py:147][0m Total time:       0.20 min
[32m[20230114 18:42:16 @agent_ppo2.py:149][0m 14336 total steps have happened
[32m[20230114 18:42:16 @agent_ppo2.py:125][0m #------------------------ Iteration 7 --------------------------#
[32m[20230114 18:42:17 @agent_ppo2.py:131][0m Sampling time: 0.36 s by 4 slaves
[32m[20230114 18:42:17 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:42:17 @agent_ppo2.py:189][0m |          -0.0003 |          18.0914 |           1.8997 |
[32m[20230114 18:42:17 @agent_ppo2.py:189][0m |          -0.0013 |          13.9929 |           1.9006 |
[32m[20230114 18:42:17 @agent_ppo2.py:189][0m |          -0.0020 |          12.4805 |           1.9006 |
[32m[20230114 18:42:17 @agent_ppo2.py:189][0m |          -0.0032 |          11.4055 |           1.9008 |
[32m[20230114 18:42:17 @agent_ppo2.py:189][0m |          -0.0047 |          10.6864 |           1.9010 |
[32m[20230114 18:42:17 @agent_ppo2.py:189][0m |          -0.0055 |          10.1563 |           1.9012 |
[32m[20230114 18:42:18 @agent_ppo2.py:189][0m |          -0.0051 |           9.7113 |           1.9011 |
[32m[20230114 18:42:18 @agent_ppo2.py:189][0m |          -0.0062 |           9.2300 |           1.9014 |
[32m[20230114 18:42:18 @agent_ppo2.py:189][0m |          -0.0062 |           9.0040 |           1.9013 |
[32m[20230114 18:42:18 @agent_ppo2.py:189][0m |          -0.0059 |           8.8318 |           1.9009 |
[32m[20230114 18:42:18 @agent_ppo2.py:134][0m Policy update time: 1.20 s
[32m[20230114 18:42:18 @agent_ppo2.py:142][0m Average TRAINING episode reward: -102.40
[32m[20230114 18:42:18 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: -95.92
[32m[20230114 18:42:18 @agent_ppo2.py:144][0m Average EVALUATION episode reward: -36.56
[32m[20230114 18:42:18 @agent_ppo2.py:147][0m Total time:       0.23 min
[32m[20230114 18:42:18 @agent_ppo2.py:149][0m 16384 total steps have happened
[32m[20230114 18:42:18 @agent_ppo2.py:125][0m #------------------------ Iteration 8 --------------------------#
[32m[20230114 18:42:19 @agent_ppo2.py:131][0m Sampling time: 0.37 s by 4 slaves
[32m[20230114 18:42:19 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:42:19 @agent_ppo2.py:189][0m |           0.0007 |          12.4739 |           1.8796 |
[32m[20230114 18:42:19 @agent_ppo2.py:189][0m |          -0.0009 |           8.8694 |           1.8805 |
[32m[20230114 18:42:19 @agent_ppo2.py:189][0m |          -0.0016 |           7.8388 |           1.8811 |
[32m[20230114 18:42:19 @agent_ppo2.py:189][0m |          -0.0030 |           6.9807 |           1.8807 |
[32m[20230114 18:42:19 @agent_ppo2.py:189][0m |          -0.0037 |           6.9291 |           1.8809 |
[32m[20230114 18:42:19 @agent_ppo2.py:189][0m |          -0.0037 |           6.4576 |           1.8815 |
[32m[20230114 18:42:19 @agent_ppo2.py:189][0m |          -0.0039 |           6.2984 |           1.8818 |
[32m[20230114 18:42:20 @agent_ppo2.py:189][0m |          -0.0043 |           6.0326 |           1.8822 |
[32m[20230114 18:42:20 @agent_ppo2.py:189][0m |          -0.0045 |           5.7504 |           1.8825 |
[32m[20230114 18:42:20 @agent_ppo2.py:189][0m |          -0.0046 |           5.6018 |           1.8833 |
[32m[20230114 18:42:20 @agent_ppo2.py:134][0m Policy update time: 1.13 s
[32m[20230114 18:42:20 @agent_ppo2.py:142][0m Average TRAINING episode reward: -98.64
[32m[20230114 18:42:20 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: -91.05
[32m[20230114 18:42:20 @agent_ppo2.py:144][0m Average EVALUATION episode reward: -93.53
[32m[20230114 18:42:20 @agent_ppo2.py:147][0m Total time:       0.26 min
[32m[20230114 18:42:20 @agent_ppo2.py:149][0m 18432 total steps have happened
[32m[20230114 18:42:20 @agent_ppo2.py:125][0m #------------------------ Iteration 9 --------------------------#
[32m[20230114 18:42:20 @agent_ppo2.py:131][0m Sampling time: 0.39 s by 4 slaves
[32m[20230114 18:42:20 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:42:20 @agent_ppo2.py:189][0m |          -0.0013 |          23.7992 |           1.9189 |
[32m[20230114 18:42:21 @agent_ppo2.py:189][0m |          -0.0038 |          13.7954 |           1.9182 |
[32m[20230114 18:42:21 @agent_ppo2.py:189][0m |          -0.0004 |          12.2314 |           1.9169 |
[32m[20230114 18:42:21 @agent_ppo2.py:189][0m |          -0.0090 |          10.5985 |           1.9155 |
[32m[20230114 18:42:21 @agent_ppo2.py:189][0m |          -0.0067 |           9.6074 |           1.9142 |
[32m[20230114 18:42:21 @agent_ppo2.py:189][0m |          -0.0057 |           9.0181 |           1.9139 |
[32m[20230114 18:42:21 @agent_ppo2.py:189][0m |          -0.0082 |           8.7287 |           1.9139 |
[32m[20230114 18:42:21 @agent_ppo2.py:189][0m |          -0.0093 |           8.0389 |           1.9141 |
[32m[20230114 18:42:21 @agent_ppo2.py:189][0m |          -0.0079 |           7.8450 |           1.9143 |
[32m[20230114 18:42:21 @agent_ppo2.py:189][0m |          -0.0078 |           7.5720 |           1.9137 |
[32m[20230114 18:42:21 @agent_ppo2.py:134][0m Policy update time: 1.16 s
[32m[20230114 18:42:22 @agent_ppo2.py:142][0m Average TRAINING episode reward: -104.76
[32m[20230114 18:42:22 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: -88.10
[32m[20230114 18:42:22 @agent_ppo2.py:144][0m Average EVALUATION episode reward: -50.23
[32m[20230114 18:42:22 @agent_ppo2.py:147][0m Total time:       0.29 min
[32m[20230114 18:42:22 @agent_ppo2.py:149][0m 20480 total steps have happened
[32m[20230114 18:42:22 @agent_ppo2.py:125][0m #------------------------ Iteration 10 --------------------------#
[32m[20230114 18:42:22 @agent_ppo2.py:131][0m Sampling time: 0.33 s by 4 slaves
[32m[20230114 18:42:22 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:42:22 @agent_ppo2.py:189][0m |          -0.0010 |          11.9191 |           1.9237 |
[32m[20230114 18:42:22 @agent_ppo2.py:189][0m |          -0.0037 |           9.9486 |           1.9233 |
[32m[20230114 18:42:22 @agent_ppo2.py:189][0m |          -0.0050 |           9.5673 |           1.9223 |
[32m[20230114 18:42:23 @agent_ppo2.py:189][0m |          -0.0053 |           9.3100 |           1.9219 |
[32m[20230114 18:42:23 @agent_ppo2.py:189][0m |          -0.0064 |           9.0983 |           1.9219 |
[32m[20230114 18:42:23 @agent_ppo2.py:189][0m |          -0.0072 |           8.8419 |           1.9211 |
[32m[20230114 18:42:23 @agent_ppo2.py:189][0m |          -0.0070 |           8.6599 |           1.9207 |
[32m[20230114 18:42:23 @agent_ppo2.py:189][0m |          -0.0071 |           8.5812 |           1.9207 |
[32m[20230114 18:42:23 @agent_ppo2.py:189][0m |          -0.0076 |           8.5047 |           1.9214 |
[32m[20230114 18:42:23 @agent_ppo2.py:189][0m |          -0.0077 |           8.3645 |           1.9208 |
[32m[20230114 18:42:23 @agent_ppo2.py:134][0m Policy update time: 1.10 s
[32m[20230114 18:42:24 @agent_ppo2.py:142][0m Average TRAINING episode reward: -93.74
[32m[20230114 18:42:24 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: -75.57
[32m[20230114 18:42:24 @agent_ppo2.py:144][0m Average EVALUATION episode reward: -46.95
[32m[20230114 18:42:24 @agent_ppo2.py:147][0m Total time:       0.32 min
[32m[20230114 18:42:24 @agent_ppo2.py:149][0m 22528 total steps have happened
[32m[20230114 18:42:24 @agent_ppo2.py:125][0m #------------------------ Iteration 11 --------------------------#
[32m[20230114 18:42:24 @agent_ppo2.py:131][0m Sampling time: 0.37 s by 4 slaves
[32m[20230114 18:42:24 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:42:24 @agent_ppo2.py:189][0m |          -0.0024 |          15.2898 |           1.9323 |
[32m[20230114 18:42:24 @agent_ppo2.py:189][0m |          -0.0005 |          11.2597 |           1.9318 |
[32m[20230114 18:42:24 @agent_ppo2.py:189][0m |          -0.0041 |           9.6952 |           1.9316 |
[32m[20230114 18:42:24 @agent_ppo2.py:189][0m |          -0.0018 |           8.9270 |           1.9316 |
[32m[20230114 18:42:25 @agent_ppo2.py:189][0m |          -0.0099 |           8.4689 |           1.9316 |
[32m[20230114 18:42:25 @agent_ppo2.py:189][0m |           0.0035 |           8.2187 |           1.9313 |
[32m[20230114 18:42:25 @agent_ppo2.py:189][0m |          -0.0058 |           7.9832 |           1.9316 |
[32m[20230114 18:42:25 @agent_ppo2.py:189][0m |          -0.0015 |           7.6524 |           1.9321 |
[32m[20230114 18:42:25 @agent_ppo2.py:189][0m |          -0.0086 |           7.4300 |           1.9310 |
[32m[20230114 18:42:25 @agent_ppo2.py:189][0m |          -0.0108 |           7.1878 |           1.9315 |
[32m[20230114 18:42:25 @agent_ppo2.py:134][0m Policy update time: 1.24 s
[32m[20230114 18:42:26 @agent_ppo2.py:142][0m Average TRAINING episode reward: -100.49
[32m[20230114 18:42:26 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: -84.25
[32m[20230114 18:42:26 @agent_ppo2.py:144][0m Average EVALUATION episode reward: -25.53
[32m[20230114 18:42:26 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: -25.53
[32m[20230114 18:42:26 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards -25.53
[32m[20230114 18:42:26 @agent_ppo2.py:147][0m Total time:       0.36 min
[32m[20230114 18:42:26 @agent_ppo2.py:149][0m 24576 total steps have happened
[32m[20230114 18:42:26 @agent_ppo2.py:125][0m #------------------------ Iteration 12 --------------------------#
[32m[20230114 18:42:26 @agent_ppo2.py:131][0m Sampling time: 0.40 s by 4 slaves
[32m[20230114 18:42:26 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:42:26 @agent_ppo2.py:189][0m |          -0.0015 |          23.6940 |           1.9721 |
[32m[20230114 18:42:26 @agent_ppo2.py:189][0m |          -0.0037 |          16.6761 |           1.9710 |
[32m[20230114 18:42:26 @agent_ppo2.py:189][0m |          -0.0050 |          14.0287 |           1.9701 |
[32m[20230114 18:42:26 @agent_ppo2.py:189][0m |          -0.0071 |          12.4510 |           1.9688 |
[32m[20230114 18:42:27 @agent_ppo2.py:189][0m |          -0.0097 |          11.7963 |           1.9682 |
[32m[20230114 18:42:27 @agent_ppo2.py:189][0m |          -0.0074 |          10.6741 |           1.9682 |
[32m[20230114 18:42:27 @agent_ppo2.py:189][0m |          -0.0071 |           9.6851 |           1.9679 |
[32m[20230114 18:42:27 @agent_ppo2.py:189][0m |          -0.0085 |           9.1251 |           1.9684 |
[32m[20230114 18:42:27 @agent_ppo2.py:189][0m |          -0.0102 |           8.5920 |           1.9679 |
[32m[20230114 18:42:27 @agent_ppo2.py:189][0m |           0.0032 |           8.3088 |           1.9687 |
[32m[20230114 18:42:27 @agent_ppo2.py:134][0m Policy update time: 1.18 s
[32m[20230114 18:42:27 @agent_ppo2.py:142][0m Average TRAINING episode reward: -99.80
[32m[20230114 18:42:27 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: -75.32
[32m[20230114 18:42:27 @agent_ppo2.py:144][0m Average EVALUATION episode reward: -27.33
[32m[20230114 18:42:27 @agent_ppo2.py:147][0m Total time:       0.39 min
[32m[20230114 18:42:27 @agent_ppo2.py:149][0m 26624 total steps have happened
[32m[20230114 18:42:27 @agent_ppo2.py:125][0m #------------------------ Iteration 13 --------------------------#
[32m[20230114 18:42:28 @agent_ppo2.py:131][0m Sampling time: 0.34 s by 4 slaves
[32m[20230114 18:42:28 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:42:28 @agent_ppo2.py:189][0m |           0.0001 |          13.3292 |           1.9631 |
[32m[20230114 18:42:28 @agent_ppo2.py:189][0m |          -0.0029 |           9.7074 |           1.9622 |
[32m[20230114 18:42:28 @agent_ppo2.py:189][0m |          -0.0051 |           8.0500 |           1.9615 |
[32m[20230114 18:42:28 @agent_ppo2.py:189][0m |          -0.0053 |           7.2405 |           1.9608 |
[32m[20230114 18:42:28 @agent_ppo2.py:189][0m |          -0.0074 |           6.8412 |           1.9603 |
[32m[20230114 18:42:29 @agent_ppo2.py:189][0m |          -0.0064 |           6.4404 |           1.9596 |
[32m[20230114 18:42:29 @agent_ppo2.py:189][0m |          -0.0084 |           6.2511 |           1.9596 |
[32m[20230114 18:42:29 @agent_ppo2.py:189][0m |          -0.0085 |           6.0291 |           1.9593 |
[32m[20230114 18:42:29 @agent_ppo2.py:189][0m |          -0.0079 |           5.9107 |           1.9593 |
[32m[20230114 18:42:29 @agent_ppo2.py:189][0m |          -0.0094 |           5.6429 |           1.9591 |
[32m[20230114 18:42:29 @agent_ppo2.py:134][0m Policy update time: 1.15 s
[32m[20230114 18:42:29 @agent_ppo2.py:142][0m Average TRAINING episode reward: -96.79
[32m[20230114 18:42:29 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: -88.65
[32m[20230114 18:42:29 @agent_ppo2.py:144][0m Average EVALUATION episode reward: -44.76
[32m[20230114 18:42:29 @agent_ppo2.py:147][0m Total time:       0.42 min
[32m[20230114 18:42:29 @agent_ppo2.py:149][0m 28672 total steps have happened
[32m[20230114 18:42:29 @agent_ppo2.py:125][0m #------------------------ Iteration 14 --------------------------#
[32m[20230114 18:42:30 @agent_ppo2.py:131][0m Sampling time: 0.37 s by 4 slaves
[32m[20230114 18:42:30 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:42:30 @agent_ppo2.py:189][0m |          -0.0003 |          22.8247 |           1.9700 |
[32m[20230114 18:42:30 @agent_ppo2.py:189][0m |          -0.0036 |          19.4487 |           1.9701 |
[32m[20230114 18:42:30 @agent_ppo2.py:189][0m |          -0.0047 |          17.7949 |           1.9702 |
[32m[20230114 18:42:30 @agent_ppo2.py:189][0m |          -0.0061 |          16.3224 |           1.9707 |
[32m[20230114 18:42:30 @agent_ppo2.py:189][0m |          -0.0060 |          15.7428 |           1.9708 |
[32m[20230114 18:42:30 @agent_ppo2.py:189][0m |          -0.0074 |          14.6925 |           1.9717 |
[32m[20230114 18:42:30 @agent_ppo2.py:189][0m |          -0.0081 |          14.2699 |           1.9719 |
[32m[20230114 18:42:31 @agent_ppo2.py:189][0m |          -0.0085 |          13.3992 |           1.9710 |
[32m[20230114 18:42:31 @agent_ppo2.py:189][0m |          -0.0083 |          13.0851 |           1.9719 |
[32m[20230114 18:42:31 @agent_ppo2.py:189][0m |          -0.0086 |          12.6765 |           1.9722 |
[32m[20230114 18:42:31 @agent_ppo2.py:134][0m Policy update time: 1.18 s
[32m[20230114 18:42:31 @agent_ppo2.py:142][0m Average TRAINING episode reward: -99.91
[32m[20230114 18:42:31 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: -82.10
[32m[20230114 18:42:31 @agent_ppo2.py:144][0m Average EVALUATION episode reward: -27.86
[32m[20230114 18:42:31 @agent_ppo2.py:147][0m Total time:       0.45 min
[32m[20230114 18:42:31 @agent_ppo2.py:149][0m 30720 total steps have happened
[32m[20230114 18:42:31 @agent_ppo2.py:125][0m #------------------------ Iteration 15 --------------------------#
[32m[20230114 18:42:32 @agent_ppo2.py:131][0m Sampling time: 0.33 s by 4 slaves
[32m[20230114 18:42:32 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:42:32 @agent_ppo2.py:189][0m |          -0.0010 |           7.9571 |           1.9745 |
[32m[20230114 18:42:32 @agent_ppo2.py:189][0m |          -0.0039 |           5.9935 |           1.9737 |
[32m[20230114 18:42:32 @agent_ppo2.py:189][0m |          -0.0038 |           5.2258 |           1.9728 |
[32m[20230114 18:42:32 @agent_ppo2.py:189][0m |          -0.0053 |           4.7423 |           1.9726 |
[32m[20230114 18:42:32 @agent_ppo2.py:189][0m |          -0.0049 |           4.3847 |           1.9717 |
[32m[20230114 18:42:32 @agent_ppo2.py:189][0m |          -0.0055 |           4.1515 |           1.9717 |
[32m[20230114 18:42:32 @agent_ppo2.py:189][0m |          -0.0073 |           3.9652 |           1.9710 |
[32m[20230114 18:42:32 @agent_ppo2.py:189][0m |          -0.0085 |           3.8696 |           1.9714 |
[32m[20230114 18:42:33 @agent_ppo2.py:189][0m |          -0.0051 |           3.7552 |           1.9713 |
[32m[20230114 18:42:33 @agent_ppo2.py:189][0m |          -0.0074 |           3.6271 |           1.9707 |
[32m[20230114 18:42:33 @agent_ppo2.py:134][0m Policy update time: 1.13 s
[32m[20230114 18:42:33 @agent_ppo2.py:142][0m Average TRAINING episode reward: -97.04
[32m[20230114 18:42:33 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: -93.05
[32m[20230114 18:42:33 @agent_ppo2.py:144][0m Average EVALUATION episode reward: -40.60
[32m[20230114 18:42:33 @agent_ppo2.py:147][0m Total time:       0.48 min
[32m[20230114 18:42:33 @agent_ppo2.py:149][0m 32768 total steps have happened
[32m[20230114 18:42:33 @agent_ppo2.py:125][0m #------------------------ Iteration 16 --------------------------#
[32m[20230114 18:42:33 @agent_ppo2.py:131][0m Sampling time: 0.35 s by 4 slaves
[32m[20230114 18:42:33 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:42:34 @agent_ppo2.py:189][0m |           0.0005 |           5.6373 |           1.9596 |
[32m[20230114 18:42:34 @agent_ppo2.py:189][0m |          -0.0053 |           4.5929 |           1.9599 |
[32m[20230114 18:42:34 @agent_ppo2.py:189][0m |          -0.0045 |           4.2089 |           1.9586 |
[32m[20230114 18:42:34 @agent_ppo2.py:189][0m |          -0.0056 |           4.0416 |           1.9586 |
[32m[20230114 18:42:34 @agent_ppo2.py:189][0m |          -0.0067 |           3.9963 |           1.9590 |
[32m[20230114 18:42:34 @agent_ppo2.py:189][0m |          -0.0048 |           3.9596 |           1.9601 |
[32m[20230114 18:42:34 @agent_ppo2.py:189][0m |          -0.0063 |           3.9609 |           1.9611 |
[32m[20230114 18:42:34 @agent_ppo2.py:189][0m |          -0.0062 |           3.9497 |           1.9610 |
[32m[20230114 18:42:34 @agent_ppo2.py:189][0m |          -0.0069 |           3.9194 |           1.9617 |
[32m[20230114 18:42:34 @agent_ppo2.py:189][0m |          -0.0074 |           3.8416 |           1.9623 |
[32m[20230114 18:42:34 @agent_ppo2.py:134][0m Policy update time: 1.17 s
[32m[20230114 18:42:35 @agent_ppo2.py:142][0m Average TRAINING episode reward: -99.42
[32m[20230114 18:42:35 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: -95.97
[32m[20230114 18:42:35 @agent_ppo2.py:144][0m Average EVALUATION episode reward: -37.88
[32m[20230114 18:42:35 @agent_ppo2.py:147][0m Total time:       0.51 min
[32m[20230114 18:42:35 @agent_ppo2.py:149][0m 34816 total steps have happened
[32m[20230114 18:42:35 @agent_ppo2.py:125][0m #------------------------ Iteration 17 --------------------------#
[32m[20230114 18:42:35 @agent_ppo2.py:131][0m Sampling time: 0.36 s by 4 slaves
[32m[20230114 18:42:35 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:42:35 @agent_ppo2.py:189][0m |          -0.0012 |          15.4613 |           1.9817 |
[32m[20230114 18:42:36 @agent_ppo2.py:189][0m |          -0.0051 |          10.5370 |           1.9823 |
[32m[20230114 18:42:36 @agent_ppo2.py:189][0m |          -0.0052 |           8.9537 |           1.9836 |
[32m[20230114 18:42:36 @agent_ppo2.py:189][0m |          -0.0089 |           8.1851 |           1.9851 |
[32m[20230114 18:42:36 @agent_ppo2.py:189][0m |          -0.0082 |           7.6051 |           1.9859 |
[32m[20230114 18:42:36 @agent_ppo2.py:189][0m |          -0.0096 |           7.4639 |           1.9871 |
[32m[20230114 18:42:36 @agent_ppo2.py:189][0m |          -0.0087 |           6.8714 |           1.9880 |
[32m[20230114 18:42:36 @agent_ppo2.py:189][0m |          -0.0083 |           6.6746 |           1.9888 |
[32m[20230114 18:42:36 @agent_ppo2.py:189][0m |          -0.0074 |           6.9359 |           1.9897 |
[32m[20230114 18:42:36 @agent_ppo2.py:189][0m |          -0.0116 |           6.8081 |           1.9906 |
[32m[20230114 18:42:36 @agent_ppo2.py:134][0m Policy update time: 1.21 s
[32m[20230114 18:42:37 @agent_ppo2.py:142][0m Average TRAINING episode reward: -101.17
[32m[20230114 18:42:37 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: -95.08
[32m[20230114 18:42:37 @agent_ppo2.py:144][0m Average EVALUATION episode reward: -46.59
[32m[20230114 18:42:37 @agent_ppo2.py:147][0m Total time:       0.54 min
[32m[20230114 18:42:37 @agent_ppo2.py:149][0m 36864 total steps have happened
[32m[20230114 18:42:37 @agent_ppo2.py:125][0m #------------------------ Iteration 18 --------------------------#
[32m[20230114 18:42:37 @agent_ppo2.py:131][0m Sampling time: 0.35 s by 4 slaves
[32m[20230114 18:42:37 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:42:37 @agent_ppo2.py:189][0m |          -0.0001 |           2.9272 |           2.0520 |
[32m[20230114 18:42:37 @agent_ppo2.py:189][0m |          -0.0025 |           2.2419 |           2.0521 |
[32m[20230114 18:42:38 @agent_ppo2.py:189][0m |          -0.0036 |           2.1566 |           2.0523 |
[32m[20230114 18:42:38 @agent_ppo2.py:189][0m |          -0.0045 |           2.0872 |           2.0520 |
[32m[20230114 18:42:38 @agent_ppo2.py:189][0m |          -0.0051 |           2.0403 |           2.0526 |
[32m[20230114 18:42:38 @agent_ppo2.py:189][0m |          -0.0052 |           2.0178 |           2.0534 |
[32m[20230114 18:42:38 @agent_ppo2.py:189][0m |          -0.0056 |           1.9851 |           2.0545 |
[32m[20230114 18:42:38 @agent_ppo2.py:189][0m |          -0.0058 |           1.9724 |           2.0556 |
[32m[20230114 18:42:38 @agent_ppo2.py:189][0m |          -0.0061 |           1.9340 |           2.0566 |
[32m[20230114 18:42:38 @agent_ppo2.py:189][0m |          -0.0061 |           1.9222 |           2.0574 |
[32m[20230114 18:42:38 @agent_ppo2.py:134][0m Policy update time: 1.12 s
[32m[20230114 18:42:39 @agent_ppo2.py:142][0m Average TRAINING episode reward: -91.70
[32m[20230114 18:42:39 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: -88.34
[32m[20230114 18:42:39 @agent_ppo2.py:144][0m Average EVALUATION episode reward: -40.15
[32m[20230114 18:42:39 @agent_ppo2.py:147][0m Total time:       0.57 min
[32m[20230114 18:42:39 @agent_ppo2.py:149][0m 38912 total steps have happened
[32m[20230114 18:42:39 @agent_ppo2.py:125][0m #------------------------ Iteration 19 --------------------------#
[32m[20230114 18:42:39 @agent_ppo2.py:131][0m Sampling time: 0.33 s by 4 slaves
[32m[20230114 18:42:39 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:42:39 @agent_ppo2.py:189][0m |           0.0001 |           1.8426 |           2.0524 |
[32m[20230114 18:42:39 @agent_ppo2.py:189][0m |          -0.0026 |           1.5321 |           2.0526 |
[32m[20230114 18:42:39 @agent_ppo2.py:189][0m |          -0.0045 |           1.4503 |           2.0522 |
[32m[20230114 18:42:39 @agent_ppo2.py:189][0m |          -0.0055 |           1.4058 |           2.0522 |
[32m[20230114 18:42:40 @agent_ppo2.py:189][0m |          -0.0065 |           1.3655 |           2.0525 |
[32m[20230114 18:42:40 @agent_ppo2.py:189][0m |          -0.0070 |           1.3376 |           2.0521 |
[32m[20230114 18:42:40 @agent_ppo2.py:189][0m |          -0.0073 |           1.3118 |           2.0525 |
[32m[20230114 18:42:40 @agent_ppo2.py:189][0m |          -0.0080 |           1.2948 |           2.0532 |
[32m[20230114 18:42:40 @agent_ppo2.py:189][0m |          -0.0082 |           1.2813 |           2.0538 |
[32m[20230114 18:42:40 @agent_ppo2.py:189][0m |          -0.0084 |           1.2680 |           2.0547 |
[32m[20230114 18:42:40 @agent_ppo2.py:134][0m Policy update time: 1.15 s
[32m[20230114 18:42:40 @agent_ppo2.py:142][0m Average TRAINING episode reward: -93.80
[32m[20230114 18:42:40 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: -86.60
[32m[20230114 18:42:40 @agent_ppo2.py:144][0m Average EVALUATION episode reward: -43.55
[32m[20230114 18:42:40 @agent_ppo2.py:147][0m Total time:       0.60 min
[32m[20230114 18:42:40 @agent_ppo2.py:149][0m 40960 total steps have happened
[32m[20230114 18:42:40 @agent_ppo2.py:125][0m #------------------------ Iteration 20 --------------------------#
[32m[20230114 18:42:41 @agent_ppo2.py:131][0m Sampling time: 0.36 s by 4 slaves
[32m[20230114 18:42:41 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:42:41 @agent_ppo2.py:189][0m |           0.0003 |           0.9157 |           2.1011 |
[32m[20230114 18:42:41 @agent_ppo2.py:189][0m |          -0.0017 |           0.8616 |           2.1020 |
[32m[20230114 18:42:41 @agent_ppo2.py:189][0m |          -0.0034 |           0.8431 |           2.1027 |
[32m[20230114 18:42:41 @agent_ppo2.py:189][0m |          -0.0041 |           0.8323 |           2.1039 |
[32m[20230114 18:42:41 @agent_ppo2.py:189][0m |          -0.0048 |           0.8246 |           2.1058 |
[32m[20230114 18:42:42 @agent_ppo2.py:189][0m |          -0.0051 |           0.8212 |           2.1082 |
[32m[20230114 18:42:42 @agent_ppo2.py:189][0m |          -0.0055 |           0.8156 |           2.1097 |
[32m[20230114 18:42:42 @agent_ppo2.py:189][0m |          -0.0060 |           0.8121 |           2.1114 |
[32m[20230114 18:42:42 @agent_ppo2.py:189][0m |          -0.0062 |           0.8073 |           2.1129 |
[32m[20230114 18:42:42 @agent_ppo2.py:189][0m |          -0.0065 |           0.8037 |           2.1146 |
[32m[20230114 18:42:42 @agent_ppo2.py:134][0m Policy update time: 1.13 s
[32m[20230114 18:42:42 @agent_ppo2.py:142][0m Average TRAINING episode reward: -90.73
[32m[20230114 18:42:42 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: -88.20
[32m[20230114 18:42:42 @agent_ppo2.py:144][0m Average EVALUATION episode reward: -41.09
[32m[20230114 18:42:42 @agent_ppo2.py:147][0m Total time:       0.63 min
[32m[20230114 18:42:42 @agent_ppo2.py:149][0m 43008 total steps have happened
[32m[20230114 18:42:42 @agent_ppo2.py:125][0m #------------------------ Iteration 21 --------------------------#
[32m[20230114 18:42:43 @agent_ppo2.py:131][0m Sampling time: 0.35 s by 4 slaves
[32m[20230114 18:42:43 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:42:43 @agent_ppo2.py:189][0m |           0.0008 |           3.3114 |           2.1138 |
[32m[20230114 18:42:43 @agent_ppo2.py:189][0m |          -0.0041 |           2.0649 |           2.1124 |
[32m[20230114 18:42:43 @agent_ppo2.py:189][0m |          -0.0049 |           1.6854 |           2.1099 |
[32m[20230114 18:42:43 @agent_ppo2.py:189][0m |          -0.0053 |           1.4719 |           2.1091 |
[32m[20230114 18:42:43 @agent_ppo2.py:189][0m |          -0.0071 |           1.3472 |           2.1082 |
[32m[20230114 18:42:43 @agent_ppo2.py:189][0m |          -0.0081 |           1.2438 |           2.1069 |
[32m[20230114 18:42:43 @agent_ppo2.py:189][0m |          -0.0000 |           1.1853 |           2.1057 |
[32m[20230114 18:42:44 @agent_ppo2.py:189][0m |          -0.0077 |           1.1216 |           2.1040 |
[32m[20230114 18:42:44 @agent_ppo2.py:189][0m |          -0.0074 |           1.0649 |           2.1033 |
[32m[20230114 18:42:44 @agent_ppo2.py:189][0m |          -0.0088 |           1.0415 |           2.1032 |
[32m[20230114 18:42:44 @agent_ppo2.py:134][0m Policy update time: 1.12 s
[32m[20230114 18:42:44 @agent_ppo2.py:142][0m Average TRAINING episode reward: -96.26
[32m[20230114 18:42:44 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: -86.87
[32m[20230114 18:42:44 @agent_ppo2.py:144][0m Average EVALUATION episode reward: -40.90
[32m[20230114 18:42:44 @agent_ppo2.py:147][0m Total time:       0.66 min
[32m[20230114 18:42:44 @agent_ppo2.py:149][0m 45056 total steps have happened
[32m[20230114 18:42:44 @agent_ppo2.py:125][0m #------------------------ Iteration 22 --------------------------#
[32m[20230114 18:42:44 @agent_ppo2.py:131][0m Sampling time: 0.35 s by 4 slaves
[32m[20230114 18:42:44 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:42:45 @agent_ppo2.py:189][0m |          -0.0014 |           0.8642 |           2.1212 |
[32m[20230114 18:42:45 @agent_ppo2.py:189][0m |          -0.0044 |           0.8074 |           2.1214 |
[32m[20230114 18:42:45 @agent_ppo2.py:189][0m |          -0.0060 |           0.7862 |           2.1233 |
[32m[20230114 18:42:45 @agent_ppo2.py:189][0m |          -0.0072 |           0.7747 |           2.1251 |
[32m[20230114 18:42:45 @agent_ppo2.py:189][0m |          -0.0078 |           0.7676 |           2.1274 |
[32m[20230114 18:42:45 @agent_ppo2.py:189][0m |          -0.0083 |           0.7564 |           2.1292 |
[32m[20230114 18:42:45 @agent_ppo2.py:189][0m |          -0.0089 |           0.7533 |           2.1311 |
[32m[20230114 18:42:45 @agent_ppo2.py:189][0m |          -0.0094 |           0.7488 |           2.1335 |
[32m[20230114 18:42:45 @agent_ppo2.py:189][0m |          -0.0096 |           0.7451 |           2.1355 |
[32m[20230114 18:42:45 @agent_ppo2.py:189][0m |          -0.0101 |           0.7423 |           2.1374 |
[32m[20230114 18:42:45 @agent_ppo2.py:134][0m Policy update time: 1.10 s
[32m[20230114 18:42:46 @agent_ppo2.py:142][0m Average TRAINING episode reward: -89.95
[32m[20230114 18:42:46 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: -88.07
[32m[20230114 18:42:46 @agent_ppo2.py:144][0m Average EVALUATION episode reward: -39.16
[32m[20230114 18:42:46 @agent_ppo2.py:147][0m Total time:       0.69 min
[32m[20230114 18:42:46 @agent_ppo2.py:149][0m 47104 total steps have happened
[32m[20230114 18:42:46 @agent_ppo2.py:125][0m #------------------------ Iteration 23 --------------------------#
[32m[20230114 18:42:46 @agent_ppo2.py:131][0m Sampling time: 0.33 s by 4 slaves
[32m[20230114 18:42:46 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:42:46 @agent_ppo2.py:189][0m |          -0.0004 |           0.7837 |           2.1853 |
[32m[20230114 18:42:46 @agent_ppo2.py:189][0m |          -0.0045 |           0.7426 |           2.1830 |
[32m[20230114 18:42:46 @agent_ppo2.py:189][0m |          -0.0066 |           0.7224 |           2.1822 |
[32m[20230114 18:42:47 @agent_ppo2.py:189][0m |          -0.0075 |           0.7070 |           2.1818 |
[32m[20230114 18:42:47 @agent_ppo2.py:189][0m |          -0.0081 |           0.6978 |           2.1823 |
[32m[20230114 18:42:47 @agent_ppo2.py:189][0m |          -0.0086 |           0.6906 |           2.1838 |
[32m[20230114 18:42:47 @agent_ppo2.py:189][0m |          -0.0088 |           0.6863 |           2.1842 |
[32m[20230114 18:42:47 @agent_ppo2.py:189][0m |          -0.0091 |           0.6791 |           2.1858 |
[32m[20230114 18:42:47 @agent_ppo2.py:189][0m |          -0.0094 |           0.6743 |           2.1873 |
[32m[20230114 18:42:47 @agent_ppo2.py:189][0m |          -0.0097 |           0.6729 |           2.1887 |
[32m[20230114 18:42:47 @agent_ppo2.py:134][0m Policy update time: 1.09 s
[32m[20230114 18:42:47 @agent_ppo2.py:142][0m Average TRAINING episode reward: -82.16
[32m[20230114 18:42:47 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: -77.85
[32m[20230114 18:42:47 @agent_ppo2.py:144][0m Average EVALUATION episode reward: -28.83
[32m[20230114 18:42:47 @agent_ppo2.py:147][0m Total time:       0.72 min
[32m[20230114 18:42:47 @agent_ppo2.py:149][0m 49152 total steps have happened
[32m[20230114 18:42:47 @agent_ppo2.py:125][0m #------------------------ Iteration 24 --------------------------#
[32m[20230114 18:42:48 @agent_ppo2.py:131][0m Sampling time: 0.33 s by 4 slaves
[32m[20230114 18:42:48 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:42:48 @agent_ppo2.py:189][0m |          -0.0015 |           0.8512 |           2.1953 |
[32m[20230114 18:42:48 @agent_ppo2.py:189][0m |          -0.0048 |           0.7632 |           2.1938 |
[32m[20230114 18:42:48 @agent_ppo2.py:189][0m |          -0.0060 |           0.7484 |           2.1931 |
[32m[20230114 18:42:48 @agent_ppo2.py:189][0m |          -0.0068 |           0.7343 |           2.1925 |
[32m[20230114 18:42:48 @agent_ppo2.py:189][0m |          -0.0074 |           0.7308 |           2.1928 |
[32m[20230114 18:42:48 @agent_ppo2.py:189][0m |          -0.0077 |           0.7227 |           2.1934 |
[32m[20230114 18:42:49 @agent_ppo2.py:189][0m |          -0.0082 |           0.7217 |           2.1938 |
[32m[20230114 18:42:49 @agent_ppo2.py:189][0m |          -0.0086 |           0.7157 |           2.1944 |
[32m[20230114 18:42:49 @agent_ppo2.py:189][0m |          -0.0086 |           0.7059 |           2.1953 |
[32m[20230114 18:42:49 @agent_ppo2.py:189][0m |          -0.0090 |           0.7028 |           2.1947 |
[32m[20230114 18:42:49 @agent_ppo2.py:134][0m Policy update time: 1.09 s
[32m[20230114 18:42:49 @agent_ppo2.py:142][0m Average TRAINING episode reward: -82.21
[32m[20230114 18:42:49 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: -74.15
[32m[20230114 18:42:49 @agent_ppo2.py:144][0m Average EVALUATION episode reward: -27.42
[32m[20230114 18:42:49 @agent_ppo2.py:147][0m Total time:       0.75 min
[32m[20230114 18:42:49 @agent_ppo2.py:149][0m 51200 total steps have happened
[32m[20230114 18:42:49 @agent_ppo2.py:125][0m #------------------------ Iteration 25 --------------------------#
[32m[20230114 18:42:49 @agent_ppo2.py:131][0m Sampling time: 0.32 s by 4 slaves
[32m[20230114 18:42:50 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:42:50 @agent_ppo2.py:189][0m |          -0.0003 |           0.7847 |           2.2352 |
[32m[20230114 18:42:50 @agent_ppo2.py:189][0m |          -0.0033 |           0.7365 |           2.2364 |
[32m[20230114 18:42:50 @agent_ppo2.py:189][0m |          -0.0048 |           0.7138 |           2.2367 |
[32m[20230114 18:42:50 @agent_ppo2.py:189][0m |          -0.0061 |           0.7065 |           2.2380 |
[32m[20230114 18:42:50 @agent_ppo2.py:189][0m |          -0.0068 |           0.6983 |           2.2393 |
[32m[20230114 18:42:50 @agent_ppo2.py:189][0m |          -0.0071 |           0.6959 |           2.2397 |
[32m[20230114 18:42:50 @agent_ppo2.py:189][0m |          -0.0077 |           0.6848 |           2.2418 |
[32m[20230114 18:42:50 @agent_ppo2.py:189][0m |          -0.0081 |           0.6815 |           2.2430 |
[32m[20230114 18:42:50 @agent_ppo2.py:189][0m |          -0.0084 |           0.6800 |           2.2444 |
[32m[20230114 18:42:51 @agent_ppo2.py:189][0m |          -0.0087 |           0.6722 |           2.2460 |
[32m[20230114 18:42:51 @agent_ppo2.py:134][0m Policy update time: 1.09 s
[32m[20230114 18:42:51 @agent_ppo2.py:142][0m Average TRAINING episode reward: -72.70
[32m[20230114 18:42:51 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: -66.99
[32m[20230114 18:42:51 @agent_ppo2.py:144][0m Average EVALUATION episode reward: -27.98
[32m[20230114 18:42:51 @agent_ppo2.py:147][0m Total time:       0.78 min
[32m[20230114 18:42:51 @agent_ppo2.py:149][0m 53248 total steps have happened
[32m[20230114 18:42:51 @agent_ppo2.py:125][0m #------------------------ Iteration 26 --------------------------#
[32m[20230114 18:42:51 @agent_ppo2.py:131][0m Sampling time: 0.38 s by 4 slaves
[32m[20230114 18:42:51 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:42:51 @agent_ppo2.py:189][0m |          -0.0011 |           0.7774 |           2.2434 |
[32m[20230114 18:42:52 @agent_ppo2.py:189][0m |          -0.0053 |           0.7257 |           2.2413 |
[32m[20230114 18:42:52 @agent_ppo2.py:189][0m |          -0.0069 |           0.7044 |           2.2412 |
[32m[20230114 18:42:52 @agent_ppo2.py:189][0m |          -0.0080 |           0.7001 |           2.2413 |
[32m[20230114 18:42:52 @agent_ppo2.py:189][0m |          -0.0086 |           0.6887 |           2.2426 |
[32m[20230114 18:42:52 @agent_ppo2.py:189][0m |          -0.0090 |           0.6853 |           2.2435 |
[32m[20230114 18:42:52 @agent_ppo2.py:189][0m |          -0.0094 |           0.6816 |           2.2450 |
[32m[20230114 18:42:52 @agent_ppo2.py:189][0m |          -0.0095 |           0.6773 |           2.2454 |
[32m[20230114 18:42:52 @agent_ppo2.py:189][0m |          -0.0099 |           0.6737 |           2.2467 |
[32m[20230114 18:42:52 @agent_ppo2.py:189][0m |          -0.0102 |           0.6693 |           2.2488 |
[32m[20230114 18:42:52 @agent_ppo2.py:134][0m Policy update time: 1.08 s
[32m[20230114 18:42:53 @agent_ppo2.py:142][0m Average TRAINING episode reward: -64.80
[32m[20230114 18:42:53 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: -57.19
[32m[20230114 18:42:53 @agent_ppo2.py:144][0m Average EVALUATION episode reward: -30.01
[32m[20230114 18:42:53 @agent_ppo2.py:147][0m Total time:       0.81 min
[32m[20230114 18:42:53 @agent_ppo2.py:149][0m 55296 total steps have happened
[32m[20230114 18:42:53 @agent_ppo2.py:125][0m #------------------------ Iteration 27 --------------------------#
[32m[20230114 18:42:53 @agent_ppo2.py:131][0m Sampling time: 0.32 s by 4 slaves
[32m[20230114 18:42:53 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:42:53 @agent_ppo2.py:189][0m |          -0.0027 |           0.6777 |           2.2870 |
[32m[20230114 18:42:53 @agent_ppo2.py:189][0m |          -0.0075 |           0.6441 |           2.2851 |
[32m[20230114 18:42:53 @agent_ppo2.py:189][0m |          -0.0088 |           0.6304 |           2.2847 |
[32m[20230114 18:42:53 @agent_ppo2.py:189][0m |          -0.0094 |           0.6244 |           2.2889 |
[32m[20230114 18:42:54 @agent_ppo2.py:189][0m |          -0.0103 |           0.6178 |           2.2902 |
[32m[20230114 18:42:54 @agent_ppo2.py:189][0m |          -0.0106 |           0.6134 |           2.2935 |
[32m[20230114 18:42:54 @agent_ppo2.py:189][0m |          -0.0112 |           0.6089 |           2.2956 |
[32m[20230114 18:42:54 @agent_ppo2.py:189][0m |          -0.0114 |           0.6045 |           2.2977 |
[32m[20230114 18:42:54 @agent_ppo2.py:189][0m |          -0.0120 |           0.6006 |           2.2997 |
[32m[20230114 18:42:54 @agent_ppo2.py:189][0m |          -0.0121 |           0.6035 |           2.3000 |
[32m[20230114 18:42:54 @agent_ppo2.py:134][0m Policy update time: 1.08 s
[32m[20230114 18:42:54 @agent_ppo2.py:142][0m Average TRAINING episode reward: -51.04
[32m[20230114 18:42:54 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: -49.52
[32m[20230114 18:42:54 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 88.48
[32m[20230114 18:42:54 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 88.48
[32m[20230114 18:42:54 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 88.48
[32m[20230114 18:42:54 @agent_ppo2.py:147][0m Total time:       0.84 min
[32m[20230114 18:42:54 @agent_ppo2.py:149][0m 57344 total steps have happened
[32m[20230114 18:42:54 @agent_ppo2.py:125][0m #------------------------ Iteration 28 --------------------------#
[32m[20230114 18:42:55 @agent_ppo2.py:131][0m Sampling time: 0.50 s by 4 slaves
[32m[20230114 18:42:55 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:42:55 @agent_ppo2.py:189][0m |          -0.0016 |           0.7224 |           2.3413 |
[32m[20230114 18:42:55 @agent_ppo2.py:189][0m |          -0.0062 |           0.6875 |           2.3366 |
[32m[20230114 18:42:55 @agent_ppo2.py:189][0m |          -0.0078 |           0.6807 |           2.3360 |
[32m[20230114 18:42:55 @agent_ppo2.py:189][0m |          -0.0085 |           0.6754 |           2.3366 |
[32m[20230114 18:42:55 @agent_ppo2.py:189][0m |          -0.0090 |           0.6687 |           2.3365 |
[32m[20230114 18:42:56 @agent_ppo2.py:189][0m |          -0.0097 |           0.6642 |           2.3376 |
[32m[20230114 18:42:56 @agent_ppo2.py:189][0m |          -0.0101 |           0.6604 |           2.3391 |
[32m[20230114 18:42:56 @agent_ppo2.py:189][0m |          -0.0101 |           0.6594 |           2.3396 |
[32m[20230114 18:42:56 @agent_ppo2.py:189][0m |          -0.0107 |           0.6560 |           2.3414 |
[32m[20230114 18:42:56 @agent_ppo2.py:189][0m |          -0.0112 |           0.6514 |           2.3416 |
[32m[20230114 18:42:56 @agent_ppo2.py:134][0m Policy update time: 1.11 s
[32m[20230114 18:42:56 @agent_ppo2.py:142][0m Average TRAINING episode reward: -40.87
[32m[20230114 18:42:56 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: -29.16
[32m[20230114 18:42:56 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 93.14
[32m[20230114 18:42:56 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 93.14
[32m[20230114 18:42:56 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 93.14
[32m[20230114 18:42:56 @agent_ppo2.py:147][0m Total time:       0.87 min
[32m[20230114 18:42:56 @agent_ppo2.py:149][0m 59392 total steps have happened
[32m[20230114 18:42:56 @agent_ppo2.py:125][0m #------------------------ Iteration 29 --------------------------#
[32m[20230114 18:42:57 @agent_ppo2.py:131][0m Sampling time: 0.34 s by 4 slaves
[32m[20230114 18:42:57 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:42:57 @agent_ppo2.py:189][0m |          -0.0039 |           0.7338 |           2.4010 |
[32m[20230114 18:42:57 @agent_ppo2.py:189][0m |          -0.0083 |           0.6883 |           2.4004 |
[32m[20230114 18:42:57 @agent_ppo2.py:189][0m |          -0.0099 |           0.6811 |           2.4048 |
[32m[20230114 18:42:57 @agent_ppo2.py:189][0m |          -0.0108 |           0.6699 |           2.4070 |
[32m[20230114 18:42:57 @agent_ppo2.py:189][0m |          -0.0112 |           0.6638 |           2.4101 |
[32m[20230114 18:42:57 @agent_ppo2.py:189][0m |          -0.0119 |           0.6592 |           2.4114 |
[32m[20230114 18:42:58 @agent_ppo2.py:189][0m |          -0.0121 |           0.6533 |           2.4153 |
[32m[20230114 18:42:58 @agent_ppo2.py:189][0m |          -0.0127 |           0.6492 |           2.4171 |
[32m[20230114 18:42:58 @agent_ppo2.py:189][0m |          -0.0129 |           0.6445 |           2.4189 |
[32m[20230114 18:42:58 @agent_ppo2.py:189][0m |          -0.0134 |           0.6414 |           2.4215 |
[32m[20230114 18:42:58 @agent_ppo2.py:134][0m Policy update time: 1.39 s
[32m[20230114 18:42:58 @agent_ppo2.py:142][0m Average TRAINING episode reward: -32.21
[32m[20230114 18:42:58 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: -22.56
[32m[20230114 18:42:58 @agent_ppo2.py:144][0m Average EVALUATION episode reward: -25.47
[32m[20230114 18:42:58 @agent_ppo2.py:147][0m Total time:       0.90 min
[32m[20230114 18:42:58 @agent_ppo2.py:149][0m 61440 total steps have happened
[32m[20230114 18:42:58 @agent_ppo2.py:125][0m #------------------------ Iteration 30 --------------------------#
[32m[20230114 18:42:59 @agent_ppo2.py:131][0m Sampling time: 0.32 s by 4 slaves
[32m[20230114 18:42:59 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:42:59 @agent_ppo2.py:189][0m |          -0.0034 |           0.6875 |           2.4562 |
[32m[20230114 18:42:59 @agent_ppo2.py:189][0m |          -0.0082 |           0.6503 |           2.4545 |
[32m[20230114 18:42:59 @agent_ppo2.py:189][0m |          -0.0099 |           0.6375 |           2.4558 |
[32m[20230114 18:42:59 @agent_ppo2.py:189][0m |          -0.0105 |           0.6295 |           2.4566 |
[32m[20230114 18:42:59 @agent_ppo2.py:189][0m |          -0.0116 |           0.6223 |           2.4575 |
[32m[20230114 18:42:59 @agent_ppo2.py:189][0m |          -0.0114 |           0.6174 |           2.4589 |
[32m[20230114 18:42:59 @agent_ppo2.py:189][0m |          -0.0121 |           0.6169 |           2.4604 |
[32m[20230114 18:43:00 @agent_ppo2.py:189][0m |          -0.0125 |           0.6094 |           2.4604 |
[32m[20230114 18:43:00 @agent_ppo2.py:189][0m |          -0.0127 |           0.6107 |           2.4619 |
[32m[20230114 18:43:00 @agent_ppo2.py:189][0m |          -0.0131 |           0.6042 |           2.4632 |
[32m[20230114 18:43:00 @agent_ppo2.py:134][0m Policy update time: 1.17 s
[32m[20230114 18:43:00 @agent_ppo2.py:142][0m Average TRAINING episode reward: -26.53
[32m[20230114 18:43:00 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: -20.37
[32m[20230114 18:43:00 @agent_ppo2.py:144][0m Average EVALUATION episode reward: -42.04
[32m[20230114 18:43:00 @agent_ppo2.py:147][0m Total time:       0.93 min
[32m[20230114 18:43:00 @agent_ppo2.py:149][0m 63488 total steps have happened
[32m[20230114 18:43:00 @agent_ppo2.py:125][0m #------------------------ Iteration 31 --------------------------#
[32m[20230114 18:43:00 @agent_ppo2.py:131][0m Sampling time: 0.32 s by 4 slaves
[32m[20230114 18:43:01 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:43:01 @agent_ppo2.py:189][0m |          -0.0018 |           0.7753 |           2.4742 |
[32m[20230114 18:43:01 @agent_ppo2.py:189][0m |          -0.0060 |           0.7120 |           2.4731 |
[32m[20230114 18:43:01 @agent_ppo2.py:189][0m |          -0.0070 |           0.7017 |           2.4733 |
[32m[20230114 18:43:01 @agent_ppo2.py:189][0m |          -0.0079 |           0.6921 |           2.4756 |
[32m[20230114 18:43:01 @agent_ppo2.py:189][0m |          -0.0082 |           0.6916 |           2.4753 |
[32m[20230114 18:43:01 @agent_ppo2.py:189][0m |          -0.0084 |           0.6844 |           2.4781 |
[32m[20230114 18:43:01 @agent_ppo2.py:189][0m |          -0.0090 |           0.6796 |           2.4788 |
[32m[20230114 18:43:01 @agent_ppo2.py:189][0m |          -0.0090 |           0.6755 |           2.4812 |
[32m[20230114 18:43:01 @agent_ppo2.py:189][0m |          -0.0096 |           0.6718 |           2.4837 |
[32m[20230114 18:43:02 @agent_ppo2.py:189][0m |          -0.0098 |           0.6698 |           2.4844 |
[32m[20230114 18:43:02 @agent_ppo2.py:134][0m Policy update time: 1.16 s
[32m[20230114 18:43:02 @agent_ppo2.py:142][0m Average TRAINING episode reward: -7.76
[32m[20230114 18:43:02 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 6.42
[32m[20230114 18:43:02 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 21.92
[32m[20230114 18:43:02 @agent_ppo2.py:147][0m Total time:       0.96 min
[32m[20230114 18:43:02 @agent_ppo2.py:149][0m 65536 total steps have happened
[32m[20230114 18:43:02 @agent_ppo2.py:125][0m #------------------------ Iteration 32 --------------------------#
[32m[20230114 18:43:02 @agent_ppo2.py:131][0m Sampling time: 0.40 s by 4 slaves
[32m[20230114 18:43:02 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:43:03 @agent_ppo2.py:189][0m |          -0.0004 |           2.5641 |           2.5129 |
[32m[20230114 18:43:03 @agent_ppo2.py:189][0m |          -0.0062 |           1.6404 |           2.5110 |
[32m[20230114 18:43:03 @agent_ppo2.py:189][0m |          -0.0079 |           1.4368 |           2.5088 |
[32m[20230114 18:43:03 @agent_ppo2.py:189][0m |          -0.0087 |           1.3939 |           2.5075 |
[32m[20230114 18:43:03 @agent_ppo2.py:189][0m |          -0.0087 |           1.2533 |           2.5063 |
[32m[20230114 18:43:03 @agent_ppo2.py:189][0m |          -0.0103 |           1.2151 |           2.5069 |
[32m[20230114 18:43:03 @agent_ppo2.py:189][0m |          -0.0102 |           1.1497 |           2.5062 |
[32m[20230114 18:43:03 @agent_ppo2.py:189][0m |          -0.0102 |           1.1325 |           2.5061 |
[32m[20230114 18:43:03 @agent_ppo2.py:189][0m |          -0.0111 |           1.1166 |           2.5058 |
[32m[20230114 18:43:04 @agent_ppo2.py:189][0m |          -0.0107 |           1.0830 |           2.5065 |
[32m[20230114 18:43:04 @agent_ppo2.py:134][0m Policy update time: 1.23 s
[32m[20230114 18:43:04 @agent_ppo2.py:142][0m Average TRAINING episode reward: -19.27
[32m[20230114 18:43:04 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 25.92
[32m[20230114 18:43:04 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 64.88
[32m[20230114 18:43:04 @agent_ppo2.py:147][0m Total time:       1.00 min
[32m[20230114 18:43:04 @agent_ppo2.py:149][0m 67584 total steps have happened
[32m[20230114 18:43:04 @agent_ppo2.py:125][0m #------------------------ Iteration 33 --------------------------#
[32m[20230114 18:43:04 @agent_ppo2.py:131][0m Sampling time: 0.38 s by 4 slaves
[32m[20230114 18:43:04 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:43:04 @agent_ppo2.py:189][0m |          -0.0008 |           1.9766 |           2.5655 |
[32m[20230114 18:43:05 @agent_ppo2.py:189][0m |          -0.0037 |           1.1010 |           2.5639 |
[32m[20230114 18:43:05 @agent_ppo2.py:189][0m |          -0.0044 |           1.0302 |           2.5618 |
[32m[20230114 18:43:05 @agent_ppo2.py:189][0m |          -0.0049 |           1.0095 |           2.5624 |
[32m[20230114 18:43:05 @agent_ppo2.py:189][0m |          -0.0056 |           0.9685 |           2.5616 |
[32m[20230114 18:43:05 @agent_ppo2.py:189][0m |          -0.0058 |           0.9549 |           2.5615 |
[32m[20230114 18:43:05 @agent_ppo2.py:189][0m |          -0.0061 |           0.9375 |           2.5599 |
[32m[20230114 18:43:05 @agent_ppo2.py:189][0m |          -0.0058 |           0.9214 |           2.5615 |
[32m[20230114 18:43:05 @agent_ppo2.py:189][0m |          -0.0062 |           0.9134 |           2.5602 |
[32m[20230114 18:43:06 @agent_ppo2.py:189][0m |          -0.0064 |           0.9143 |           2.5602 |
[32m[20230114 18:43:06 @agent_ppo2.py:134][0m Policy update time: 1.34 s
[32m[20230114 18:43:06 @agent_ppo2.py:142][0m Average TRAINING episode reward: -20.72
[32m[20230114 18:43:06 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 8.14
[32m[20230114 18:43:06 @agent_ppo2.py:144][0m Average EVALUATION episode reward: -60.93
[32m[20230114 18:43:06 @agent_ppo2.py:147][0m Total time:       1.03 min
[32m[20230114 18:43:06 @agent_ppo2.py:149][0m 69632 total steps have happened
[32m[20230114 18:43:06 @agent_ppo2.py:125][0m #------------------------ Iteration 34 --------------------------#
[32m[20230114 18:43:06 @agent_ppo2.py:131][0m Sampling time: 0.40 s by 4 slaves
[32m[20230114 18:43:06 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:43:06 @agent_ppo2.py:189][0m |           0.0004 |           3.5921 |           2.5080 |
[32m[20230114 18:43:07 @agent_ppo2.py:189][0m |          -0.0009 |           2.3602 |           2.5057 |
[32m[20230114 18:43:07 @agent_ppo2.py:189][0m |          -0.0024 |           2.0790 |           2.5059 |
[32m[20230114 18:43:07 @agent_ppo2.py:189][0m |          -0.0051 |           1.9715 |           2.5054 |
[32m[20230114 18:43:07 @agent_ppo2.py:189][0m |          -0.0046 |           1.9264 |           2.5047 |
[32m[20230114 18:43:07 @agent_ppo2.py:189][0m |          -0.0063 |           1.7931 |           2.5049 |
[32m[20230114 18:43:07 @agent_ppo2.py:189][0m |          -0.0070 |           1.7499 |           2.5053 |
[32m[20230114 18:43:07 @agent_ppo2.py:189][0m |          -0.0073 |           1.7111 |           2.5052 |
[32m[20230114 18:43:07 @agent_ppo2.py:189][0m |          -0.0076 |           1.6830 |           2.5039 |
[32m[20230114 18:43:08 @agent_ppo2.py:189][0m |          -0.0067 |           1.6124 |           2.5033 |
[32m[20230114 18:43:08 @agent_ppo2.py:134][0m Policy update time: 1.40 s
[32m[20230114 18:43:08 @agent_ppo2.py:142][0m Average TRAINING episode reward: -8.62
[32m[20230114 18:43:08 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 21.48
[32m[20230114 18:43:08 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 17.22
[32m[20230114 18:43:08 @agent_ppo2.py:147][0m Total time:       1.06 min
[32m[20230114 18:43:08 @agent_ppo2.py:149][0m 71680 total steps have happened
[32m[20230114 18:43:08 @agent_ppo2.py:125][0m #------------------------ Iteration 35 --------------------------#
[32m[20230114 18:43:08 @agent_ppo2.py:131][0m Sampling time: 0.33 s by 4 slaves
[32m[20230114 18:43:08 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:43:09 @agent_ppo2.py:189][0m |           0.0004 |          11.9570 |           2.5558 |
[32m[20230114 18:43:09 @agent_ppo2.py:189][0m |          -0.0036 |           8.2892 |           2.5553 |
[32m[20230114 18:43:09 @agent_ppo2.py:189][0m |          -0.0054 |           7.1565 |           2.5551 |
[32m[20230114 18:43:09 @agent_ppo2.py:189][0m |          -0.0068 |           6.7446 |           2.5551 |
[32m[20230114 18:43:09 @agent_ppo2.py:189][0m |          -0.0075 |           6.3599 |           2.5555 |
[32m[20230114 18:43:09 @agent_ppo2.py:189][0m |          -0.0082 |           5.9584 |           2.5564 |
[32m[20230114 18:43:09 @agent_ppo2.py:189][0m |          -0.0088 |           5.8255 |           2.5561 |
[32m[20230114 18:43:09 @agent_ppo2.py:189][0m |          -0.0092 |           5.5374 |           2.5568 |
[32m[20230114 18:43:09 @agent_ppo2.py:189][0m |          -0.0095 |           5.4506 |           2.5579 |
[32m[20230114 18:43:09 @agent_ppo2.py:189][0m |          -0.0095 |           5.2077 |           2.5583 |
[32m[20230114 18:43:09 @agent_ppo2.py:134][0m Policy update time: 1.15 s
[32m[20230114 18:43:10 @agent_ppo2.py:142][0m Average TRAINING episode reward: -26.13
[32m[20230114 18:43:10 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 49.01
[32m[20230114 18:43:10 @agent_ppo2.py:144][0m Average EVALUATION episode reward: -33.27
[32m[20230114 18:43:10 @agent_ppo2.py:147][0m Total time:       1.09 min
[32m[20230114 18:43:10 @agent_ppo2.py:149][0m 73728 total steps have happened
[32m[20230114 18:43:10 @agent_ppo2.py:125][0m #------------------------ Iteration 36 --------------------------#
[32m[20230114 18:43:10 @agent_ppo2.py:131][0m Sampling time: 0.32 s by 4 slaves
[32m[20230114 18:43:10 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:43:10 @agent_ppo2.py:189][0m |          -0.0007 |           6.0962 |           2.5289 |
[32m[20230114 18:43:10 @agent_ppo2.py:189][0m |          -0.0055 |           5.1813 |           2.5264 |
[32m[20230114 18:43:10 @agent_ppo2.py:189][0m |          -0.0062 |           4.9103 |           2.5228 |
[32m[20230114 18:43:10 @agent_ppo2.py:189][0m |          -0.0083 |           4.7015 |           2.5218 |
[32m[20230114 18:43:11 @agent_ppo2.py:189][0m |          -0.0091 |           4.6232 |           2.5213 |
[32m[20230114 18:43:11 @agent_ppo2.py:189][0m |          -0.0098 |           4.5225 |           2.5211 |
[32m[20230114 18:43:11 @agent_ppo2.py:189][0m |          -0.0102 |           4.3522 |           2.5198 |
[32m[20230114 18:43:11 @agent_ppo2.py:189][0m |          -0.0107 |           4.2095 |           2.5207 |
[32m[20230114 18:43:11 @agent_ppo2.py:189][0m |          -0.0106 |           4.1102 |           2.5198 |
[32m[20230114 18:43:11 @agent_ppo2.py:189][0m |          -0.0118 |           4.0319 |           2.5197 |
[32m[20230114 18:43:11 @agent_ppo2.py:134][0m Policy update time: 0.99 s
[32m[20230114 18:43:11 @agent_ppo2.py:142][0m Average TRAINING episode reward: -1.08
[32m[20230114 18:43:11 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 37.90
[32m[20230114 18:43:11 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 133.24
[32m[20230114 18:43:11 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 133.24
[32m[20230114 18:43:11 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 133.24
[32m[20230114 18:43:11 @agent_ppo2.py:147][0m Total time:       1.12 min
[32m[20230114 18:43:11 @agent_ppo2.py:149][0m 75776 total steps have happened
[32m[20230114 18:43:11 @agent_ppo2.py:125][0m #------------------------ Iteration 37 --------------------------#
[32m[20230114 18:43:12 @agent_ppo2.py:131][0m Sampling time: 0.39 s by 4 slaves
[32m[20230114 18:43:12 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:43:12 @agent_ppo2.py:189][0m |           0.0003 |           2.8734 |           2.5087 |
[32m[20230114 18:43:12 @agent_ppo2.py:189][0m |          -0.0036 |           2.0994 |           2.5084 |
[32m[20230114 18:43:12 @agent_ppo2.py:189][0m |          -0.0018 |           1.9529 |           2.5066 |
[32m[20230114 18:43:12 @agent_ppo2.py:189][0m |          -0.0049 |           1.7781 |           2.5071 |
[32m[20230114 18:43:12 @agent_ppo2.py:189][0m |          -0.0061 |           1.6665 |           2.5067 |
[32m[20230114 18:43:12 @agent_ppo2.py:189][0m |          -0.0067 |           1.5826 |           2.5068 |
[32m[20230114 18:43:12 @agent_ppo2.py:189][0m |          -0.0064 |           1.6127 |           2.5071 |
[32m[20230114 18:43:13 @agent_ppo2.py:189][0m |          -0.0083 |           1.4464 |           2.5053 |
[32m[20230114 18:43:13 @agent_ppo2.py:189][0m |          -0.0091 |           1.4823 |           2.5059 |
[32m[20230114 18:43:13 @agent_ppo2.py:189][0m |          -0.0089 |           1.4449 |           2.5060 |
[32m[20230114 18:43:13 @agent_ppo2.py:134][0m Policy update time: 1.11 s
[32m[20230114 18:43:13 @agent_ppo2.py:142][0m Average TRAINING episode reward: 6.46
[32m[20230114 18:43:13 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 46.34
[32m[20230114 18:43:13 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 120.37
[32m[20230114 18:43:13 @agent_ppo2.py:147][0m Total time:       1.15 min
[32m[20230114 18:43:13 @agent_ppo2.py:149][0m 77824 total steps have happened
[32m[20230114 18:43:13 @agent_ppo2.py:125][0m #------------------------ Iteration 38 --------------------------#
[32m[20230114 18:43:13 @agent_ppo2.py:131][0m Sampling time: 0.32 s by 4 slaves
[32m[20230114 18:43:14 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:43:14 @agent_ppo2.py:189][0m |          -0.0037 |           2.3492 |           2.5024 |
[32m[20230114 18:43:14 @agent_ppo2.py:189][0m |          -0.0080 |           2.0008 |           2.5051 |
[32m[20230114 18:43:14 @agent_ppo2.py:189][0m |          -0.0096 |           1.9045 |           2.5052 |
[32m[20230114 18:43:14 @agent_ppo2.py:189][0m |          -0.0109 |           1.8409 |           2.5066 |
[32m[20230114 18:43:14 @agent_ppo2.py:189][0m |          -0.0117 |           1.7926 |           2.5078 |
[32m[20230114 18:43:14 @agent_ppo2.py:189][0m |          -0.0121 |           1.7479 |           2.5091 |
[32m[20230114 18:43:14 @agent_ppo2.py:189][0m |          -0.0126 |           1.7251 |           2.5106 |
[32m[20230114 18:43:14 @agent_ppo2.py:189][0m |          -0.0129 |           1.6960 |           2.5108 |
[32m[20230114 18:43:14 @agent_ppo2.py:189][0m |          -0.0138 |           1.6721 |           2.5121 |
[32m[20230114 18:43:15 @agent_ppo2.py:189][0m |          -0.0139 |           1.6615 |           2.5119 |
[32m[20230114 18:43:15 @agent_ppo2.py:134][0m Policy update time: 1.13 s
[32m[20230114 18:43:15 @agent_ppo2.py:142][0m Average TRAINING episode reward: 10.98
[32m[20230114 18:43:15 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 23.01
[32m[20230114 18:43:15 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 124.73
[32m[20230114 18:43:15 @agent_ppo2.py:147][0m Total time:       1.18 min
[32m[20230114 18:43:15 @agent_ppo2.py:149][0m 79872 total steps have happened
[32m[20230114 18:43:15 @agent_ppo2.py:125][0m #------------------------ Iteration 39 --------------------------#
[32m[20230114 18:43:15 @agent_ppo2.py:131][0m Sampling time: 0.34 s by 4 slaves
[32m[20230114 18:43:15 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:43:15 @agent_ppo2.py:189][0m |          -0.0039 |           0.7644 |           2.5981 |
[32m[20230114 18:43:15 @agent_ppo2.py:189][0m |          -0.0068 |           0.6925 |           2.6002 |
[32m[20230114 18:43:16 @agent_ppo2.py:189][0m |          -0.0081 |           0.6807 |           2.6020 |
[32m[20230114 18:43:16 @agent_ppo2.py:189][0m |          -0.0090 |           0.6711 |           2.6054 |
[32m[20230114 18:43:16 @agent_ppo2.py:189][0m |          -0.0095 |           0.6641 |           2.6067 |
[32m[20230114 18:43:16 @agent_ppo2.py:189][0m |          -0.0099 |           0.6606 |           2.6109 |
[32m[20230114 18:43:16 @agent_ppo2.py:189][0m |          -0.0105 |           0.6535 |           2.6130 |
[32m[20230114 18:43:16 @agent_ppo2.py:189][0m |          -0.0107 |           0.6494 |           2.6152 |
[32m[20230114 18:43:16 @agent_ppo2.py:189][0m |          -0.0113 |           0.6461 |           2.6188 |
[32m[20230114 18:43:16 @agent_ppo2.py:189][0m |          -0.0115 |           0.6430 |           2.6185 |
[32m[20230114 18:43:16 @agent_ppo2.py:134][0m Policy update time: 1.08 s
[32m[20230114 18:43:17 @agent_ppo2.py:142][0m Average TRAINING episode reward: 48.46
[32m[20230114 18:43:17 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 60.42
[32m[20230114 18:43:17 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 131.74
[32m[20230114 18:43:17 @agent_ppo2.py:147][0m Total time:       1.21 min
[32m[20230114 18:43:17 @agent_ppo2.py:149][0m 81920 total steps have happened
[32m[20230114 18:43:17 @agent_ppo2.py:125][0m #------------------------ Iteration 40 --------------------------#
[32m[20230114 18:43:17 @agent_ppo2.py:131][0m Sampling time: 0.31 s by 4 slaves
[32m[20230114 18:43:17 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:43:17 @agent_ppo2.py:189][0m |          -0.0020 |           0.8769 |           2.6364 |
[32m[20230114 18:43:17 @agent_ppo2.py:189][0m |          -0.0060 |           0.7947 |           2.6362 |
[32m[20230114 18:43:17 @agent_ppo2.py:189][0m |          -0.0078 |           0.7714 |           2.6375 |
[32m[20230114 18:43:17 @agent_ppo2.py:189][0m |          -0.0089 |           0.7533 |           2.6375 |
[32m[20230114 18:43:17 @agent_ppo2.py:189][0m |          -0.0092 |           0.7428 |           2.6400 |
[32m[20230114 18:43:18 @agent_ppo2.py:189][0m |          -0.0101 |           0.7307 |           2.6409 |
[32m[20230114 18:43:18 @agent_ppo2.py:189][0m |          -0.0108 |           0.7205 |           2.6435 |
[32m[20230114 18:43:18 @agent_ppo2.py:189][0m |          -0.0111 |           0.7138 |           2.6452 |
[32m[20230114 18:43:18 @agent_ppo2.py:189][0m |          -0.0113 |           0.7060 |           2.6454 |
[32m[20230114 18:43:18 @agent_ppo2.py:189][0m |          -0.0120 |           0.7012 |           2.6487 |
[32m[20230114 18:43:18 @agent_ppo2.py:134][0m Policy update time: 1.15 s
[32m[20230114 18:43:18 @agent_ppo2.py:142][0m Average TRAINING episode reward: 53.46
[32m[20230114 18:43:18 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 62.32
[32m[20230114 18:43:18 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 134.43
[32m[20230114 18:43:18 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 134.43
[32m[20230114 18:43:18 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 134.43
[32m[20230114 18:43:18 @agent_ppo2.py:147][0m Total time:       1.24 min
[32m[20230114 18:43:18 @agent_ppo2.py:149][0m 83968 total steps have happened
[32m[20230114 18:43:18 @agent_ppo2.py:125][0m #------------------------ Iteration 41 --------------------------#
[32m[20230114 18:43:19 @agent_ppo2.py:131][0m Sampling time: 0.34 s by 4 slaves
[32m[20230114 18:43:19 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:43:19 @agent_ppo2.py:189][0m |          -0.0007 |           3.0072 |           2.7286 |
[32m[20230114 18:43:19 @agent_ppo2.py:189][0m |          -0.0042 |           1.9553 |           2.7254 |
[32m[20230114 18:43:19 @agent_ppo2.py:189][0m |          -0.0050 |           1.7551 |           2.7215 |
[32m[20230114 18:43:19 @agent_ppo2.py:189][0m |          -0.0054 |           1.6138 |           2.7197 |
[32m[20230114 18:43:19 @agent_ppo2.py:189][0m |          -0.0064 |           1.5090 |           2.7187 |
[32m[20230114 18:43:19 @agent_ppo2.py:189][0m |          -0.0062 |           1.4465 |           2.7186 |
[32m[20230114 18:43:20 @agent_ppo2.py:189][0m |          -0.0072 |           1.3826 |           2.7173 |
[32m[20230114 18:43:20 @agent_ppo2.py:189][0m |          -0.0082 |           1.3448 |           2.7169 |
[32m[20230114 18:43:20 @agent_ppo2.py:189][0m |          -0.0081 |           1.3425 |           2.7151 |
[32m[20230114 18:43:20 @agent_ppo2.py:189][0m |          -0.0090 |           1.2760 |           2.7143 |
[32m[20230114 18:43:20 @agent_ppo2.py:134][0m Policy update time: 1.13 s
[32m[20230114 18:43:20 @agent_ppo2.py:142][0m Average TRAINING episode reward: 56.96
[32m[20230114 18:43:20 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 88.32
[32m[20230114 18:43:20 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 140.96
[32m[20230114 18:43:20 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 140.96
[32m[20230114 18:43:20 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 140.96
[32m[20230114 18:43:20 @agent_ppo2.py:147][0m Total time:       1.27 min
[32m[20230114 18:43:20 @agent_ppo2.py:149][0m 86016 total steps have happened
[32m[20230114 18:43:20 @agent_ppo2.py:125][0m #------------------------ Iteration 42 --------------------------#
[32m[20230114 18:43:20 @agent_ppo2.py:131][0m Sampling time: 0.31 s by 4 slaves
[32m[20230114 18:43:21 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:43:21 @agent_ppo2.py:189][0m |          -0.0014 |           5.4205 |           2.7062 |
[32m[20230114 18:43:21 @agent_ppo2.py:189][0m |          -0.0025 |           4.3433 |           2.7032 |
[32m[20230114 18:43:21 @agent_ppo2.py:189][0m |          -0.0052 |           3.8866 |           2.7012 |
[32m[20230114 18:43:21 @agent_ppo2.py:189][0m |          -0.0043 |           3.7348 |           2.7030 |
[32m[20230114 18:43:21 @agent_ppo2.py:189][0m |          -0.0065 |           3.5335 |           2.7021 |
[32m[20230114 18:43:21 @agent_ppo2.py:189][0m |          -0.0070 |           3.4142 |           2.7026 |
[32m[20230114 18:43:21 @agent_ppo2.py:189][0m |          -0.0065 |           3.2850 |           2.7025 |
[32m[20230114 18:43:21 @agent_ppo2.py:189][0m |          -0.0052 |           3.2787 |           2.7028 |
[32m[20230114 18:43:21 @agent_ppo2.py:189][0m |          -0.0059 |           3.2229 |           2.7014 |
[32m[20230114 18:43:21 @agent_ppo2.py:189][0m |          -0.0079 |           3.0215 |           2.7026 |
[32m[20230114 18:43:21 @agent_ppo2.py:134][0m Policy update time: 0.97 s
[32m[20230114 18:43:22 @agent_ppo2.py:142][0m Average TRAINING episode reward: 51.70
[32m[20230114 18:43:22 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 93.46
[32m[20230114 18:43:22 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 170.17
[32m[20230114 18:43:22 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 170.17
[32m[20230114 18:43:22 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 170.17
[32m[20230114 18:43:22 @agent_ppo2.py:147][0m Total time:       1.29 min
[32m[20230114 18:43:22 @agent_ppo2.py:149][0m 88064 total steps have happened
[32m[20230114 18:43:22 @agent_ppo2.py:125][0m #------------------------ Iteration 43 --------------------------#
[32m[20230114 18:43:22 @agent_ppo2.py:131][0m Sampling time: 0.32 s by 4 slaves
[32m[20230114 18:43:22 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:43:22 @agent_ppo2.py:189][0m |           0.0007 |           1.5043 |           2.7582 |
[32m[20230114 18:43:22 @agent_ppo2.py:189][0m |          -0.0014 |           1.2475 |           2.7559 |
[32m[20230114 18:43:23 @agent_ppo2.py:189][0m |          -0.0033 |           1.2090 |           2.7572 |
[32m[20230114 18:43:23 @agent_ppo2.py:189][0m |          -0.0048 |           1.1691 |           2.7588 |
[32m[20230114 18:43:23 @agent_ppo2.py:189][0m |          -0.0055 |           1.1447 |           2.7602 |
[32m[20230114 18:43:23 @agent_ppo2.py:189][0m |          -0.0066 |           1.1287 |           2.7610 |
[32m[20230114 18:43:23 @agent_ppo2.py:189][0m |          -0.0074 |           1.1125 |           2.7632 |
[32m[20230114 18:43:23 @agent_ppo2.py:189][0m |          -0.0078 |           1.1027 |           2.7646 |
[32m[20230114 18:43:23 @agent_ppo2.py:189][0m |          -0.0087 |           1.0879 |           2.7651 |
[32m[20230114 18:43:23 @agent_ppo2.py:189][0m |          -0.0090 |           1.0851 |           2.7695 |
[32m[20230114 18:43:23 @agent_ppo2.py:134][0m Policy update time: 1.13 s
[32m[20230114 18:43:23 @agent_ppo2.py:142][0m Average TRAINING episode reward: 72.65
[32m[20230114 18:43:23 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 82.85
[32m[20230114 18:43:23 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 3.43
[32m[20230114 18:43:23 @agent_ppo2.py:147][0m Total time:       1.32 min
[32m[20230114 18:43:23 @agent_ppo2.py:149][0m 90112 total steps have happened
[32m[20230114 18:43:23 @agent_ppo2.py:125][0m #------------------------ Iteration 44 --------------------------#
[32m[20230114 18:43:24 @agent_ppo2.py:131][0m Sampling time: 0.31 s by 4 slaves
[32m[20230114 18:43:24 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:43:24 @agent_ppo2.py:189][0m |          -0.0021 |           0.8510 |           2.7921 |
[32m[20230114 18:43:24 @agent_ppo2.py:189][0m |          -0.0065 |           0.7571 |           2.7892 |
[32m[20230114 18:43:24 @agent_ppo2.py:189][0m |          -0.0074 |           0.7382 |           2.7921 |
[32m[20230114 18:43:24 @agent_ppo2.py:189][0m |          -0.0083 |           0.7277 |           2.7930 |
[32m[20230114 18:43:24 @agent_ppo2.py:189][0m |          -0.0090 |           0.7192 |           2.7953 |
[32m[20230114 18:43:24 @agent_ppo2.py:189][0m |          -0.0095 |           0.7113 |           2.7972 |
[32m[20230114 18:43:25 @agent_ppo2.py:189][0m |          -0.0101 |           0.7055 |           2.7988 |
[32m[20230114 18:43:25 @agent_ppo2.py:189][0m |          -0.0105 |           0.7001 |           2.8009 |
[32m[20230114 18:43:25 @agent_ppo2.py:189][0m |          -0.0109 |           0.6949 |           2.8023 |
[32m[20230114 18:43:25 @agent_ppo2.py:189][0m |          -0.0108 |           0.6904 |           2.8040 |
[32m[20230114 18:43:25 @agent_ppo2.py:134][0m Policy update time: 1.19 s
[32m[20230114 18:43:25 @agent_ppo2.py:142][0m Average TRAINING episode reward: 85.41
[32m[20230114 18:43:25 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 90.62
[32m[20230114 18:43:25 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 156.24
[32m[20230114 18:43:25 @agent_ppo2.py:147][0m Total time:       1.35 min
[32m[20230114 18:43:25 @agent_ppo2.py:149][0m 92160 total steps have happened
[32m[20230114 18:43:25 @agent_ppo2.py:125][0m #------------------------ Iteration 45 --------------------------#
[32m[20230114 18:43:26 @agent_ppo2.py:131][0m Sampling time: 0.35 s by 4 slaves
[32m[20230114 18:43:26 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:43:26 @agent_ppo2.py:189][0m |          -0.0001 |           3.5380 |           2.8209 |
[32m[20230114 18:43:26 @agent_ppo2.py:189][0m |          -0.0046 |           2.4563 |           2.8189 |
[32m[20230114 18:43:26 @agent_ppo2.py:189][0m |          -0.0061 |           2.3419 |           2.8158 |
[32m[20230114 18:43:26 @agent_ppo2.py:189][0m |          -0.0072 |           2.2394 |           2.8158 |
[32m[20230114 18:43:26 @agent_ppo2.py:189][0m |          -0.0080 |           2.2152 |           2.8154 |
[32m[20230114 18:43:26 @agent_ppo2.py:189][0m |          -0.0086 |           2.1767 |           2.8153 |
[32m[20230114 18:43:26 @agent_ppo2.py:189][0m |          -0.0092 |           2.2193 |           2.8152 |
[32m[20230114 18:43:26 @agent_ppo2.py:189][0m |          -0.0097 |           2.1541 |           2.8155 |
[32m[20230114 18:43:27 @agent_ppo2.py:189][0m |          -0.0098 |           2.0869 |           2.8154 |
[32m[20230114 18:43:27 @agent_ppo2.py:189][0m |          -0.0103 |           2.0150 |           2.8132 |
[32m[20230114 18:43:27 @agent_ppo2.py:134][0m Policy update time: 1.12 s
[32m[20230114 18:43:27 @agent_ppo2.py:142][0m Average TRAINING episode reward: 51.83
[32m[20230114 18:43:27 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 101.33
[32m[20230114 18:43:27 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 169.58
[32m[20230114 18:43:27 @agent_ppo2.py:147][0m Total time:       1.38 min
[32m[20230114 18:43:27 @agent_ppo2.py:149][0m 94208 total steps have happened
[32m[20230114 18:43:27 @agent_ppo2.py:125][0m #------------------------ Iteration 46 --------------------------#
[32m[20230114 18:43:27 @agent_ppo2.py:131][0m Sampling time: 0.33 s by 4 slaves
[32m[20230114 18:43:27 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:43:28 @agent_ppo2.py:189][0m |           0.0002 |           4.2492 |           2.7907 |
[32m[20230114 18:43:28 @agent_ppo2.py:189][0m |          -0.0031 |           2.9699 |           2.7890 |
[32m[20230114 18:43:28 @agent_ppo2.py:189][0m |          -0.0053 |           2.9120 |           2.7902 |
[32m[20230114 18:43:28 @agent_ppo2.py:189][0m |          -0.0057 |           2.6039 |           2.7888 |
[32m[20230114 18:43:28 @agent_ppo2.py:189][0m |          -0.0072 |           2.4349 |           2.7902 |
[32m[20230114 18:43:28 @agent_ppo2.py:189][0m |          -0.0067 |           2.3600 |           2.7914 |
[32m[20230114 18:43:28 @agent_ppo2.py:189][0m |          -0.0083 |           2.2856 |           2.7931 |
[32m[20230114 18:43:28 @agent_ppo2.py:189][0m |          -0.0080 |           2.1969 |           2.7938 |
[32m[20230114 18:43:28 @agent_ppo2.py:189][0m |          -0.0093 |           2.2226 |           2.7923 |
[32m[20230114 18:43:28 @agent_ppo2.py:189][0m |          -0.0095 |           2.2211 |           2.7939 |
[32m[20230114 18:43:28 @agent_ppo2.py:134][0m Policy update time: 1.02 s
[32m[20230114 18:43:29 @agent_ppo2.py:142][0m Average TRAINING episode reward: 58.97
[32m[20230114 18:43:29 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 103.46
[32m[20230114 18:43:29 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 158.67
[32m[20230114 18:43:29 @agent_ppo2.py:147][0m Total time:       1.41 min
[32m[20230114 18:43:29 @agent_ppo2.py:149][0m 96256 total steps have happened
[32m[20230114 18:43:29 @agent_ppo2.py:125][0m #------------------------ Iteration 47 --------------------------#
[32m[20230114 18:43:29 @agent_ppo2.py:131][0m Sampling time: 0.33 s by 4 slaves
[32m[20230114 18:43:29 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:43:29 @agent_ppo2.py:189][0m |          -0.0050 |           7.4919 |           2.7515 |
[32m[20230114 18:43:29 @agent_ppo2.py:189][0m |          -0.0039 |           5.9304 |           2.7494 |
[32m[20230114 18:43:29 @agent_ppo2.py:189][0m |          -0.0116 |           5.3336 |           2.7462 |
[32m[20230114 18:43:29 @agent_ppo2.py:189][0m |          -0.0084 |           5.0042 |           2.7431 |
[32m[20230114 18:43:29 @agent_ppo2.py:189][0m |          -0.0102 |           4.7045 |           2.7440 |
[32m[20230114 18:43:30 @agent_ppo2.py:189][0m |          -0.0109 |           4.5202 |           2.7451 |
[32m[20230114 18:43:30 @agent_ppo2.py:189][0m |          -0.0116 |           4.3746 |           2.7440 |
[32m[20230114 18:43:30 @agent_ppo2.py:189][0m |          -0.0145 |           4.3799 |           2.7448 |
[32m[20230114 18:43:30 @agent_ppo2.py:189][0m |          -0.0172 |           4.1625 |           2.7460 |
[32m[20230114 18:43:30 @agent_ppo2.py:189][0m |          -0.0105 |           4.0228 |           2.7445 |
[32m[20230114 18:43:30 @agent_ppo2.py:134][0m Policy update time: 0.86 s
[32m[20230114 18:43:30 @agent_ppo2.py:142][0m Average TRAINING episode reward: 29.71
[32m[20230114 18:43:30 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 109.45
[32m[20230114 18:43:30 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 179.89
[32m[20230114 18:43:30 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 179.89
[32m[20230114 18:43:30 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 179.89
[32m[20230114 18:43:30 @agent_ppo2.py:147][0m Total time:       1.43 min
[32m[20230114 18:43:30 @agent_ppo2.py:149][0m 98304 total steps have happened
[32m[20230114 18:43:30 @agent_ppo2.py:125][0m #------------------------ Iteration 48 --------------------------#
[32m[20230114 18:43:30 @agent_ppo2.py:131][0m Sampling time: 0.31 s by 4 slaves
[32m[20230114 18:43:31 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:43:31 @agent_ppo2.py:189][0m |          -0.0013 |           1.1754 |           2.8117 |
[32m[20230114 18:43:31 @agent_ppo2.py:189][0m |          -0.0058 |           0.9970 |           2.8084 |
[32m[20230114 18:43:31 @agent_ppo2.py:189][0m |          -0.0075 |           0.9466 |           2.8126 |
[32m[20230114 18:43:31 @agent_ppo2.py:189][0m |          -0.0087 |           0.9129 |           2.8137 |
[32m[20230114 18:43:31 @agent_ppo2.py:189][0m |          -0.0094 |           0.8911 |           2.8177 |
[32m[20230114 18:43:31 @agent_ppo2.py:189][0m |          -0.0103 |           0.8767 |           2.8187 |
[32m[20230114 18:43:31 @agent_ppo2.py:189][0m |          -0.0103 |           0.8633 |           2.8202 |
[32m[20230114 18:43:31 @agent_ppo2.py:189][0m |          -0.0111 |           0.8526 |           2.8223 |
[32m[20230114 18:43:31 @agent_ppo2.py:189][0m |          -0.0118 |           0.8451 |           2.8239 |
[32m[20230114 18:43:32 @agent_ppo2.py:189][0m |          -0.0119 |           0.8372 |           2.8258 |
[32m[20230114 18:43:32 @agent_ppo2.py:134][0m Policy update time: 1.14 s
[32m[20230114 18:43:32 @agent_ppo2.py:142][0m Average TRAINING episode reward: 104.41
[32m[20230114 18:43:32 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 120.72
[32m[20230114 18:43:32 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 193.84
[32m[20230114 18:43:32 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 193.84
[32m[20230114 18:43:32 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 193.84
[32m[20230114 18:43:32 @agent_ppo2.py:147][0m Total time:       1.46 min
[32m[20230114 18:43:32 @agent_ppo2.py:149][0m 100352 total steps have happened
[32m[20230114 18:43:32 @agent_ppo2.py:125][0m #------------------------ Iteration 49 --------------------------#
[32m[20230114 18:43:32 @agent_ppo2.py:131][0m Sampling time: 0.39 s by 4 slaves
[32m[20230114 18:43:32 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:43:32 @agent_ppo2.py:189][0m |           0.0005 |           1.2469 |           2.9341 |
[32m[20230114 18:43:33 @agent_ppo2.py:189][0m |          -0.0022 |           1.0906 |           2.9384 |
[32m[20230114 18:43:33 @agent_ppo2.py:189][0m |          -0.0038 |           1.0594 |           2.9424 |
[32m[20230114 18:43:33 @agent_ppo2.py:189][0m |          -0.0050 |           1.0408 |           2.9451 |
[32m[20230114 18:43:33 @agent_ppo2.py:189][0m |          -0.0060 |           1.0269 |           2.9465 |
[32m[20230114 18:43:33 @agent_ppo2.py:189][0m |          -0.0060 |           1.0093 |           2.9493 |
[32m[20230114 18:43:33 @agent_ppo2.py:189][0m |          -0.0070 |           0.9996 |           2.9511 |
[32m[20230114 18:43:33 @agent_ppo2.py:189][0m |          -0.0076 |           0.9911 |           2.9562 |
[32m[20230114 18:43:33 @agent_ppo2.py:189][0m |          -0.0081 |           0.9794 |           2.9582 |
[32m[20230114 18:43:33 @agent_ppo2.py:189][0m |          -0.0085 |           0.9711 |           2.9632 |
[32m[20230114 18:43:33 @agent_ppo2.py:134][0m Policy update time: 1.08 s
[32m[20230114 18:43:34 @agent_ppo2.py:142][0m Average TRAINING episode reward: 108.22
[32m[20230114 18:43:34 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 124.16
[32m[20230114 18:43:34 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 182.63
[32m[20230114 18:43:34 @agent_ppo2.py:147][0m Total time:       1.49 min
[32m[20230114 18:43:34 @agent_ppo2.py:149][0m 102400 total steps have happened
[32m[20230114 18:43:34 @agent_ppo2.py:125][0m #------------------------ Iteration 50 --------------------------#
[32m[20230114 18:43:34 @agent_ppo2.py:131][0m Sampling time: 0.31 s by 4 slaves
[32m[20230114 18:43:34 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:43:34 @agent_ppo2.py:189][0m |          -0.0010 |           1.0259 |           2.9736 |
[32m[20230114 18:43:34 @agent_ppo2.py:189][0m |          -0.0044 |           0.9836 |           2.9756 |
[32m[20230114 18:43:34 @agent_ppo2.py:189][0m |          -0.0060 |           0.9694 |           2.9760 |
[32m[20230114 18:43:34 @agent_ppo2.py:189][0m |          -0.0068 |           0.9613 |           2.9786 |
[32m[20230114 18:43:35 @agent_ppo2.py:189][0m |          -0.0078 |           0.9546 |           2.9796 |
[32m[20230114 18:43:35 @agent_ppo2.py:189][0m |          -0.0084 |           0.9520 |           2.9807 |
[32m[20230114 18:43:35 @agent_ppo2.py:189][0m |          -0.0082 |           0.9396 |           2.9817 |
[32m[20230114 18:43:35 @agent_ppo2.py:189][0m |          -0.0091 |           0.9380 |           2.9848 |
[32m[20230114 18:43:35 @agent_ppo2.py:189][0m |          -0.0092 |           0.9301 |           2.9854 |
[32m[20230114 18:43:35 @agent_ppo2.py:189][0m |          -0.0096 |           0.9270 |           2.9864 |
[32m[20230114 18:43:35 @agent_ppo2.py:134][0m Policy update time: 1.12 s
[32m[20230114 18:43:35 @agent_ppo2.py:142][0m Average TRAINING episode reward: 108.19
[32m[20230114 18:43:35 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 115.67
[32m[20230114 18:43:35 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 200.08
[32m[20230114 18:43:35 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 200.08
[32m[20230114 18:43:35 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 200.08
[32m[20230114 18:43:35 @agent_ppo2.py:147][0m Total time:       1.52 min
[32m[20230114 18:43:35 @agent_ppo2.py:149][0m 104448 total steps have happened
[32m[20230114 18:43:35 @agent_ppo2.py:125][0m #------------------------ Iteration 51 --------------------------#
[32m[20230114 18:43:36 @agent_ppo2.py:131][0m Sampling time: 0.39 s by 4 slaves
[32m[20230114 18:43:36 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:43:36 @agent_ppo2.py:189][0m |           0.0001 |           3.4053 |           3.0197 |
[32m[20230114 18:43:36 @agent_ppo2.py:189][0m |          -0.0017 |           2.2125 |           3.0170 |
[32m[20230114 18:43:36 @agent_ppo2.py:189][0m |          -0.0031 |           1.9438 |           3.0178 |
[32m[20230114 18:43:36 @agent_ppo2.py:189][0m |          -0.0031 |           1.8156 |           3.0163 |
[32m[20230114 18:43:36 @agent_ppo2.py:189][0m |          -0.0046 |           1.8340 |           3.0175 |
[32m[20230114 18:43:37 @agent_ppo2.py:189][0m |          -0.0056 |           1.7571 |           3.0184 |
[32m[20230114 18:43:37 @agent_ppo2.py:189][0m |          -0.0058 |           1.7316 |           3.0176 |
[32m[20230114 18:43:37 @agent_ppo2.py:189][0m |          -0.0048 |           1.7105 |           3.0200 |
[32m[20230114 18:43:37 @agent_ppo2.py:189][0m |          -0.0065 |           1.5807 |           3.0194 |
[32m[20230114 18:43:37 @agent_ppo2.py:189][0m |          -0.0056 |           1.5732 |           3.0212 |
[32m[20230114 18:43:37 @agent_ppo2.py:134][0m Policy update time: 1.21 s
[32m[20230114 18:43:37 @agent_ppo2.py:142][0m Average TRAINING episode reward: 82.61
[32m[20230114 18:43:37 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 131.57
[32m[20230114 18:43:37 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 147.63
[32m[20230114 18:43:37 @agent_ppo2.py:147][0m Total time:       1.55 min
[32m[20230114 18:43:37 @agent_ppo2.py:149][0m 106496 total steps have happened
[32m[20230114 18:43:37 @agent_ppo2.py:125][0m #------------------------ Iteration 52 --------------------------#
[32m[20230114 18:43:38 @agent_ppo2.py:131][0m Sampling time: 0.31 s by 4 slaves
[32m[20230114 18:43:38 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:43:38 @agent_ppo2.py:189][0m |          -0.0023 |           1.2985 |           3.0098 |
[32m[20230114 18:43:38 @agent_ppo2.py:189][0m |          -0.0051 |           1.2059 |           3.0077 |
[32m[20230114 18:43:38 @agent_ppo2.py:189][0m |          -0.0067 |           1.1914 |           3.0083 |
[32m[20230114 18:43:38 @agent_ppo2.py:189][0m |          -0.0074 |           1.1823 |           3.0104 |
[32m[20230114 18:43:38 @agent_ppo2.py:189][0m |          -0.0078 |           1.1714 |           3.0102 |
[32m[20230114 18:43:38 @agent_ppo2.py:189][0m |          -0.0085 |           1.1682 |           3.0142 |
[32m[20230114 18:43:38 @agent_ppo2.py:189][0m |          -0.0090 |           1.1626 |           3.0154 |
[32m[20230114 18:43:39 @agent_ppo2.py:189][0m |          -0.0094 |           1.1536 |           3.0162 |
[32m[20230114 18:43:39 @agent_ppo2.py:189][0m |          -0.0089 |           1.1476 |           3.0186 |
[32m[20230114 18:43:39 @agent_ppo2.py:189][0m |          -0.0093 |           1.1454 |           3.0193 |
[32m[20230114 18:43:39 @agent_ppo2.py:134][0m Policy update time: 1.08 s
[32m[20230114 18:43:39 @agent_ppo2.py:142][0m Average TRAINING episode reward: 122.97
[32m[20230114 18:43:39 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 131.74
[32m[20230114 18:43:39 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 214.87
[32m[20230114 18:43:39 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 214.87
[32m[20230114 18:43:39 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 214.87
[32m[20230114 18:43:39 @agent_ppo2.py:147][0m Total time:       1.58 min
[32m[20230114 18:43:39 @agent_ppo2.py:149][0m 108544 total steps have happened
[32m[20230114 18:43:39 @agent_ppo2.py:125][0m #------------------------ Iteration 53 --------------------------#
[32m[20230114 18:43:39 @agent_ppo2.py:131][0m Sampling time: 0.30 s by 4 slaves
[32m[20230114 18:43:39 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:43:40 @agent_ppo2.py:189][0m |           0.0053 |           4.9217 |           2.9988 |
[32m[20230114 18:43:40 @agent_ppo2.py:189][0m |          -0.0039 |           4.0488 |           2.9957 |
[32m[20230114 18:43:40 @agent_ppo2.py:189][0m |          -0.0058 |           3.5768 |           2.9926 |
[32m[20230114 18:43:40 @agent_ppo2.py:189][0m |          -0.0031 |           3.5277 |           2.9924 |
[32m[20230114 18:43:40 @agent_ppo2.py:189][0m |          -0.0063 |           3.2654 |           2.9934 |
[32m[20230114 18:43:40 @agent_ppo2.py:189][0m |          -0.0071 |           3.1712 |           2.9928 |
[32m[20230114 18:43:40 @agent_ppo2.py:189][0m |          -0.0076 |           2.9673 |           2.9915 |
[32m[20230114 18:43:40 @agent_ppo2.py:189][0m |          -0.0074 |           2.9595 |           2.9923 |
[32m[20230114 18:43:40 @agent_ppo2.py:189][0m |          -0.0074 |           2.8870 |           2.9915 |
[32m[20230114 18:43:40 @agent_ppo2.py:189][0m |          -0.0083 |           2.8107 |           2.9942 |
[32m[20230114 18:43:40 @agent_ppo2.py:134][0m Policy update time: 0.95 s
[32m[20230114 18:43:41 @agent_ppo2.py:142][0m Average TRAINING episode reward: 91.70
[32m[20230114 18:43:41 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 147.28
[32m[20230114 18:43:41 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 209.15
[32m[20230114 18:43:41 @agent_ppo2.py:147][0m Total time:       1.61 min
[32m[20230114 18:43:41 @agent_ppo2.py:149][0m 110592 total steps have happened
[32m[20230114 18:43:41 @agent_ppo2.py:125][0m #------------------------ Iteration 54 --------------------------#
[32m[20230114 18:43:41 @agent_ppo2.py:131][0m Sampling time: 0.31 s by 4 slaves
[32m[20230114 18:43:41 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:43:41 @agent_ppo2.py:189][0m |          -0.0024 |           1.3589 |           3.0796 |
[32m[20230114 18:43:41 @agent_ppo2.py:189][0m |          -0.0057 |           1.2299 |           3.0787 |
[32m[20230114 18:43:41 @agent_ppo2.py:189][0m |          -0.0071 |           1.1922 |           3.0813 |
[32m[20230114 18:43:41 @agent_ppo2.py:189][0m |          -0.0080 |           1.1606 |           3.0851 |
[32m[20230114 18:43:41 @agent_ppo2.py:189][0m |          -0.0086 |           1.1440 |           3.0853 |
[32m[20230114 18:43:42 @agent_ppo2.py:189][0m |          -0.0090 |           1.1299 |           3.0880 |
[32m[20230114 18:43:42 @agent_ppo2.py:189][0m |          -0.0097 |           1.1201 |           3.0902 |
[32m[20230114 18:43:42 @agent_ppo2.py:189][0m |          -0.0099 |           1.1097 |           3.0934 |
[32m[20230114 18:43:42 @agent_ppo2.py:189][0m |          -0.0103 |           1.1005 |           3.0951 |
[32m[20230114 18:43:42 @agent_ppo2.py:189][0m |          -0.0108 |           1.0955 |           3.0968 |
[32m[20230114 18:43:42 @agent_ppo2.py:134][0m Policy update time: 1.07 s
[32m[20230114 18:43:42 @agent_ppo2.py:142][0m Average TRAINING episode reward: 139.45
[32m[20230114 18:43:42 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 142.49
[32m[20230114 18:43:42 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 147.73
[32m[20230114 18:43:42 @agent_ppo2.py:147][0m Total time:       1.64 min
[32m[20230114 18:43:42 @agent_ppo2.py:149][0m 112640 total steps have happened
[32m[20230114 18:43:42 @agent_ppo2.py:125][0m #------------------------ Iteration 55 --------------------------#
[32m[20230114 18:43:43 @agent_ppo2.py:131][0m Sampling time: 0.32 s by 4 slaves
[32m[20230114 18:43:43 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:43:43 @agent_ppo2.py:189][0m |           0.0039 |          13.0225 |           3.1200 |
[32m[20230114 18:43:43 @agent_ppo2.py:189][0m |          -0.0026 |           8.5983 |           3.1169 |
[32m[20230114 18:43:43 @agent_ppo2.py:189][0m |          -0.0051 |           7.4166 |           3.1149 |
[32m[20230114 18:43:43 @agent_ppo2.py:189][0m |          -0.0057 |           7.1712 |           3.1148 |
[32m[20230114 18:43:43 @agent_ppo2.py:189][0m |           0.0011 |           6.6263 |           3.1122 |
[32m[20230114 18:43:43 @agent_ppo2.py:189][0m |          -0.0075 |           6.1346 |           3.1126 |
[32m[20230114 18:43:43 @agent_ppo2.py:189][0m |          -0.0087 |           6.0527 |           3.1120 |
[32m[20230114 18:43:43 @agent_ppo2.py:189][0m |          -0.0093 |           5.8761 |           3.1111 |
[32m[20230114 18:43:44 @agent_ppo2.py:189][0m |          -0.0092 |           5.5165 |           3.1092 |
[32m[20230114 18:43:44 @agent_ppo2.py:189][0m |          -0.0030 |           5.5309 |           3.1103 |
[32m[20230114 18:43:44 @agent_ppo2.py:134][0m Policy update time: 0.99 s
[32m[20230114 18:43:44 @agent_ppo2.py:142][0m Average TRAINING episode reward: 26.72
[32m[20230114 18:43:44 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 129.94
[32m[20230114 18:43:44 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 206.10
[32m[20230114 18:43:44 @agent_ppo2.py:147][0m Total time:       1.66 min
[32m[20230114 18:43:44 @agent_ppo2.py:149][0m 114688 total steps have happened
[32m[20230114 18:43:44 @agent_ppo2.py:125][0m #------------------------ Iteration 56 --------------------------#
[32m[20230114 18:43:44 @agent_ppo2.py:131][0m Sampling time: 0.31 s by 4 slaves
[32m[20230114 18:43:44 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:43:44 @agent_ppo2.py:189][0m |           0.0014 |           9.8126 |           3.1317 |
[32m[20230114 18:43:44 @agent_ppo2.py:189][0m |          -0.0059 |           8.7989 |           3.1239 |
[32m[20230114 18:43:45 @agent_ppo2.py:189][0m |          -0.0067 |           8.2085 |           3.1210 |
[32m[20230114 18:43:45 @agent_ppo2.py:189][0m |          -0.0071 |           8.0859 |           3.1178 |
[32m[20230114 18:43:45 @agent_ppo2.py:189][0m |          -0.0074 |           7.7625 |           3.1159 |
[32m[20230114 18:43:45 @agent_ppo2.py:189][0m |          -0.0088 |           7.5013 |           3.1168 |
[32m[20230114 18:43:45 @agent_ppo2.py:189][0m |          -0.0105 |           7.3924 |           3.1157 |
[32m[20230114 18:43:45 @agent_ppo2.py:189][0m |          -0.0091 |           7.2008 |           3.1143 |
[32m[20230114 18:43:45 @agent_ppo2.py:189][0m |          -0.0103 |           7.1403 |           3.1134 |
[32m[20230114 18:43:45 @agent_ppo2.py:189][0m |          -0.0107 |           7.0204 |           3.1136 |
[32m[20230114 18:43:45 @agent_ppo2.py:134][0m Policy update time: 1.03 s
[32m[20230114 18:43:46 @agent_ppo2.py:142][0m Average TRAINING episode reward: 101.65
[32m[20230114 18:43:46 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 151.17
[32m[20230114 18:43:46 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 197.49
[32m[20230114 18:43:46 @agent_ppo2.py:147][0m Total time:       1.69 min
[32m[20230114 18:43:46 @agent_ppo2.py:149][0m 116736 total steps have happened
[32m[20230114 18:43:46 @agent_ppo2.py:125][0m #------------------------ Iteration 57 --------------------------#
[32m[20230114 18:43:46 @agent_ppo2.py:131][0m Sampling time: 0.31 s by 4 slaves
[32m[20230114 18:43:46 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:43:46 @agent_ppo2.py:189][0m |          -0.0003 |           2.1546 |           3.1386 |
[32m[20230114 18:43:46 @agent_ppo2.py:189][0m |          -0.0034 |           1.7535 |           3.1354 |
[32m[20230114 18:43:46 @agent_ppo2.py:189][0m |          -0.0046 |           1.6854 |           3.1372 |
[32m[20230114 18:43:46 @agent_ppo2.py:189][0m |          -0.0053 |           1.6473 |           3.1362 |
[32m[20230114 18:43:46 @agent_ppo2.py:189][0m |          -0.0059 |           1.6226 |           3.1344 |
[32m[20230114 18:43:47 @agent_ppo2.py:189][0m |          -0.0065 |           1.5933 |           3.1339 |
[32m[20230114 18:43:47 @agent_ppo2.py:189][0m |          -0.0068 |           1.5712 |           3.1349 |
[32m[20230114 18:43:47 @agent_ppo2.py:189][0m |          -0.0073 |           1.5512 |           3.1347 |
[32m[20230114 18:43:47 @agent_ppo2.py:189][0m |          -0.0077 |           1.5350 |           3.1338 |
[32m[20230114 18:43:47 @agent_ppo2.py:189][0m |          -0.0086 |           1.5187 |           3.1346 |
[32m[20230114 18:43:47 @agent_ppo2.py:134][0m Policy update time: 1.08 s
[32m[20230114 18:43:47 @agent_ppo2.py:142][0m Average TRAINING episode reward: 137.78
[32m[20230114 18:43:47 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 149.92
[32m[20230114 18:43:47 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 157.47
[32m[20230114 18:43:47 @agent_ppo2.py:147][0m Total time:       1.72 min
[32m[20230114 18:43:47 @agent_ppo2.py:149][0m 118784 total steps have happened
[32m[20230114 18:43:47 @agent_ppo2.py:125][0m #------------------------ Iteration 58 --------------------------#
[32m[20230114 18:43:48 @agent_ppo2.py:131][0m Sampling time: 0.31 s by 4 slaves
[32m[20230114 18:43:48 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:43:48 @agent_ppo2.py:189][0m |          -0.0000 |          36.4738 |           3.1406 |
[32m[20230114 18:43:48 @agent_ppo2.py:189][0m |          -0.0043 |          27.4099 |           3.1371 |
[32m[20230114 18:43:48 @agent_ppo2.py:189][0m |          -0.0054 |          22.6352 |           3.1360 |
[32m[20230114 18:43:48 @agent_ppo2.py:189][0m |          -0.0058 |          18.9191 |           3.1341 |
[32m[20230114 18:43:48 @agent_ppo2.py:189][0m |          -0.0070 |          16.1934 |           3.1314 |
[32m[20230114 18:43:48 @agent_ppo2.py:189][0m |          -0.0077 |          11.0790 |           3.1304 |
[32m[20230114 18:43:48 @agent_ppo2.py:189][0m |          -0.0072 |           7.9825 |           3.1266 |
[32m[20230114 18:43:48 @agent_ppo2.py:189][0m |          -0.0083 |           6.8655 |           3.1294 |
[32m[20230114 18:43:49 @agent_ppo2.py:189][0m |          -0.0085 |           5.9880 |           3.1280 |
[32m[20230114 18:43:49 @agent_ppo2.py:189][0m |          -0.0092 |           5.3201 |           3.1265 |
[32m[20230114 18:43:49 @agent_ppo2.py:134][0m Policy update time: 1.10 s
[32m[20230114 18:43:49 @agent_ppo2.py:142][0m Average TRAINING episode reward: 60.97
[32m[20230114 18:43:49 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 142.39
[32m[20230114 18:43:49 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 176.45
[32m[20230114 18:43:49 @agent_ppo2.py:147][0m Total time:       1.75 min
[32m[20230114 18:43:49 @agent_ppo2.py:149][0m 120832 total steps have happened
[32m[20230114 18:43:49 @agent_ppo2.py:125][0m #------------------------ Iteration 59 --------------------------#
[32m[20230114 18:43:49 @agent_ppo2.py:131][0m Sampling time: 0.35 s by 4 slaves
[32m[20230114 18:43:49 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:43:50 @agent_ppo2.py:189][0m |          -0.0010 |           5.6186 |           3.0630 |
[32m[20230114 18:43:50 @agent_ppo2.py:189][0m |          -0.0030 |           2.6458 |           3.0683 |
[32m[20230114 18:43:50 @agent_ppo2.py:189][0m |          -0.0046 |           2.2122 |           3.0661 |
[32m[20230114 18:43:50 @agent_ppo2.py:189][0m |          -0.0073 |           2.0220 |           3.0680 |
[32m[20230114 18:43:50 @agent_ppo2.py:189][0m |          -0.0076 |           1.9118 |           3.0685 |
[32m[20230114 18:43:50 @agent_ppo2.py:189][0m |          -0.0087 |           1.8064 |           3.0709 |
[32m[20230114 18:43:50 @agent_ppo2.py:189][0m |          -0.0097 |           1.7382 |           3.0698 |
[32m[20230114 18:43:50 @agent_ppo2.py:189][0m |          -0.0095 |           1.7052 |           3.0728 |
[32m[20230114 18:43:50 @agent_ppo2.py:189][0m |          -0.0089 |           1.6911 |           3.0726 |
[32m[20230114 18:43:50 @agent_ppo2.py:189][0m |          -0.0100 |           1.6343 |           3.0757 |
[32m[20230114 18:43:50 @agent_ppo2.py:134][0m Policy update time: 1.13 s
[32m[20230114 18:43:51 @agent_ppo2.py:142][0m Average TRAINING episode reward: 93.32
[32m[20230114 18:43:51 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 149.27
[32m[20230114 18:43:51 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 195.87
[32m[20230114 18:43:51 @agent_ppo2.py:147][0m Total time:       1.78 min
[32m[20230114 18:43:51 @agent_ppo2.py:149][0m 122880 total steps have happened
[32m[20230114 18:43:51 @agent_ppo2.py:125][0m #------------------------ Iteration 60 --------------------------#
[32m[20230114 18:43:51 @agent_ppo2.py:131][0m Sampling time: 0.40 s by 4 slaves
[32m[20230114 18:43:51 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:43:51 @agent_ppo2.py:189][0m |          -0.0001 |           3.1415 |           3.1121 |
[32m[20230114 18:43:52 @agent_ppo2.py:189][0m |          -0.0019 |           2.1120 |           3.1117 |
[32m[20230114 18:43:52 @agent_ppo2.py:189][0m |          -0.0045 |           1.8962 |           3.1093 |
[32m[20230114 18:43:52 @agent_ppo2.py:189][0m |          -0.0056 |           1.8730 |           3.1127 |
[32m[20230114 18:43:52 @agent_ppo2.py:189][0m |          -0.0063 |           1.8092 |           3.1121 |
[32m[20230114 18:43:52 @agent_ppo2.py:189][0m |          -0.0058 |           1.8002 |           3.1122 |
[32m[20230114 18:43:52 @agent_ppo2.py:189][0m |          -0.0071 |           1.7394 |           3.1117 |
[32m[20230114 18:43:52 @agent_ppo2.py:189][0m |          -0.0067 |           1.6929 |           3.1125 |
[32m[20230114 18:43:52 @agent_ppo2.py:189][0m |          -0.0077 |           1.6709 |           3.1133 |
[32m[20230114 18:43:52 @agent_ppo2.py:189][0m |          -0.0066 |           1.6960 |           3.1125 |
[32m[20230114 18:43:52 @agent_ppo2.py:134][0m Policy update time: 1.17 s
[32m[20230114 18:43:53 @agent_ppo2.py:142][0m Average TRAINING episode reward: 94.79
[32m[20230114 18:43:53 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 145.77
[32m[20230114 18:43:53 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 173.14
[32m[20230114 18:43:53 @agent_ppo2.py:147][0m Total time:       1.81 min
[32m[20230114 18:43:53 @agent_ppo2.py:149][0m 124928 total steps have happened
[32m[20230114 18:43:53 @agent_ppo2.py:125][0m #------------------------ Iteration 61 --------------------------#
[32m[20230114 18:43:53 @agent_ppo2.py:131][0m Sampling time: 0.31 s by 4 slaves
[32m[20230114 18:43:53 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:43:53 @agent_ppo2.py:189][0m |          -0.0033 |           8.3856 |           3.1373 |
[32m[20230114 18:43:53 @agent_ppo2.py:189][0m |          -0.0040 |           5.8795 |           3.1361 |
[32m[20230114 18:43:53 @agent_ppo2.py:189][0m |          -0.0065 |           5.2964 |           3.1342 |
[32m[20230114 18:43:53 @agent_ppo2.py:189][0m |          -0.0046 |           4.9806 |           3.1332 |
[32m[20230114 18:43:54 @agent_ppo2.py:189][0m |          -0.0041 |           4.7862 |           3.1329 |
[32m[20230114 18:43:54 @agent_ppo2.py:189][0m |           0.0477 |           7.6261 |           3.1287 |
[32m[20230114 18:43:54 @agent_ppo2.py:189][0m |          -0.0083 |           8.3085 |           3.1212 |
[32m[20230114 18:43:54 @agent_ppo2.py:189][0m |          -0.0060 |           4.6911 |           3.1232 |
[32m[20230114 18:43:54 @agent_ppo2.py:189][0m |          -0.0054 |           4.4092 |           3.1248 |
[32m[20230114 18:43:54 @agent_ppo2.py:189][0m |          -0.0059 |           4.3542 |           3.1242 |
[32m[20230114 18:43:54 @agent_ppo2.py:134][0m Policy update time: 0.98 s
[32m[20230114 18:43:54 @agent_ppo2.py:142][0m Average TRAINING episode reward: 62.43
[32m[20230114 18:43:54 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 150.99
[32m[20230114 18:43:54 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 218.85
[32m[20230114 18:43:54 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 218.85
[32m[20230114 18:43:54 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 218.85
[32m[20230114 18:43:54 @agent_ppo2.py:147][0m Total time:       1.84 min
[32m[20230114 18:43:54 @agent_ppo2.py:149][0m 126976 total steps have happened
[32m[20230114 18:43:54 @agent_ppo2.py:125][0m #------------------------ Iteration 62 --------------------------#
[32m[20230114 18:43:55 @agent_ppo2.py:131][0m Sampling time: 0.37 s by 4 slaves
[32m[20230114 18:43:55 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:43:55 @agent_ppo2.py:189][0m |          -0.0002 |           1.8579 |           3.1338 |
[32m[20230114 18:43:55 @agent_ppo2.py:189][0m |          -0.0042 |           1.7566 |           3.1363 |
[32m[20230114 18:43:55 @agent_ppo2.py:189][0m |          -0.0050 |           1.7235 |           3.1350 |
[32m[20230114 18:43:55 @agent_ppo2.py:189][0m |          -0.0060 |           1.7073 |           3.1385 |
[32m[20230114 18:43:55 @agent_ppo2.py:189][0m |          -0.0068 |           1.6923 |           3.1384 |
[32m[20230114 18:43:55 @agent_ppo2.py:189][0m |          -0.0074 |           1.6795 |           3.1379 |
[32m[20230114 18:43:55 @agent_ppo2.py:189][0m |          -0.0081 |           1.6745 |           3.1402 |
[32m[20230114 18:43:56 @agent_ppo2.py:189][0m |          -0.0082 |           1.6644 |           3.1405 |
[32m[20230114 18:43:56 @agent_ppo2.py:189][0m |          -0.0086 |           1.6539 |           3.1413 |
[32m[20230114 18:43:56 @agent_ppo2.py:189][0m |          -0.0089 |           1.6468 |           3.1424 |
[32m[20230114 18:43:56 @agent_ppo2.py:134][0m Policy update time: 1.10 s
[32m[20230114 18:43:56 @agent_ppo2.py:142][0m Average TRAINING episode reward: 141.25
[32m[20230114 18:43:56 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 154.42
[32m[20230114 18:43:56 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 212.91
[32m[20230114 18:43:56 @agent_ppo2.py:147][0m Total time:       1.86 min
[32m[20230114 18:43:56 @agent_ppo2.py:149][0m 129024 total steps have happened
[32m[20230114 18:43:56 @agent_ppo2.py:125][0m #------------------------ Iteration 63 --------------------------#
[32m[20230114 18:43:56 @agent_ppo2.py:131][0m Sampling time: 0.31 s by 4 slaves
[32m[20230114 18:43:56 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:43:57 @agent_ppo2.py:189][0m |          -0.0010 |          11.0653 |           3.1264 |
[32m[20230114 18:43:57 @agent_ppo2.py:189][0m |          -0.0038 |           5.5248 |           3.1231 |
[32m[20230114 18:43:57 @agent_ppo2.py:189][0m |          -0.0050 |           4.8107 |           3.1219 |
[32m[20230114 18:43:57 @agent_ppo2.py:189][0m |          -0.0053 |           4.4007 |           3.1224 |
[32m[20230114 18:43:57 @agent_ppo2.py:189][0m |          -0.0060 |           4.2882 |           3.1185 |
[32m[20230114 18:43:57 @agent_ppo2.py:189][0m |          -0.0063 |           3.9995 |           3.1186 |
[32m[20230114 18:43:57 @agent_ppo2.py:189][0m |          -0.0068 |           3.8290 |           3.1198 |
[32m[20230114 18:43:57 @agent_ppo2.py:189][0m |          -0.0071 |           3.7243 |           3.1189 |
[32m[20230114 18:43:57 @agent_ppo2.py:189][0m |          -0.0071 |           3.5675 |           3.1185 |
[32m[20230114 18:43:57 @agent_ppo2.py:189][0m |          -0.0076 |           3.5517 |           3.1184 |
[32m[20230114 18:43:57 @agent_ppo2.py:134][0m Policy update time: 1.10 s
[32m[20230114 18:43:58 @agent_ppo2.py:142][0m Average TRAINING episode reward: 154.78
[32m[20230114 18:43:58 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 160.67
[32m[20230114 18:43:58 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 215.25
[32m[20230114 18:43:58 @agent_ppo2.py:147][0m Total time:       1.89 min
[32m[20230114 18:43:58 @agent_ppo2.py:149][0m 131072 total steps have happened
[32m[20230114 18:43:58 @agent_ppo2.py:125][0m #------------------------ Iteration 64 --------------------------#
[32m[20230114 18:43:58 @agent_ppo2.py:131][0m Sampling time: 0.31 s by 4 slaves
[32m[20230114 18:43:58 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:43:58 @agent_ppo2.py:189][0m |           0.0016 |           1.5782 |           3.2074 |
[32m[20230114 18:43:58 @agent_ppo2.py:189][0m |          -0.0013 |           1.4389 |           3.2067 |
[32m[20230114 18:43:58 @agent_ppo2.py:189][0m |          -0.0035 |           1.4040 |           3.2048 |
[32m[20230114 18:43:59 @agent_ppo2.py:189][0m |          -0.0037 |           1.3779 |           3.2077 |
[32m[20230114 18:43:59 @agent_ppo2.py:189][0m |          -0.0049 |           1.3562 |           3.2077 |
[32m[20230114 18:43:59 @agent_ppo2.py:189][0m |          -0.0050 |           1.3392 |           3.2093 |
[32m[20230114 18:43:59 @agent_ppo2.py:189][0m |          -0.0056 |           1.3277 |           3.2103 |
[32m[20230114 18:43:59 @agent_ppo2.py:189][0m |          -0.0065 |           1.3170 |           3.2135 |
[32m[20230114 18:43:59 @agent_ppo2.py:189][0m |          -0.0068 |           1.3055 |           3.2161 |
[32m[20230114 18:43:59 @agent_ppo2.py:189][0m |          -0.0073 |           1.2955 |           3.2181 |
[32m[20230114 18:43:59 @agent_ppo2.py:134][0m Policy update time: 1.08 s
[32m[20230114 18:43:59 @agent_ppo2.py:142][0m Average TRAINING episode reward: 153.29
[32m[20230114 18:43:59 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 163.69
[32m[20230114 18:43:59 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 242.60
[32m[20230114 18:43:59 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 242.60
[32m[20230114 18:43:59 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 242.60
[32m[20230114 18:43:59 @agent_ppo2.py:147][0m Total time:       1.92 min
[32m[20230114 18:43:59 @agent_ppo2.py:149][0m 133120 total steps have happened
[32m[20230114 18:43:59 @agent_ppo2.py:125][0m #------------------------ Iteration 65 --------------------------#
[32m[20230114 18:44:00 @agent_ppo2.py:131][0m Sampling time: 0.34 s by 4 slaves
[32m[20230114 18:44:00 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:44:00 @agent_ppo2.py:189][0m |           0.0009 |           5.5534 |           3.2072 |
[32m[20230114 18:44:00 @agent_ppo2.py:189][0m |          -0.0039 |           3.7515 |           3.2066 |
[32m[20230114 18:44:00 @agent_ppo2.py:189][0m |          -0.0055 |           3.5957 |           3.2055 |
[32m[20230114 18:44:00 @agent_ppo2.py:189][0m |          -0.0066 |           3.4863 |           3.2067 |
[32m[20230114 18:44:00 @agent_ppo2.py:189][0m |          -0.0072 |           3.3692 |           3.2043 |
[32m[20230114 18:44:00 @agent_ppo2.py:189][0m |          -0.0079 |           3.3197 |           3.2044 |
[32m[20230114 18:44:01 @agent_ppo2.py:189][0m |          -0.0088 |           3.2785 |           3.2040 |
[32m[20230114 18:44:01 @agent_ppo2.py:189][0m |          -0.0084 |           3.2274 |           3.2050 |
[32m[20230114 18:44:01 @agent_ppo2.py:189][0m |          -0.0091 |           3.2944 |           3.2049 |
[32m[20230114 18:44:01 @agent_ppo2.py:189][0m |          -0.0087 |           3.2268 |           3.2038 |
[32m[20230114 18:44:01 @agent_ppo2.py:134][0m Policy update time: 1.08 s
[32m[20230114 18:44:01 @agent_ppo2.py:142][0m Average TRAINING episode reward: 93.42
[32m[20230114 18:44:01 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 148.68
[32m[20230114 18:44:01 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 202.29
[32m[20230114 18:44:01 @agent_ppo2.py:147][0m Total time:       1.95 min
[32m[20230114 18:44:01 @agent_ppo2.py:149][0m 135168 total steps have happened
[32m[20230114 18:44:01 @agent_ppo2.py:125][0m #------------------------ Iteration 66 --------------------------#
[32m[20230114 18:44:02 @agent_ppo2.py:131][0m Sampling time: 0.37 s by 4 slaves
[32m[20230114 18:44:02 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:44:02 @agent_ppo2.py:189][0m |           0.0021 |           6.1235 |           3.2553 |
[32m[20230114 18:44:02 @agent_ppo2.py:189][0m |          -0.0028 |           4.4211 |           3.2500 |
[32m[20230114 18:44:02 @agent_ppo2.py:189][0m |          -0.0028 |           3.8774 |           3.2465 |
[32m[20230114 18:44:02 @agent_ppo2.py:189][0m |          -0.0058 |           3.5422 |           3.2433 |
[32m[20230114 18:44:02 @agent_ppo2.py:189][0m |          -0.0045 |           3.3231 |           3.2437 |
[32m[20230114 18:44:02 @agent_ppo2.py:189][0m |          -0.0071 |           3.0627 |           3.2403 |
[32m[20230114 18:44:02 @agent_ppo2.py:189][0m |          -0.0074 |           2.9022 |           3.2381 |
[32m[20230114 18:44:02 @agent_ppo2.py:189][0m |          -0.0078 |           2.7612 |           3.2375 |
[32m[20230114 18:44:03 @agent_ppo2.py:189][0m |          -0.0073 |           2.6287 |           3.2365 |
[32m[20230114 18:44:03 @agent_ppo2.py:189][0m |          -0.0089 |           2.4970 |           3.2347 |
[32m[20230114 18:44:03 @agent_ppo2.py:134][0m Policy update time: 1.13 s
[32m[20230114 18:44:03 @agent_ppo2.py:142][0m Average TRAINING episode reward: 106.80
[32m[20230114 18:44:03 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 154.34
[32m[20230114 18:44:03 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 195.04
[32m[20230114 18:44:03 @agent_ppo2.py:147][0m Total time:       1.98 min
[32m[20230114 18:44:03 @agent_ppo2.py:149][0m 137216 total steps have happened
[32m[20230114 18:44:03 @agent_ppo2.py:125][0m #------------------------ Iteration 67 --------------------------#
[32m[20230114 18:44:03 @agent_ppo2.py:131][0m Sampling time: 0.31 s by 4 slaves
[32m[20230114 18:44:03 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:44:03 @agent_ppo2.py:189][0m |          -0.0003 |           5.4398 |           3.1857 |
[32m[20230114 18:44:04 @agent_ppo2.py:189][0m |          -0.0034 |           4.3123 |           3.1835 |
[32m[20230114 18:44:04 @agent_ppo2.py:189][0m |          -0.0046 |           4.1489 |           3.1833 |
[32m[20230114 18:44:04 @agent_ppo2.py:189][0m |          -0.0058 |           4.0298 |           3.1845 |
[32m[20230114 18:44:04 @agent_ppo2.py:189][0m |          -0.0056 |           3.9555 |           3.1846 |
[32m[20230114 18:44:04 @agent_ppo2.py:189][0m |          -0.0057 |           3.8185 |           3.1839 |
[32m[20230114 18:44:04 @agent_ppo2.py:189][0m |          -0.0067 |           3.7971 |           3.1862 |
[32m[20230114 18:44:04 @agent_ppo2.py:189][0m |          -0.0068 |           3.7161 |           3.1869 |
[32m[20230114 18:44:04 @agent_ppo2.py:189][0m |          -0.0071 |           3.6684 |           3.1865 |
[32m[20230114 18:44:04 @agent_ppo2.py:189][0m |          -0.0078 |           3.5896 |           3.1867 |
[32m[20230114 18:44:04 @agent_ppo2.py:134][0m Policy update time: 1.06 s
[32m[20230114 18:44:05 @agent_ppo2.py:142][0m Average TRAINING episode reward: 143.49
[32m[20230114 18:44:05 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 157.16
[32m[20230114 18:44:05 @agent_ppo2.py:144][0m Average EVALUATION episode reward: -69.65
[32m[20230114 18:44:05 @agent_ppo2.py:147][0m Total time:       2.01 min
[32m[20230114 18:44:05 @agent_ppo2.py:149][0m 139264 total steps have happened
[32m[20230114 18:44:05 @agent_ppo2.py:125][0m #------------------------ Iteration 68 --------------------------#
[32m[20230114 18:44:05 @agent_ppo2.py:131][0m Sampling time: 0.37 s by 4 slaves
[32m[20230114 18:44:05 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:44:05 @agent_ppo2.py:189][0m |           0.0006 |           4.8959 |           3.1467 |
[32m[20230114 18:44:05 @agent_ppo2.py:189][0m |          -0.0071 |           3.1923 |           3.1492 |
[32m[20230114 18:44:05 @agent_ppo2.py:189][0m |           0.0035 |           3.0127 |           3.1494 |
[32m[20230114 18:44:05 @agent_ppo2.py:189][0m |          -0.0064 |           2.8409 |           3.1493 |
[32m[20230114 18:44:06 @agent_ppo2.py:189][0m |          -0.0044 |           2.7233 |           3.1483 |
[32m[20230114 18:44:06 @agent_ppo2.py:189][0m |          -0.0063 |           2.6269 |           3.1462 |
[32m[20230114 18:44:06 @agent_ppo2.py:189][0m |          -0.0064 |           2.5876 |           3.1477 |
[32m[20230114 18:44:06 @agent_ppo2.py:189][0m |          -0.0075 |           2.5695 |           3.1475 |
[32m[20230114 18:44:06 @agent_ppo2.py:189][0m |          -0.0100 |           2.5132 |           3.1478 |
[32m[20230114 18:44:06 @agent_ppo2.py:189][0m |          -0.0088 |           2.4855 |           3.1483 |
[32m[20230114 18:44:06 @agent_ppo2.py:134][0m Policy update time: 1.14 s
[32m[20230114 18:44:06 @agent_ppo2.py:142][0m Average TRAINING episode reward: 104.30
[32m[20230114 18:44:06 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 152.26
[32m[20230114 18:44:06 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 3.53
[32m[20230114 18:44:06 @agent_ppo2.py:147][0m Total time:       2.03 min
[32m[20230114 18:44:06 @agent_ppo2.py:149][0m 141312 total steps have happened
[32m[20230114 18:44:06 @agent_ppo2.py:125][0m #------------------------ Iteration 69 --------------------------#
[32m[20230114 18:44:07 @agent_ppo2.py:131][0m Sampling time: 0.31 s by 4 slaves
[32m[20230114 18:44:07 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:44:07 @agent_ppo2.py:189][0m |           0.0001 |           1.8247 |           3.2684 |
[32m[20230114 18:44:07 @agent_ppo2.py:189][0m |          -0.0039 |           1.7097 |           3.2613 |
[32m[20230114 18:44:07 @agent_ppo2.py:189][0m |          -0.0061 |           1.6732 |           3.2638 |
[32m[20230114 18:44:07 @agent_ppo2.py:189][0m |          -0.0065 |           1.6479 |           3.2643 |
[32m[20230114 18:44:07 @agent_ppo2.py:189][0m |          -0.0084 |           1.6254 |           3.2661 |
[32m[20230114 18:44:07 @agent_ppo2.py:189][0m |          -0.0085 |           1.6028 |           3.2641 |
[32m[20230114 18:44:07 @agent_ppo2.py:189][0m |          -0.0096 |           1.5998 |           3.2689 |
[32m[20230114 18:44:07 @agent_ppo2.py:189][0m |          -0.0101 |           1.5838 |           3.2707 |
[32m[20230114 18:44:08 @agent_ppo2.py:189][0m |          -0.0105 |           1.5765 |           3.2703 |
[32m[20230114 18:44:08 @agent_ppo2.py:189][0m |          -0.0110 |           1.5654 |           3.2721 |
[32m[20230114 18:44:08 @agent_ppo2.py:134][0m Policy update time: 1.06 s
[32m[20230114 18:44:08 @agent_ppo2.py:142][0m Average TRAINING episode reward: 151.30
[32m[20230114 18:44:08 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 155.95
[32m[20230114 18:44:08 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 192.80
[32m[20230114 18:44:08 @agent_ppo2.py:147][0m Total time:       2.06 min
[32m[20230114 18:44:08 @agent_ppo2.py:149][0m 143360 total steps have happened
[32m[20230114 18:44:08 @agent_ppo2.py:125][0m #------------------------ Iteration 70 --------------------------#
[32m[20230114 18:44:08 @agent_ppo2.py:131][0m Sampling time: 0.31 s by 4 slaves
[32m[20230114 18:44:08 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:44:08 @agent_ppo2.py:189][0m |          -0.0011 |           2.2043 |           3.3088 |
[32m[20230114 18:44:09 @agent_ppo2.py:189][0m |          -0.0046 |           1.8531 |           3.3013 |
[32m[20230114 18:44:09 @agent_ppo2.py:189][0m |          -0.0062 |           1.7696 |           3.2983 |
[32m[20230114 18:44:09 @agent_ppo2.py:189][0m |          -0.0080 |           1.7149 |           3.2963 |
[32m[20230114 18:44:09 @agent_ppo2.py:189][0m |          -0.0075 |           1.6852 |           3.2973 |
[32m[20230114 18:44:09 @agent_ppo2.py:189][0m |          -0.0088 |           1.6606 |           3.2988 |
[32m[20230114 18:44:09 @agent_ppo2.py:189][0m |          -0.0088 |           1.6357 |           3.2966 |
[32m[20230114 18:44:09 @agent_ppo2.py:189][0m |          -0.0098 |           1.6201 |           3.2976 |
[32m[20230114 18:44:09 @agent_ppo2.py:189][0m |          -0.0094 |           1.6069 |           3.2973 |
[32m[20230114 18:44:09 @agent_ppo2.py:189][0m |          -0.0097 |           1.5912 |           3.2969 |
[32m[20230114 18:44:09 @agent_ppo2.py:134][0m Policy update time: 1.09 s
[32m[20230114 18:44:10 @agent_ppo2.py:142][0m Average TRAINING episode reward: 160.26
[32m[20230114 18:44:10 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 174.43
[32m[20230114 18:44:10 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 212.84
[32m[20230114 18:44:10 @agent_ppo2.py:147][0m Total time:       2.09 min
[32m[20230114 18:44:10 @agent_ppo2.py:149][0m 145408 total steps have happened
[32m[20230114 18:44:10 @agent_ppo2.py:125][0m #------------------------ Iteration 71 --------------------------#
[32m[20230114 18:44:10 @agent_ppo2.py:131][0m Sampling time: 0.30 s by 4 slaves
[32m[20230114 18:44:10 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:44:10 @agent_ppo2.py:189][0m |           0.0009 |           8.4811 |           3.2218 |
[32m[20230114 18:44:10 @agent_ppo2.py:189][0m |          -0.0038 |           2.9816 |           3.2199 |
[32m[20230114 18:44:10 @agent_ppo2.py:189][0m |          -0.0046 |           2.3872 |           3.2166 |
[32m[20230114 18:44:10 @agent_ppo2.py:189][0m |          -0.0054 |           2.1224 |           3.2176 |
[32m[20230114 18:44:10 @agent_ppo2.py:189][0m |          -0.0058 |           1.9873 |           3.2163 |
[32m[20230114 18:44:11 @agent_ppo2.py:189][0m |          -0.0063 |           1.8773 |           3.2164 |
[32m[20230114 18:44:11 @agent_ppo2.py:189][0m |          -0.0068 |           1.8036 |           3.2152 |
[32m[20230114 18:44:11 @agent_ppo2.py:189][0m |          -0.0078 |           1.7490 |           3.2139 |
[32m[20230114 18:44:11 @agent_ppo2.py:189][0m |          -0.0078 |           1.7090 |           3.2127 |
[32m[20230114 18:44:11 @agent_ppo2.py:189][0m |          -0.0082 |           1.6691 |           3.2103 |
[32m[20230114 18:44:11 @agent_ppo2.py:134][0m Policy update time: 0.94 s
[32m[20230114 18:44:11 @agent_ppo2.py:142][0m Average TRAINING episode reward: 109.65
[32m[20230114 18:44:11 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 176.19
[32m[20230114 18:44:11 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 209.99
[32m[20230114 18:44:11 @agent_ppo2.py:147][0m Total time:       2.12 min
[32m[20230114 18:44:11 @agent_ppo2.py:149][0m 147456 total steps have happened
[32m[20230114 18:44:11 @agent_ppo2.py:125][0m #------------------------ Iteration 72 --------------------------#
[32m[20230114 18:44:12 @agent_ppo2.py:131][0m Sampling time: 0.30 s by 4 slaves
[32m[20230114 18:44:12 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:44:12 @agent_ppo2.py:189][0m |          -0.0004 |           4.8731 |           3.2149 |
[32m[20230114 18:44:12 @agent_ppo2.py:189][0m |          -0.0062 |           2.6598 |           3.2125 |
[32m[20230114 18:44:12 @agent_ppo2.py:189][0m |          -0.0059 |           2.3895 |           3.2113 |
[32m[20230114 18:44:12 @agent_ppo2.py:189][0m |          -0.0065 |           2.1672 |           3.2110 |
[32m[20230114 18:44:12 @agent_ppo2.py:189][0m |          -0.0002 |           2.0679 |           3.2134 |
[32m[20230114 18:44:12 @agent_ppo2.py:189][0m |          -0.0057 |           2.0044 |           3.2132 |
[32m[20230114 18:44:12 @agent_ppo2.py:189][0m |          -0.0077 |           1.9113 |           3.2113 |
[32m[20230114 18:44:12 @agent_ppo2.py:189][0m |           0.0066 |           1.8856 |           3.2115 |
[32m[20230114 18:44:12 @agent_ppo2.py:189][0m |          -0.0078 |           1.8398 |           3.2137 |
[32m[20230114 18:44:12 @agent_ppo2.py:189][0m |          -0.0008 |           1.8102 |           3.2130 |
[32m[20230114 18:44:12 @agent_ppo2.py:134][0m Policy update time: 0.93 s
[32m[20230114 18:44:13 @agent_ppo2.py:142][0m Average TRAINING episode reward: 103.00
[32m[20230114 18:44:13 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 166.40
[32m[20230114 18:44:13 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 203.58
[32m[20230114 18:44:13 @agent_ppo2.py:147][0m Total time:       2.14 min
[32m[20230114 18:44:13 @agent_ppo2.py:149][0m 149504 total steps have happened
[32m[20230114 18:44:13 @agent_ppo2.py:125][0m #------------------------ Iteration 73 --------------------------#
[32m[20230114 18:44:13 @agent_ppo2.py:131][0m Sampling time: 0.33 s by 4 slaves
[32m[20230114 18:44:13 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:44:13 @agent_ppo2.py:189][0m |           0.0011 |           1.3441 |           3.2285 |
[32m[20230114 18:44:13 @agent_ppo2.py:189][0m |          -0.0030 |           1.2557 |           3.2285 |
[32m[20230114 18:44:14 @agent_ppo2.py:189][0m |          -0.0040 |           1.2311 |           3.2297 |
[32m[20230114 18:44:14 @agent_ppo2.py:189][0m |          -0.0052 |           1.2169 |           3.2305 |
[32m[20230114 18:44:14 @agent_ppo2.py:189][0m |          -0.0059 |           1.2034 |           3.2331 |
[32m[20230114 18:44:14 @agent_ppo2.py:189][0m |          -0.0065 |           1.1947 |           3.2331 |
[32m[20230114 18:44:14 @agent_ppo2.py:189][0m |          -0.0071 |           1.1901 |           3.2353 |
[32m[20230114 18:44:14 @agent_ppo2.py:189][0m |          -0.0074 |           1.1847 |           3.2346 |
[32m[20230114 18:44:14 @agent_ppo2.py:189][0m |          -0.0077 |           1.1802 |           3.2379 |
[32m[20230114 18:44:14 @agent_ppo2.py:189][0m |          -0.0083 |           1.1735 |           3.2362 |
[32m[20230114 18:44:14 @agent_ppo2.py:134][0m Policy update time: 1.07 s
[32m[20230114 18:44:14 @agent_ppo2.py:142][0m Average TRAINING episode reward: 155.49
[32m[20230114 18:44:14 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 160.45
[32m[20230114 18:44:14 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 220.21
[32m[20230114 18:44:14 @agent_ppo2.py:147][0m Total time:       2.17 min
[32m[20230114 18:44:14 @agent_ppo2.py:149][0m 151552 total steps have happened
[32m[20230114 18:44:14 @agent_ppo2.py:125][0m #------------------------ Iteration 74 --------------------------#
[32m[20230114 18:44:15 @agent_ppo2.py:131][0m Sampling time: 0.30 s by 4 slaves
[32m[20230114 18:44:15 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:44:15 @agent_ppo2.py:189][0m |           0.0000 |           6.1345 |           3.2701 |
[32m[20230114 18:44:15 @agent_ppo2.py:189][0m |           0.0018 |           2.5751 |           3.2683 |
[32m[20230114 18:44:15 @agent_ppo2.py:189][0m |          -0.0025 |           2.0321 |           3.2654 |
[32m[20230114 18:44:15 @agent_ppo2.py:189][0m |          -0.0001 |           1.8202 |           3.2618 |
[32m[20230114 18:44:15 @agent_ppo2.py:189][0m |          -0.0049 |           1.7697 |           3.2610 |
[32m[20230114 18:44:15 @agent_ppo2.py:189][0m |          -0.0054 |           1.7514 |           3.2581 |
[32m[20230114 18:44:16 @agent_ppo2.py:189][0m |          -0.0074 |           1.6968 |           3.2576 |
[32m[20230114 18:44:16 @agent_ppo2.py:189][0m |           0.0009 |           1.6604 |           3.2559 |
[32m[20230114 18:44:16 @agent_ppo2.py:189][0m |          -0.0069 |           1.6287 |           3.2563 |
[32m[20230114 18:44:16 @agent_ppo2.py:189][0m |          -0.0063 |           1.6574 |           3.2550 |
[32m[20230114 18:44:16 @agent_ppo2.py:134][0m Policy update time: 1.06 s
[32m[20230114 18:44:16 @agent_ppo2.py:142][0m Average TRAINING episode reward: 132.42
[32m[20230114 18:44:16 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 168.11
[32m[20230114 18:44:16 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 192.50
[32m[20230114 18:44:16 @agent_ppo2.py:147][0m Total time:       2.20 min
[32m[20230114 18:44:16 @agent_ppo2.py:149][0m 153600 total steps have happened
[32m[20230114 18:44:16 @agent_ppo2.py:125][0m #------------------------ Iteration 75 --------------------------#
[32m[20230114 18:44:16 @agent_ppo2.py:131][0m Sampling time: 0.34 s by 4 slaves
[32m[20230114 18:44:17 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:44:17 @agent_ppo2.py:189][0m |           0.0002 |           8.7435 |           3.2938 |
[32m[20230114 18:44:17 @agent_ppo2.py:189][0m |          -0.0054 |           3.7171 |           3.2902 |
[32m[20230114 18:44:17 @agent_ppo2.py:189][0m |          -0.0032 |           3.1485 |           3.2914 |
[32m[20230114 18:44:17 @agent_ppo2.py:189][0m |          -0.0024 |           2.9580 |           3.2888 |
[32m[20230114 18:44:17 @agent_ppo2.py:189][0m |          -0.0065 |           2.6466 |           3.2862 |
[32m[20230114 18:44:17 @agent_ppo2.py:189][0m |          -0.0053 |           2.5298 |           3.2860 |
[32m[20230114 18:44:17 @agent_ppo2.py:189][0m |          -0.0079 |           2.4811 |           3.2832 |
[32m[20230114 18:44:17 @agent_ppo2.py:189][0m |          -0.0046 |           2.4900 |           3.2860 |
[32m[20230114 18:44:17 @agent_ppo2.py:189][0m |          -0.0086 |           2.3511 |           3.2857 |
[32m[20230114 18:44:17 @agent_ppo2.py:189][0m |          -0.0097 |           2.2588 |           3.2827 |
[32m[20230114 18:44:17 @agent_ppo2.py:134][0m Policy update time: 0.98 s
[32m[20230114 18:44:18 @agent_ppo2.py:142][0m Average TRAINING episode reward: 108.78
[32m[20230114 18:44:18 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 164.67
[32m[20230114 18:44:18 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 217.70
[32m[20230114 18:44:18 @agent_ppo2.py:147][0m Total time:       2.23 min
[32m[20230114 18:44:18 @agent_ppo2.py:149][0m 155648 total steps have happened
[32m[20230114 18:44:18 @agent_ppo2.py:125][0m #------------------------ Iteration 76 --------------------------#
[32m[20230114 18:44:18 @agent_ppo2.py:131][0m Sampling time: 0.30 s by 4 slaves
[32m[20230114 18:44:18 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:44:18 @agent_ppo2.py:189][0m |           0.0012 |          10.5601 |           3.2595 |
[32m[20230114 18:44:18 @agent_ppo2.py:189][0m |          -0.0022 |           5.8755 |           3.2546 |
[32m[20230114 18:44:18 @agent_ppo2.py:189][0m |          -0.0025 |           4.9318 |           3.2558 |
[32m[20230114 18:44:18 @agent_ppo2.py:189][0m |          -0.0041 |           4.4185 |           3.2572 |
[32m[20230114 18:44:19 @agent_ppo2.py:189][0m |          -0.0045 |           4.0922 |           3.2568 |
[32m[20230114 18:44:19 @agent_ppo2.py:189][0m |          -0.0057 |           3.8625 |           3.2617 |
[32m[20230114 18:44:19 @agent_ppo2.py:189][0m |          -0.0065 |           3.7532 |           3.2598 |
[32m[20230114 18:44:19 @agent_ppo2.py:189][0m |          -0.0065 |           3.5766 |           3.2613 |
[32m[20230114 18:44:19 @agent_ppo2.py:189][0m |          -0.0072 |           3.4007 |           3.2622 |
[32m[20230114 18:44:19 @agent_ppo2.py:189][0m |          -0.0076 |           3.3609 |           3.2642 |
[32m[20230114 18:44:19 @agent_ppo2.py:134][0m Policy update time: 0.95 s
[32m[20230114 18:44:19 @agent_ppo2.py:142][0m Average TRAINING episode reward: 114.01
[32m[20230114 18:44:19 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 186.83
[32m[20230114 18:44:19 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 229.19
[32m[20230114 18:44:19 @agent_ppo2.py:147][0m Total time:       2.25 min
[32m[20230114 18:44:19 @agent_ppo2.py:149][0m 157696 total steps have happened
[32m[20230114 18:44:19 @agent_ppo2.py:125][0m #------------------------ Iteration 77 --------------------------#
[32m[20230114 18:44:20 @agent_ppo2.py:131][0m Sampling time: 0.30 s by 4 slaves
[32m[20230114 18:44:20 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:44:20 @agent_ppo2.py:189][0m |           0.0029 |           6.5732 |           3.2951 |
[32m[20230114 18:44:20 @agent_ppo2.py:189][0m |          -0.0043 |           5.2299 |           3.2901 |
[32m[20230114 18:44:20 @agent_ppo2.py:189][0m |          -0.0035 |           4.7107 |           3.2887 |
[32m[20230114 18:44:20 @agent_ppo2.py:189][0m |          -0.0045 |           4.4352 |           3.2860 |
[32m[20230114 18:44:20 @agent_ppo2.py:189][0m |          -0.0054 |           4.2302 |           3.2842 |
[32m[20230114 18:44:20 @agent_ppo2.py:189][0m |          -0.0045 |           4.0546 |           3.2865 |
[32m[20230114 18:44:20 @agent_ppo2.py:189][0m |          -0.0014 |           3.8489 |           3.2835 |
[32m[20230114 18:44:20 @agent_ppo2.py:189][0m |          -0.0072 |           3.6900 |           3.2851 |
[32m[20230114 18:44:20 @agent_ppo2.py:189][0m |          -0.0090 |           3.5995 |           3.2840 |
[32m[20230114 18:44:21 @agent_ppo2.py:189][0m |          -0.0070 |           3.5013 |           3.2829 |
[32m[20230114 18:44:21 @agent_ppo2.py:134][0m Policy update time: 0.94 s
[32m[20230114 18:44:21 @agent_ppo2.py:142][0m Average TRAINING episode reward: 121.71
[32m[20230114 18:44:21 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 189.14
[32m[20230114 18:44:21 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 155.70
[32m[20230114 18:44:21 @agent_ppo2.py:147][0m Total time:       2.28 min
[32m[20230114 18:44:21 @agent_ppo2.py:149][0m 159744 total steps have happened
[32m[20230114 18:44:21 @agent_ppo2.py:125][0m #------------------------ Iteration 78 --------------------------#
[32m[20230114 18:44:21 @agent_ppo2.py:131][0m Sampling time: 0.31 s by 4 slaves
[32m[20230114 18:44:21 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:44:21 @agent_ppo2.py:189][0m |          -0.0002 |           1.9316 |           3.3215 |
[32m[20230114 18:44:21 @agent_ppo2.py:189][0m |          -0.0025 |           1.7586 |           3.3155 |
[32m[20230114 18:44:22 @agent_ppo2.py:189][0m |          -0.0045 |           1.6937 |           3.3120 |
[32m[20230114 18:44:22 @agent_ppo2.py:189][0m |          -0.0054 |           1.6592 |           3.3094 |
[32m[20230114 18:44:22 @agent_ppo2.py:189][0m |          -0.0063 |           1.6307 |           3.3100 |
[32m[20230114 18:44:22 @agent_ppo2.py:189][0m |          -0.0071 |           1.6013 |           3.3083 |
[32m[20230114 18:44:22 @agent_ppo2.py:189][0m |          -0.0075 |           1.5827 |           3.3060 |
[32m[20230114 18:44:22 @agent_ppo2.py:189][0m |          -0.0083 |           1.5675 |           3.3053 |
[32m[20230114 18:44:22 @agent_ppo2.py:189][0m |          -0.0082 |           1.5541 |           3.3058 |
[32m[20230114 18:44:22 @agent_ppo2.py:189][0m |          -0.0082 |           1.5364 |           3.3042 |
[32m[20230114 18:44:22 @agent_ppo2.py:134][0m Policy update time: 1.06 s
[32m[20230114 18:44:23 @agent_ppo2.py:142][0m Average TRAINING episode reward: 170.80
[32m[20230114 18:44:23 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 180.07
[32m[20230114 18:44:23 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 97.32
[32m[20230114 18:44:23 @agent_ppo2.py:147][0m Total time:       2.31 min
[32m[20230114 18:44:23 @agent_ppo2.py:149][0m 161792 total steps have happened
[32m[20230114 18:44:23 @agent_ppo2.py:125][0m #------------------------ Iteration 79 --------------------------#
[32m[20230114 18:44:23 @agent_ppo2.py:131][0m Sampling time: 0.37 s by 4 slaves
[32m[20230114 18:44:23 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:44:23 @agent_ppo2.py:189][0m |           0.0040 |           4.6333 |           3.2608 |
[32m[20230114 18:44:23 @agent_ppo2.py:189][0m |          -0.0015 |           2.8994 |           3.2540 |
[32m[20230114 18:44:23 @agent_ppo2.py:189][0m |          -0.0048 |           2.5951 |           3.2523 |
[32m[20230114 18:44:23 @agent_ppo2.py:189][0m |          -0.0065 |           2.4839 |           3.2519 |
[32m[20230114 18:44:24 @agent_ppo2.py:189][0m |          -0.0067 |           2.4702 |           3.2491 |
[32m[20230114 18:44:24 @agent_ppo2.py:189][0m |          -0.0065 |           2.4149 |           3.2510 |
[32m[20230114 18:44:24 @agent_ppo2.py:189][0m |          -0.0069 |           2.3660 |           3.2518 |
[32m[20230114 18:44:24 @agent_ppo2.py:189][0m |          -0.0078 |           2.2582 |           3.2501 |
[32m[20230114 18:44:24 @agent_ppo2.py:189][0m |          -0.0083 |           2.2377 |           3.2488 |
[32m[20230114 18:44:24 @agent_ppo2.py:189][0m |          -0.0080 |           2.1967 |           3.2485 |
[32m[20230114 18:44:24 @agent_ppo2.py:134][0m Policy update time: 1.14 s
[32m[20230114 18:44:24 @agent_ppo2.py:142][0m Average TRAINING episode reward: 128.36
[32m[20230114 18:44:24 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 183.40
[32m[20230114 18:44:24 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 43.08
[32m[20230114 18:44:24 @agent_ppo2.py:147][0m Total time:       2.33 min
[32m[20230114 18:44:24 @agent_ppo2.py:149][0m 163840 total steps have happened
[32m[20230114 18:44:24 @agent_ppo2.py:125][0m #------------------------ Iteration 80 --------------------------#
[32m[20230114 18:44:25 @agent_ppo2.py:131][0m Sampling time: 0.31 s by 4 slaves
[32m[20230114 18:44:25 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:44:25 @agent_ppo2.py:189][0m |          -0.0008 |           2.0163 |           3.3001 |
[32m[20230114 18:44:25 @agent_ppo2.py:189][0m |          -0.0048 |           1.8735 |           3.2933 |
[32m[20230114 18:44:25 @agent_ppo2.py:189][0m |          -0.0057 |           1.8209 |           3.2930 |
[32m[20230114 18:44:25 @agent_ppo2.py:189][0m |          -0.0067 |           1.7746 |           3.2925 |
[32m[20230114 18:44:25 @agent_ppo2.py:189][0m |          -0.0074 |           1.7518 |           3.2900 |
[32m[20230114 18:44:25 @agent_ppo2.py:189][0m |          -0.0074 |           1.7295 |           3.2910 |
[32m[20230114 18:44:25 @agent_ppo2.py:189][0m |          -0.0085 |           1.7050 |           3.2918 |
[32m[20230114 18:44:25 @agent_ppo2.py:189][0m |          -0.0087 |           1.6898 |           3.2879 |
[32m[20230114 18:44:26 @agent_ppo2.py:189][0m |          -0.0092 |           1.6726 |           3.2906 |
[32m[20230114 18:44:26 @agent_ppo2.py:189][0m |          -0.0092 |           1.6585 |           3.2886 |
[32m[20230114 18:44:26 @agent_ppo2.py:134][0m Policy update time: 1.07 s
[32m[20230114 18:44:26 @agent_ppo2.py:142][0m Average TRAINING episode reward: 163.38
[32m[20230114 18:44:26 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 167.63
[32m[20230114 18:44:26 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 191.34
[32m[20230114 18:44:26 @agent_ppo2.py:147][0m Total time:       2.36 min
[32m[20230114 18:44:26 @agent_ppo2.py:149][0m 165888 total steps have happened
[32m[20230114 18:44:26 @agent_ppo2.py:125][0m #------------------------ Iteration 81 --------------------------#
[32m[20230114 18:44:26 @agent_ppo2.py:131][0m Sampling time: 0.33 s by 4 slaves
[32m[20230114 18:44:26 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:44:26 @agent_ppo2.py:189][0m |          -0.0008 |           1.8266 |           3.2945 |
[32m[20230114 18:44:27 @agent_ppo2.py:189][0m |          -0.0054 |           1.7698 |           3.2894 |
[32m[20230114 18:44:27 @agent_ppo2.py:189][0m |          -0.0063 |           1.7460 |           3.2922 |
[32m[20230114 18:44:27 @agent_ppo2.py:189][0m |          -0.0080 |           1.7234 |           3.2945 |
[32m[20230114 18:44:27 @agent_ppo2.py:189][0m |          -0.0090 |           1.7113 |           3.2956 |
[32m[20230114 18:44:27 @agent_ppo2.py:189][0m |          -0.0093 |           1.6998 |           3.2982 |
[32m[20230114 18:44:27 @agent_ppo2.py:189][0m |          -0.0099 |           1.6928 |           3.2985 |
[32m[20230114 18:44:27 @agent_ppo2.py:189][0m |          -0.0103 |           1.6817 |           3.3015 |
[32m[20230114 18:44:27 @agent_ppo2.py:189][0m |          -0.0114 |           1.6682 |           3.3056 |
[32m[20230114 18:44:27 @agent_ppo2.py:189][0m |          -0.0116 |           1.6570 |           3.3063 |
[32m[20230114 18:44:27 @agent_ppo2.py:134][0m Policy update time: 1.07 s
[32m[20230114 18:44:28 @agent_ppo2.py:142][0m Average TRAINING episode reward: 169.20
[32m[20230114 18:44:28 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 178.33
[32m[20230114 18:44:28 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 217.84
[32m[20230114 18:44:28 @agent_ppo2.py:147][0m Total time:       2.39 min
[32m[20230114 18:44:28 @agent_ppo2.py:149][0m 167936 total steps have happened
[32m[20230114 18:44:28 @agent_ppo2.py:125][0m #------------------------ Iteration 82 --------------------------#
[32m[20230114 18:44:28 @agent_ppo2.py:131][0m Sampling time: 0.35 s by 4 slaves
[32m[20230114 18:44:28 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:44:28 @agent_ppo2.py:189][0m |           0.0013 |          11.9128 |           3.2706 |
[32m[20230114 18:44:28 @agent_ppo2.py:189][0m |          -0.0028 |           8.8699 |           3.2669 |
[32m[20230114 18:44:28 @agent_ppo2.py:189][0m |          -0.0036 |           8.0945 |           3.2669 |
[32m[20230114 18:44:28 @agent_ppo2.py:189][0m |          -0.0051 |           7.6055 |           3.2649 |
[32m[20230114 18:44:29 @agent_ppo2.py:189][0m |          -0.0061 |           7.1375 |           3.2644 |
[32m[20230114 18:44:29 @agent_ppo2.py:189][0m |          -0.0069 |           6.7130 |           3.2643 |
[32m[20230114 18:44:29 @agent_ppo2.py:189][0m |          -0.0057 |           6.6543 |           3.2639 |
[32m[20230114 18:44:29 @agent_ppo2.py:189][0m |          -0.0072 |           6.4397 |           3.2629 |
[32m[20230114 18:44:29 @agent_ppo2.py:189][0m |          -0.0083 |           6.2339 |           3.2635 |
[32m[20230114 18:44:29 @agent_ppo2.py:189][0m |          -0.0069 |           6.0799 |           3.2619 |
[32m[20230114 18:44:29 @agent_ppo2.py:134][0m Policy update time: 1.11 s
[32m[20230114 18:44:29 @agent_ppo2.py:142][0m Average TRAINING episode reward: 73.46
[32m[20230114 18:44:29 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 191.67
[32m[20230114 18:44:29 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 214.19
[32m[20230114 18:44:29 @agent_ppo2.py:147][0m Total time:       2.42 min
[32m[20230114 18:44:29 @agent_ppo2.py:149][0m 169984 total steps have happened
[32m[20230114 18:44:29 @agent_ppo2.py:125][0m #------------------------ Iteration 83 --------------------------#
[32m[20230114 18:44:30 @agent_ppo2.py:131][0m Sampling time: 0.31 s by 4 slaves
[32m[20230114 18:44:30 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:44:30 @agent_ppo2.py:189][0m |           0.0013 |           3.1229 |           3.3669 |
[32m[20230114 18:44:30 @agent_ppo2.py:189][0m |          -0.0017 |           2.7686 |           3.3644 |
[32m[20230114 18:44:30 @agent_ppo2.py:189][0m |          -0.0036 |           2.7014 |           3.3646 |
[32m[20230114 18:44:30 @agent_ppo2.py:189][0m |          -0.0046 |           2.6567 |           3.3671 |
[32m[20230114 18:44:30 @agent_ppo2.py:189][0m |          -0.0054 |           2.6213 |           3.3677 |
[32m[20230114 18:44:30 @agent_ppo2.py:189][0m |          -0.0066 |           2.5966 |           3.3673 |
[32m[20230114 18:44:30 @agent_ppo2.py:189][0m |          -0.0077 |           2.5714 |           3.3711 |
[32m[20230114 18:44:31 @agent_ppo2.py:189][0m |          -0.0078 |           2.5493 |           3.3682 |
[32m[20230114 18:44:31 @agent_ppo2.py:189][0m |          -0.0086 |           2.5334 |           3.3739 |
[32m[20230114 18:44:31 @agent_ppo2.py:189][0m |          -0.0090 |           2.5127 |           3.3746 |
[32m[20230114 18:44:31 @agent_ppo2.py:134][0m Policy update time: 1.06 s
[32m[20230114 18:44:31 @agent_ppo2.py:142][0m Average TRAINING episode reward: 171.22
[32m[20230114 18:44:31 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 178.85
[32m[20230114 18:44:31 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 102.06
[32m[20230114 18:44:31 @agent_ppo2.py:147][0m Total time:       2.45 min
[32m[20230114 18:44:31 @agent_ppo2.py:149][0m 172032 total steps have happened
[32m[20230114 18:44:31 @agent_ppo2.py:125][0m #------------------------ Iteration 84 --------------------------#
[32m[20230114 18:44:31 @agent_ppo2.py:131][0m Sampling time: 0.30 s by 4 slaves
[32m[20230114 18:44:31 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:44:32 @agent_ppo2.py:189][0m |           0.0013 |           4.7933 |           3.2843 |
[32m[20230114 18:44:32 @agent_ppo2.py:189][0m |          -0.0024 |           3.3594 |           3.2860 |
[32m[20230114 18:44:32 @agent_ppo2.py:189][0m |          -0.0081 |           3.0898 |           3.2875 |
[32m[20230114 18:44:32 @agent_ppo2.py:189][0m |          -0.0082 |           3.0159 |           3.2867 |
[32m[20230114 18:44:32 @agent_ppo2.py:189][0m |          -0.0073 |           3.0084 |           3.2861 |
[32m[20230114 18:44:32 @agent_ppo2.py:189][0m |          -0.0099 |           2.8609 |           3.2865 |
[32m[20230114 18:44:32 @agent_ppo2.py:189][0m |          -0.0105 |           2.7854 |           3.2877 |
[32m[20230114 18:44:32 @agent_ppo2.py:189][0m |          -0.0112 |           2.7363 |           3.2868 |
[32m[20230114 18:44:32 @agent_ppo2.py:189][0m |          -0.0129 |           2.7073 |           3.2848 |
[32m[20230114 18:44:32 @agent_ppo2.py:189][0m |          -0.0116 |           2.6639 |           3.2833 |
[32m[20230114 18:44:32 @agent_ppo2.py:134][0m Policy update time: 1.02 s
[32m[20230114 18:44:33 @agent_ppo2.py:142][0m Average TRAINING episode reward: 152.44
[32m[20230114 18:44:33 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 185.47
[32m[20230114 18:44:33 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 222.80
[32m[20230114 18:44:33 @agent_ppo2.py:147][0m Total time:       2.48 min
[32m[20230114 18:44:33 @agent_ppo2.py:149][0m 174080 total steps have happened
[32m[20230114 18:44:33 @agent_ppo2.py:125][0m #------------------------ Iteration 85 --------------------------#
[32m[20230114 18:44:33 @agent_ppo2.py:131][0m Sampling time: 0.30 s by 4 slaves
[32m[20230114 18:44:33 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:44:33 @agent_ppo2.py:189][0m |           0.0006 |           9.1551 |           3.3773 |
[32m[20230114 18:44:33 @agent_ppo2.py:189][0m |          -0.0032 |           6.3248 |           3.3730 |
[32m[20230114 18:44:33 @agent_ppo2.py:189][0m |          -0.0047 |           5.8251 |           3.3738 |
[32m[20230114 18:44:33 @agent_ppo2.py:189][0m |          -0.0053 |           5.7353 |           3.3744 |
[32m[20230114 18:44:34 @agent_ppo2.py:189][0m |          -0.0062 |           5.5161 |           3.3726 |
[32m[20230114 18:44:34 @agent_ppo2.py:189][0m |          -0.0075 |           5.3322 |           3.3715 |
[32m[20230114 18:44:34 @agent_ppo2.py:189][0m |          -0.0080 |           5.3005 |           3.3703 |
[32m[20230114 18:44:34 @agent_ppo2.py:189][0m |          -0.0082 |           5.1773 |           3.3710 |
[32m[20230114 18:44:34 @agent_ppo2.py:189][0m |          -0.0086 |           5.0663 |           3.3711 |
[32m[20230114 18:44:34 @agent_ppo2.py:189][0m |          -0.0068 |           5.1759 |           3.3685 |
[32m[20230114 18:44:34 @agent_ppo2.py:134][0m Policy update time: 0.98 s
[32m[20230114 18:44:34 @agent_ppo2.py:142][0m Average TRAINING episode reward: 154.85
[32m[20230114 18:44:34 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 201.98
[32m[20230114 18:44:34 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 245.64
[32m[20230114 18:44:34 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 245.64
[32m[20230114 18:44:34 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 245.64
[32m[20230114 18:44:34 @agent_ppo2.py:147][0m Total time:       2.50 min
[32m[20230114 18:44:34 @agent_ppo2.py:149][0m 176128 total steps have happened
[32m[20230114 18:44:34 @agent_ppo2.py:125][0m #------------------------ Iteration 86 --------------------------#
[32m[20230114 18:44:35 @agent_ppo2.py:131][0m Sampling time: 0.31 s by 4 slaves
[32m[20230114 18:44:35 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:44:35 @agent_ppo2.py:189][0m |          -0.0001 |           4.6531 |           3.3412 |
[32m[20230114 18:44:35 @agent_ppo2.py:189][0m |          -0.0052 |           3.5797 |           3.3363 |
[32m[20230114 18:44:35 @agent_ppo2.py:189][0m |          -0.0044 |           3.3434 |           3.3335 |
[32m[20230114 18:44:35 @agent_ppo2.py:189][0m |           0.0066 |           3.1446 |           3.3353 |
[32m[20230114 18:44:35 @agent_ppo2.py:189][0m |          -0.0043 |           3.0103 |           3.3339 |
[32m[20230114 18:44:35 @agent_ppo2.py:189][0m |          -0.0018 |           2.9323 |           3.3329 |
[32m[20230114 18:44:35 @agent_ppo2.py:189][0m |          -0.0064 |           2.8369 |           3.3308 |
[32m[20230114 18:44:35 @agent_ppo2.py:189][0m |          -0.0039 |           2.8761 |           3.3349 |
[32m[20230114 18:44:35 @agent_ppo2.py:189][0m |          -0.0054 |           2.6913 |           3.3325 |
[32m[20230114 18:44:36 @agent_ppo2.py:189][0m |          -0.0085 |           2.6169 |           3.3351 |
[32m[20230114 18:44:36 @agent_ppo2.py:134][0m Policy update time: 1.04 s
[32m[20230114 18:44:36 @agent_ppo2.py:142][0m Average TRAINING episode reward: 156.33
[32m[20230114 18:44:36 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 203.71
[32m[20230114 18:44:36 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 72.56
[32m[20230114 18:44:36 @agent_ppo2.py:147][0m Total time:       2.53 min
[32m[20230114 18:44:36 @agent_ppo2.py:149][0m 178176 total steps have happened
[32m[20230114 18:44:36 @agent_ppo2.py:125][0m #------------------------ Iteration 87 --------------------------#
[32m[20230114 18:44:36 @agent_ppo2.py:131][0m Sampling time: 0.36 s by 4 slaves
[32m[20230114 18:44:36 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:44:36 @agent_ppo2.py:189][0m |           0.0012 |          16.3431 |           3.3512 |
[32m[20230114 18:44:36 @agent_ppo2.py:189][0m |          -0.0028 |          12.8959 |           3.3478 |
[32m[20230114 18:44:37 @agent_ppo2.py:189][0m |          -0.0044 |          12.0272 |           3.3465 |
[32m[20230114 18:44:37 @agent_ppo2.py:189][0m |          -0.0057 |          11.5366 |           3.3434 |
[32m[20230114 18:44:37 @agent_ppo2.py:189][0m |          -0.0069 |          10.9730 |           3.3443 |
[32m[20230114 18:44:37 @agent_ppo2.py:189][0m |          -0.0075 |          10.5067 |           3.3406 |
[32m[20230114 18:44:37 @agent_ppo2.py:189][0m |          -0.0077 |           9.9417 |           3.3384 |
[32m[20230114 18:44:37 @agent_ppo2.py:189][0m |          -0.0072 |           9.3759 |           3.3365 |
[32m[20230114 18:44:37 @agent_ppo2.py:189][0m |          -0.0089 |           8.8615 |           3.3359 |
[32m[20230114 18:44:37 @agent_ppo2.py:189][0m |          -0.0094 |           8.2943 |           3.3377 |
[32m[20230114 18:44:37 @agent_ppo2.py:134][0m Policy update time: 0.94 s
[32m[20230114 18:44:37 @agent_ppo2.py:142][0m Average TRAINING episode reward: 135.14
[32m[20230114 18:44:37 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 193.58
[32m[20230114 18:44:37 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 247.23
[32m[20230114 18:44:37 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 247.23
[32m[20230114 18:44:37 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 247.23
[32m[20230114 18:44:37 @agent_ppo2.py:147][0m Total time:       2.55 min
[32m[20230114 18:44:37 @agent_ppo2.py:149][0m 180224 total steps have happened
[32m[20230114 18:44:37 @agent_ppo2.py:125][0m #------------------------ Iteration 88 --------------------------#
[32m[20230114 18:44:38 @agent_ppo2.py:131][0m Sampling time: 0.31 s by 4 slaves
[32m[20230114 18:44:38 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:44:38 @agent_ppo2.py:189][0m |          -0.0007 |           3.8394 |           3.3132 |
[32m[20230114 18:44:38 @agent_ppo2.py:189][0m |          -0.0039 |           2.9549 |           3.3122 |
[32m[20230114 18:44:38 @agent_ppo2.py:189][0m |          -0.0057 |           2.8333 |           3.3105 |
[32m[20230114 18:44:38 @agent_ppo2.py:189][0m |          -0.0065 |           2.7734 |           3.3086 |
[32m[20230114 18:44:38 @agent_ppo2.py:189][0m |          -0.0071 |           2.7304 |           3.3085 |
[32m[20230114 18:44:38 @agent_ppo2.py:189][0m |          -0.0075 |           2.6960 |           3.3072 |
[32m[20230114 18:44:39 @agent_ppo2.py:189][0m |          -0.0080 |           2.6688 |           3.3066 |
[32m[20230114 18:44:39 @agent_ppo2.py:189][0m |          -0.0084 |           2.6424 |           3.3078 |
[32m[20230114 18:44:39 @agent_ppo2.py:189][0m |          -0.0087 |           2.6211 |           3.3059 |
[32m[20230114 18:44:39 @agent_ppo2.py:189][0m |          -0.0092 |           2.5995 |           3.3077 |
[32m[20230114 18:44:39 @agent_ppo2.py:134][0m Policy update time: 1.13 s
[32m[20230114 18:44:39 @agent_ppo2.py:142][0m Average TRAINING episode reward: 199.19
[32m[20230114 18:44:39 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 209.20
[32m[20230114 18:44:39 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 234.52
[32m[20230114 18:44:39 @agent_ppo2.py:147][0m Total time:       2.58 min
[32m[20230114 18:44:39 @agent_ppo2.py:149][0m 182272 total steps have happened
[32m[20230114 18:44:39 @agent_ppo2.py:125][0m #------------------------ Iteration 89 --------------------------#
[32m[20230114 18:44:40 @agent_ppo2.py:131][0m Sampling time: 0.36 s by 4 slaves
[32m[20230114 18:44:40 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:44:40 @agent_ppo2.py:189][0m |          -0.0034 |           7.7833 |           3.3269 |
[32m[20230114 18:44:40 @agent_ppo2.py:189][0m |          -0.0044 |           4.6351 |           3.3267 |
[32m[20230114 18:44:40 @agent_ppo2.py:189][0m |          -0.0060 |           3.9293 |           3.3223 |
[32m[20230114 18:44:40 @agent_ppo2.py:189][0m |          -0.0075 |           3.6733 |           3.3200 |
[32m[20230114 18:44:40 @agent_ppo2.py:189][0m |          -0.0104 |           3.4208 |           3.3190 |
[32m[20230114 18:44:40 @agent_ppo2.py:189][0m |          -0.0090 |           3.3456 |           3.3198 |
[32m[20230114 18:44:40 @agent_ppo2.py:189][0m |          -0.0111 |           3.2341 |           3.3178 |
[32m[20230114 18:44:41 @agent_ppo2.py:189][0m |          -0.0084 |           3.1779 |           3.3202 |
[32m[20230114 18:44:41 @agent_ppo2.py:189][0m |          -0.0089 |           3.0746 |           3.3190 |
[32m[20230114 18:44:41 @agent_ppo2.py:189][0m |          -0.0088 |           3.0695 |           3.3189 |
[32m[20230114 18:44:41 @agent_ppo2.py:134][0m Policy update time: 1.29 s
[32m[20230114 18:44:41 @agent_ppo2.py:142][0m Average TRAINING episode reward: 133.20
[32m[20230114 18:44:41 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 202.12
[32m[20230114 18:44:41 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 248.62
[32m[20230114 18:44:41 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 248.62
[32m[20230114 18:44:41 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 248.62
[32m[20230114 18:44:41 @agent_ppo2.py:147][0m Total time:       2.62 min
[32m[20230114 18:44:41 @agent_ppo2.py:149][0m 184320 total steps have happened
[32m[20230114 18:44:41 @agent_ppo2.py:125][0m #------------------------ Iteration 90 --------------------------#
[32m[20230114 18:44:41 @agent_ppo2.py:131][0m Sampling time: 0.31 s by 4 slaves
[32m[20230114 18:44:42 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:44:42 @agent_ppo2.py:189][0m |          -0.0004 |           2.2455 |           3.4288 |
[32m[20230114 18:44:42 @agent_ppo2.py:189][0m |          -0.0041 |           2.0290 |           3.4235 |
[32m[20230114 18:44:42 @agent_ppo2.py:189][0m |          -0.0058 |           1.9597 |           3.4279 |
[32m[20230114 18:44:42 @agent_ppo2.py:189][0m |          -0.0061 |           1.9157 |           3.4270 |
[32m[20230114 18:44:42 @agent_ppo2.py:189][0m |          -0.0067 |           1.8872 |           3.4293 |
[32m[20230114 18:44:42 @agent_ppo2.py:189][0m |          -0.0069 |           1.8594 |           3.4295 |
[32m[20230114 18:44:42 @agent_ppo2.py:189][0m |          -0.0081 |           1.8308 |           3.4317 |
[32m[20230114 18:44:42 @agent_ppo2.py:189][0m |          -0.0086 |           1.8090 |           3.4313 |
[32m[20230114 18:44:42 @agent_ppo2.py:189][0m |          -0.0092 |           1.8007 |           3.4335 |
[32m[20230114 18:44:43 @agent_ppo2.py:189][0m |          -0.0092 |           1.7840 |           3.4327 |
[32m[20230114 18:44:43 @agent_ppo2.py:134][0m Policy update time: 1.14 s
[32m[20230114 18:44:43 @agent_ppo2.py:142][0m Average TRAINING episode reward: 186.87
[32m[20230114 18:44:43 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 200.94
[32m[20230114 18:44:43 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 232.35
[32m[20230114 18:44:43 @agent_ppo2.py:147][0m Total time:       2.65 min
[32m[20230114 18:44:43 @agent_ppo2.py:149][0m 186368 total steps have happened
[32m[20230114 18:44:43 @agent_ppo2.py:125][0m #------------------------ Iteration 91 --------------------------#
[32m[20230114 18:44:43 @agent_ppo2.py:131][0m Sampling time: 0.36 s by 4 slaves
[32m[20230114 18:44:43 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:44:43 @agent_ppo2.py:189][0m |          -0.0018 |           7.6316 |           3.3748 |
[32m[20230114 18:44:44 @agent_ppo2.py:189][0m |          -0.0016 |           4.9436 |           3.3734 |
[32m[20230114 18:44:44 @agent_ppo2.py:189][0m |          -0.0034 |           4.0542 |           3.3724 |
[32m[20230114 18:44:44 @agent_ppo2.py:189][0m |          -0.0047 |           3.8112 |           3.3718 |
[32m[20230114 18:44:44 @agent_ppo2.py:189][0m |          -0.0052 |           3.6299 |           3.3710 |
[32m[20230114 18:44:44 @agent_ppo2.py:189][0m |          -0.0062 |           3.5555 |           3.3726 |
[32m[20230114 18:44:44 @agent_ppo2.py:189][0m |          -0.0070 |           3.4870 |           3.3749 |
[32m[20230114 18:44:44 @agent_ppo2.py:189][0m |          -0.0075 |           3.3607 |           3.3741 |
[32m[20230114 18:44:44 @agent_ppo2.py:189][0m |          -0.0075 |           3.2843 |           3.3731 |
[32m[20230114 18:44:44 @agent_ppo2.py:189][0m |          -0.0081 |           3.1965 |           3.3744 |
[32m[20230114 18:44:44 @agent_ppo2.py:134][0m Policy update time: 1.05 s
[32m[20230114 18:44:45 @agent_ppo2.py:142][0m Average TRAINING episode reward: 121.68
[32m[20230114 18:44:45 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 217.24
[32m[20230114 18:44:45 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 261.64
[32m[20230114 18:44:45 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 261.64
[32m[20230114 18:44:45 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 261.64
[32m[20230114 18:44:45 @agent_ppo2.py:147][0m Total time:       2.67 min
[32m[20230114 18:44:45 @agent_ppo2.py:149][0m 188416 total steps have happened
[32m[20230114 18:44:45 @agent_ppo2.py:125][0m #------------------------ Iteration 92 --------------------------#
[32m[20230114 18:44:45 @agent_ppo2.py:131][0m Sampling time: 0.30 s by 4 slaves
[32m[20230114 18:44:45 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:44:45 @agent_ppo2.py:189][0m |          -0.0026 |          13.9563 |           3.4177 |
[32m[20230114 18:44:45 @agent_ppo2.py:189][0m |          -0.0037 |          11.7878 |           3.4106 |
[32m[20230114 18:44:45 @agent_ppo2.py:189][0m |          -0.0029 |          10.5468 |           3.4062 |
[32m[20230114 18:44:45 @agent_ppo2.py:189][0m |          -0.0046 |           9.7845 |           3.4076 |
[32m[20230114 18:44:45 @agent_ppo2.py:189][0m |          -0.0044 |           9.6599 |           3.4029 |
[32m[20230114 18:44:46 @agent_ppo2.py:189][0m |          -0.0099 |           8.8210 |           3.4038 |
[32m[20230114 18:44:46 @agent_ppo2.py:189][0m |          -0.0090 |           8.7076 |           3.4041 |
[32m[20230114 18:44:46 @agent_ppo2.py:189][0m |          -0.0091 |           8.3245 |           3.4014 |
[32m[20230114 18:44:46 @agent_ppo2.py:189][0m |          -0.0104 |           8.2261 |           3.4023 |
[32m[20230114 18:44:46 @agent_ppo2.py:189][0m |          -0.0121 |           8.1747 |           3.4001 |
[32m[20230114 18:44:46 @agent_ppo2.py:134][0m Policy update time: 0.99 s
[32m[20230114 18:44:46 @agent_ppo2.py:142][0m Average TRAINING episode reward: 163.33
[32m[20230114 18:44:46 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 223.55
[32m[20230114 18:44:46 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 255.08
[32m[20230114 18:44:46 @agent_ppo2.py:147][0m Total time:       2.70 min
[32m[20230114 18:44:46 @agent_ppo2.py:149][0m 190464 total steps have happened
[32m[20230114 18:44:46 @agent_ppo2.py:125][0m #------------------------ Iteration 93 --------------------------#
[32m[20230114 18:44:47 @agent_ppo2.py:131][0m Sampling time: 0.31 s by 4 slaves
[32m[20230114 18:44:47 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:44:47 @agent_ppo2.py:189][0m |          -0.0004 |           2.5310 |           3.4380 |
[32m[20230114 18:44:47 @agent_ppo2.py:189][0m |          -0.0033 |           2.1560 |           3.4341 |
[32m[20230114 18:44:47 @agent_ppo2.py:189][0m |          -0.0048 |           2.0400 |           3.4335 |
[32m[20230114 18:44:47 @agent_ppo2.py:189][0m |          -0.0051 |           1.9657 |           3.4372 |
[32m[20230114 18:44:47 @agent_ppo2.py:189][0m |          -0.0062 |           1.9072 |           3.4359 |
[32m[20230114 18:44:47 @agent_ppo2.py:189][0m |          -0.0069 |           1.8630 |           3.4363 |
[32m[20230114 18:44:47 @agent_ppo2.py:189][0m |          -0.0077 |           1.8274 |           3.4388 |
[32m[20230114 18:44:48 @agent_ppo2.py:189][0m |          -0.0079 |           1.7910 |           3.4394 |
[32m[20230114 18:44:48 @agent_ppo2.py:189][0m |          -0.0085 |           1.7627 |           3.4409 |
[32m[20230114 18:44:48 @agent_ppo2.py:189][0m |          -0.0090 |           1.7416 |           3.4430 |
[32m[20230114 18:44:48 @agent_ppo2.py:134][0m Policy update time: 1.30 s
[32m[20230114 18:44:48 @agent_ppo2.py:142][0m Average TRAINING episode reward: 203.81
[32m[20230114 18:44:48 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 204.61
[32m[20230114 18:44:48 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 261.60
[32m[20230114 18:44:48 @agent_ppo2.py:147][0m Total time:       2.73 min
[32m[20230114 18:44:48 @agent_ppo2.py:149][0m 192512 total steps have happened
[32m[20230114 18:44:48 @agent_ppo2.py:125][0m #------------------------ Iteration 94 --------------------------#
[32m[20230114 18:44:48 @agent_ppo2.py:131][0m Sampling time: 0.33 s by 4 slaves
[32m[20230114 18:44:49 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:44:49 @agent_ppo2.py:189][0m |           0.0011 |           2.4058 |           3.4423 |
[32m[20230114 18:44:49 @agent_ppo2.py:189][0m |          -0.0019 |           2.1653 |           3.4379 |
[32m[20230114 18:44:49 @agent_ppo2.py:189][0m |          -0.0032 |           2.0803 |           3.4388 |
[32m[20230114 18:44:49 @agent_ppo2.py:189][0m |          -0.0042 |           2.0188 |           3.4383 |
[32m[20230114 18:44:49 @agent_ppo2.py:189][0m |          -0.0052 |           1.9775 |           3.4412 |
[32m[20230114 18:44:49 @agent_ppo2.py:189][0m |          -0.0055 |           1.9492 |           3.4415 |
[32m[20230114 18:44:49 @agent_ppo2.py:189][0m |          -0.0068 |           1.9195 |           3.4427 |
[32m[20230114 18:44:49 @agent_ppo2.py:189][0m |          -0.0073 |           1.8953 |           3.4439 |
[32m[20230114 18:44:50 @agent_ppo2.py:189][0m |          -0.0073 |           1.8739 |           3.4436 |
[32m[20230114 18:44:50 @agent_ppo2.py:189][0m |          -0.0082 |           1.8483 |           3.4473 |
[32m[20230114 18:44:50 @agent_ppo2.py:134][0m Policy update time: 1.26 s
[32m[20230114 18:44:50 @agent_ppo2.py:142][0m Average TRAINING episode reward: 201.22
[32m[20230114 18:44:50 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 203.56
[32m[20230114 18:44:50 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 258.87
[32m[20230114 18:44:50 @agent_ppo2.py:147][0m Total time:       2.76 min
[32m[20230114 18:44:50 @agent_ppo2.py:149][0m 194560 total steps have happened
[32m[20230114 18:44:50 @agent_ppo2.py:125][0m #------------------------ Iteration 95 --------------------------#
[32m[20230114 18:44:50 @agent_ppo2.py:131][0m Sampling time: 0.37 s by 4 slaves
[32m[20230114 18:44:50 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:44:51 @agent_ppo2.py:189][0m |           0.0005 |          13.5553 |           3.4759 |
[32m[20230114 18:44:51 @agent_ppo2.py:189][0m |          -0.0036 |           6.6394 |           3.4744 |
[32m[20230114 18:44:51 @agent_ppo2.py:189][0m |          -0.0046 |           5.7385 |           3.4734 |
[32m[20230114 18:44:51 @agent_ppo2.py:189][0m |          -0.0048 |           5.2700 |           3.4706 |
[32m[20230114 18:44:51 @agent_ppo2.py:189][0m |          -0.0047 |           5.0803 |           3.4713 |
[32m[20230114 18:44:51 @agent_ppo2.py:189][0m |          -0.0068 |           4.8913 |           3.4664 |
[32m[20230114 18:44:51 @agent_ppo2.py:189][0m |          -0.0074 |           4.7591 |           3.4682 |
[32m[20230114 18:44:51 @agent_ppo2.py:189][0m |          -0.0063 |           4.7948 |           3.4674 |
[32m[20230114 18:44:51 @agent_ppo2.py:189][0m |          -0.0075 |           4.5561 |           3.4660 |
[32m[20230114 18:44:51 @agent_ppo2.py:189][0m |          -0.0088 |           4.4425 |           3.4651 |
[32m[20230114 18:44:51 @agent_ppo2.py:134][0m Policy update time: 1.11 s
[32m[20230114 18:44:52 @agent_ppo2.py:142][0m Average TRAINING episode reward: 126.45
[32m[20230114 18:44:52 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 230.19
[32m[20230114 18:44:52 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 257.67
[32m[20230114 18:44:52 @agent_ppo2.py:147][0m Total time:       2.79 min
[32m[20230114 18:44:52 @agent_ppo2.py:149][0m 196608 total steps have happened
[32m[20230114 18:44:52 @agent_ppo2.py:125][0m #------------------------ Iteration 96 --------------------------#
[32m[20230114 18:44:52 @agent_ppo2.py:131][0m Sampling time: 0.37 s by 4 slaves
[32m[20230114 18:44:52 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:44:52 @agent_ppo2.py:189][0m |          -0.0008 |           8.1421 |           3.3716 |
[32m[20230114 18:44:52 @agent_ppo2.py:189][0m |          -0.0047 |           5.6396 |           3.3681 |
[32m[20230114 18:44:53 @agent_ppo2.py:189][0m |           0.0041 |           5.1693 |           3.3661 |
[32m[20230114 18:44:53 @agent_ppo2.py:189][0m |          -0.0058 |           4.5310 |           3.3612 |
[32m[20230114 18:44:53 @agent_ppo2.py:189][0m |          -0.0059 |           4.2800 |           3.3614 |
[32m[20230114 18:44:53 @agent_ppo2.py:189][0m |          -0.0043 |           4.2220 |           3.3610 |
[32m[20230114 18:44:53 @agent_ppo2.py:189][0m |          -0.0044 |           4.2229 |           3.3599 |
[32m[20230114 18:44:53 @agent_ppo2.py:189][0m |          -0.0073 |           3.9725 |           3.3597 |
[32m[20230114 18:44:53 @agent_ppo2.py:189][0m |          -0.0084 |           3.8296 |           3.3571 |
[32m[20230114 18:44:53 @agent_ppo2.py:189][0m |          -0.0066 |           3.9135 |           3.3566 |
[32m[20230114 18:44:53 @agent_ppo2.py:134][0m Policy update time: 1.15 s
[32m[20230114 18:44:54 @agent_ppo2.py:142][0m Average TRAINING episode reward: 188.34
[32m[20230114 18:44:54 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 229.86
[32m[20230114 18:44:54 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 261.83
[32m[20230114 18:44:54 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 261.83
[32m[20230114 18:44:54 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 261.83
[32m[20230114 18:44:54 @agent_ppo2.py:147][0m Total time:       2.82 min
[32m[20230114 18:44:54 @agent_ppo2.py:149][0m 198656 total steps have happened
[32m[20230114 18:44:54 @agent_ppo2.py:125][0m #------------------------ Iteration 97 --------------------------#
[32m[20230114 18:44:54 @agent_ppo2.py:131][0m Sampling time: 0.34 s by 4 slaves
[32m[20230114 18:44:54 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:44:54 @agent_ppo2.py:189][0m |           0.0009 |          17.3126 |           3.4037 |
[32m[20230114 18:44:54 @agent_ppo2.py:189][0m |          -0.0046 |          11.8870 |           3.3998 |
[32m[20230114 18:44:54 @agent_ppo2.py:189][0m |          -0.0064 |           9.8007 |           3.3966 |
[32m[20230114 18:44:54 @agent_ppo2.py:189][0m |          -0.0063 |           8.8861 |           3.3998 |
[32m[20230114 18:44:55 @agent_ppo2.py:189][0m |          -0.0005 |           8.3539 |           3.3993 |
[32m[20230114 18:44:55 @agent_ppo2.py:189][0m |          -0.0076 |           7.9465 |           3.3979 |
[32m[20230114 18:44:55 @agent_ppo2.py:189][0m |          -0.0070 |           7.3232 |           3.4003 |
[32m[20230114 18:44:55 @agent_ppo2.py:189][0m |          -0.0064 |           7.1249 |           3.4026 |
[32m[20230114 18:44:55 @agent_ppo2.py:189][0m |          -0.0064 |           6.8207 |           3.3988 |
[32m[20230114 18:44:55 @agent_ppo2.py:189][0m |          -0.0090 |           6.7941 |           3.3969 |
[32m[20230114 18:44:55 @agent_ppo2.py:134][0m Policy update time: 1.14 s
[32m[20230114 18:44:55 @agent_ppo2.py:142][0m Average TRAINING episode reward: 186.20
[32m[20230114 18:44:55 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 229.88
[32m[20230114 18:44:55 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 254.28
[32m[20230114 18:44:55 @agent_ppo2.py:147][0m Total time:       2.85 min
[32m[20230114 18:44:55 @agent_ppo2.py:149][0m 200704 total steps have happened
[32m[20230114 18:44:55 @agent_ppo2.py:125][0m #------------------------ Iteration 98 --------------------------#
[32m[20230114 18:44:56 @agent_ppo2.py:131][0m Sampling time: 0.33 s by 4 slaves
[32m[20230114 18:44:56 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:44:56 @agent_ppo2.py:189][0m |           0.0011 |           4.4923 |           3.4480 |
[32m[20230114 18:44:56 @agent_ppo2.py:189][0m |          -0.0028 |           3.7274 |           3.4474 |
[32m[20230114 18:44:56 @agent_ppo2.py:189][0m |          -0.0032 |           3.6196 |           3.4480 |
[32m[20230114 18:44:56 @agent_ppo2.py:189][0m |          -0.0052 |           3.5183 |           3.4455 |
[32m[20230114 18:44:56 @agent_ppo2.py:189][0m |          -0.0041 |           3.4376 |           3.4479 |
[32m[20230114 18:44:56 @agent_ppo2.py:189][0m |          -0.0065 |           3.3610 |           3.4478 |
[32m[20230114 18:44:56 @agent_ppo2.py:189][0m |          -0.0060 |           3.3181 |           3.4467 |
[32m[20230114 18:44:57 @agent_ppo2.py:189][0m |          -0.0069 |           3.2944 |           3.4483 |
[32m[20230114 18:44:57 @agent_ppo2.py:189][0m |          -0.0064 |           3.2062 |           3.4473 |
[32m[20230114 18:44:57 @agent_ppo2.py:189][0m |          -0.0077 |           3.1301 |           3.4492 |
[32m[20230114 18:44:57 @agent_ppo2.py:134][0m Policy update time: 1.07 s
[32m[20230114 18:44:57 @agent_ppo2.py:142][0m Average TRAINING episode reward: 204.73
[32m[20230114 18:44:57 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 232.15
[32m[20230114 18:44:57 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 258.88
[32m[20230114 18:44:57 @agent_ppo2.py:147][0m Total time:       2.88 min
[32m[20230114 18:44:57 @agent_ppo2.py:149][0m 202752 total steps have happened
[32m[20230114 18:44:57 @agent_ppo2.py:125][0m #------------------------ Iteration 99 --------------------------#
[32m[20230114 18:44:57 @agent_ppo2.py:131][0m Sampling time: 0.30 s by 4 slaves
[32m[20230114 18:44:57 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:44:58 @agent_ppo2.py:189][0m |          -0.0019 |           9.4711 |           3.4846 |
[32m[20230114 18:44:58 @agent_ppo2.py:189][0m |          -0.0043 |           7.8982 |           3.4785 |
[32m[20230114 18:44:58 @agent_ppo2.py:189][0m |          -0.0058 |           6.8939 |           3.4767 |
[32m[20230114 18:44:58 @agent_ppo2.py:189][0m |          -0.0091 |           6.5803 |           3.4772 |
[32m[20230114 18:44:58 @agent_ppo2.py:189][0m |          -0.0094 |           6.3618 |           3.4737 |
[32m[20230114 18:44:58 @agent_ppo2.py:189][0m |          -0.0106 |           6.0905 |           3.4739 |
[32m[20230114 18:44:58 @agent_ppo2.py:189][0m |          -0.0108 |           6.0496 |           3.4722 |
[32m[20230114 18:44:58 @agent_ppo2.py:189][0m |          -0.0113 |           5.8063 |           3.4709 |
[32m[20230114 18:44:58 @agent_ppo2.py:189][0m |          -0.0099 |           5.6607 |           3.4714 |
[32m[20230114 18:44:58 @agent_ppo2.py:189][0m |          -0.0112 |           5.5357 |           3.4687 |
[32m[20230114 18:44:58 @agent_ppo2.py:134][0m Policy update time: 1.03 s
[32m[20230114 18:44:59 @agent_ppo2.py:142][0m Average TRAINING episode reward: 183.74
[32m[20230114 18:44:59 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 231.74
[32m[20230114 18:44:59 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 257.73
[32m[20230114 18:44:59 @agent_ppo2.py:147][0m Total time:       2.91 min
[32m[20230114 18:44:59 @agent_ppo2.py:149][0m 204800 total steps have happened
[32m[20230114 18:44:59 @agent_ppo2.py:125][0m #------------------------ Iteration 100 --------------------------#
[32m[20230114 18:44:59 @agent_ppo2.py:131][0m Sampling time: 0.33 s by 4 slaves
[32m[20230114 18:44:59 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:44:59 @agent_ppo2.py:189][0m |          -0.0005 |           3.2189 |           3.4717 |
[32m[20230114 18:44:59 @agent_ppo2.py:189][0m |          -0.0047 |           2.8076 |           3.4650 |
[32m[20230114 18:44:59 @agent_ppo2.py:189][0m |          -0.0064 |           2.6762 |           3.4644 |
[32m[20230114 18:45:00 @agent_ppo2.py:189][0m |          -0.0070 |           2.5829 |           3.4623 |
[32m[20230114 18:45:00 @agent_ppo2.py:189][0m |          -0.0083 |           2.5336 |           3.4631 |
[32m[20230114 18:45:00 @agent_ppo2.py:189][0m |          -0.0083 |           2.4807 |           3.4636 |
[32m[20230114 18:45:00 @agent_ppo2.py:189][0m |          -0.0093 |           2.4524 |           3.4624 |
[32m[20230114 18:45:00 @agent_ppo2.py:189][0m |          -0.0096 |           2.4330 |           3.4623 |
[32m[20230114 18:45:00 @agent_ppo2.py:189][0m |          -0.0102 |           2.4008 |           3.4627 |
[32m[20230114 18:45:00 @agent_ppo2.py:189][0m |          -0.0106 |           2.3852 |           3.4615 |
[32m[20230114 18:45:00 @agent_ppo2.py:134][0m Policy update time: 1.07 s
[32m[20230114 18:45:00 @agent_ppo2.py:142][0m Average TRAINING episode reward: 211.19
[32m[20230114 18:45:00 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 226.52
[32m[20230114 18:45:00 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 260.55
[32m[20230114 18:45:00 @agent_ppo2.py:147][0m Total time:       2.94 min
[32m[20230114 18:45:00 @agent_ppo2.py:149][0m 206848 total steps have happened
[32m[20230114 18:45:00 @agent_ppo2.py:125][0m #------------------------ Iteration 101 --------------------------#
[32m[20230114 18:45:01 @agent_ppo2.py:131][0m Sampling time: 0.31 s by 4 slaves
[32m[20230114 18:45:01 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:45:01 @agent_ppo2.py:189][0m |          -0.0013 |           3.4950 |           3.4845 |
[32m[20230114 18:45:01 @agent_ppo2.py:189][0m |          -0.0053 |           2.7687 |           3.4796 |
[32m[20230114 18:45:01 @agent_ppo2.py:189][0m |          -0.0059 |           2.6217 |           3.4765 |
[32m[20230114 18:45:01 @agent_ppo2.py:189][0m |          -0.0074 |           2.5220 |           3.4763 |
[32m[20230114 18:45:01 @agent_ppo2.py:189][0m |          -0.0077 |           2.4598 |           3.4760 |
[32m[20230114 18:45:01 @agent_ppo2.py:189][0m |          -0.0070 |           2.3867 |           3.4747 |
[32m[20230114 18:45:01 @agent_ppo2.py:189][0m |          -0.0088 |           2.3532 |           3.4763 |
[32m[20230114 18:45:02 @agent_ppo2.py:189][0m |          -0.0087 |           2.2988 |           3.4715 |
[32m[20230114 18:45:02 @agent_ppo2.py:189][0m |          -0.0097 |           2.2654 |           3.4744 |
[32m[20230114 18:45:02 @agent_ppo2.py:189][0m |          -0.0095 |           2.2352 |           3.4706 |
[32m[20230114 18:45:02 @agent_ppo2.py:134][0m Policy update time: 1.07 s
[32m[20230114 18:45:02 @agent_ppo2.py:142][0m Average TRAINING episode reward: 212.99
[32m[20230114 18:45:02 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 228.76
[32m[20230114 18:45:02 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 250.46
[32m[20230114 18:45:02 @agent_ppo2.py:147][0m Total time:       2.96 min
[32m[20230114 18:45:02 @agent_ppo2.py:149][0m 208896 total steps have happened
[32m[20230114 18:45:02 @agent_ppo2.py:125][0m #------------------------ Iteration 102 --------------------------#
[32m[20230114 18:45:02 @agent_ppo2.py:131][0m Sampling time: 0.31 s by 4 slaves
[32m[20230114 18:45:02 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:45:03 @agent_ppo2.py:189][0m |           0.0011 |           2.0729 |           3.4324 |
[32m[20230114 18:45:03 @agent_ppo2.py:189][0m |          -0.0041 |           1.9483 |           3.4276 |
[32m[20230114 18:45:03 @agent_ppo2.py:189][0m |          -0.0041 |           1.8464 |           3.4296 |
[32m[20230114 18:45:03 @agent_ppo2.py:189][0m |          -0.0062 |           1.7777 |           3.4298 |
[32m[20230114 18:45:03 @agent_ppo2.py:189][0m |          -0.0056 |           1.7334 |           3.4283 |
[32m[20230114 18:45:03 @agent_ppo2.py:189][0m |          -0.0070 |           1.6985 |           3.4289 |
[32m[20230114 18:45:03 @agent_ppo2.py:189][0m |          -0.0076 |           1.6565 |           3.4281 |
[32m[20230114 18:45:03 @agent_ppo2.py:189][0m |          -0.0084 |           1.6181 |           3.4262 |
[32m[20230114 18:45:03 @agent_ppo2.py:189][0m |          -0.0087 |           1.6014 |           3.4283 |
[32m[20230114 18:45:03 @agent_ppo2.py:189][0m |          -0.0088 |           1.5727 |           3.4280 |
[32m[20230114 18:45:03 @agent_ppo2.py:134][0m Policy update time: 1.07 s
[32m[20230114 18:45:04 @agent_ppo2.py:142][0m Average TRAINING episode reward: 218.44
[32m[20230114 18:45:04 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 222.02
[32m[20230114 18:45:04 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 255.38
[32m[20230114 18:45:04 @agent_ppo2.py:147][0m Total time:       2.99 min
[32m[20230114 18:45:04 @agent_ppo2.py:149][0m 210944 total steps have happened
[32m[20230114 18:45:04 @agent_ppo2.py:125][0m #------------------------ Iteration 103 --------------------------#
[32m[20230114 18:45:04 @agent_ppo2.py:131][0m Sampling time: 0.31 s by 4 slaves
[32m[20230114 18:45:04 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:45:04 @agent_ppo2.py:189][0m |          -0.0019 |           2.4716 |           3.4641 |
[32m[20230114 18:45:04 @agent_ppo2.py:189][0m |          -0.0074 |           2.2440 |           3.4597 |
[32m[20230114 18:45:04 @agent_ppo2.py:189][0m |          -0.0074 |           2.1316 |           3.4611 |
[32m[20230114 18:45:04 @agent_ppo2.py:189][0m |          -0.0104 |           2.0450 |           3.4589 |
[32m[20230114 18:45:05 @agent_ppo2.py:189][0m |          -0.0102 |           1.9602 |           3.4581 |
[32m[20230114 18:45:05 @agent_ppo2.py:189][0m |          -0.0118 |           1.9163 |           3.4593 |
[32m[20230114 18:45:05 @agent_ppo2.py:189][0m |          -0.0078 |           1.8692 |           3.4603 |
[32m[20230114 18:45:05 @agent_ppo2.py:189][0m |          -0.0064 |           1.8560 |           3.4612 |
[32m[20230114 18:45:05 @agent_ppo2.py:189][0m |          -0.0118 |           1.7747 |           3.4601 |
[32m[20230114 18:45:05 @agent_ppo2.py:189][0m |          -0.0107 |           1.7220 |           3.4625 |
[32m[20230114 18:45:05 @agent_ppo2.py:134][0m Policy update time: 1.07 s
[32m[20230114 18:45:05 @agent_ppo2.py:142][0m Average TRAINING episode reward: 228.30
[32m[20230114 18:45:05 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 232.90
[32m[20230114 18:45:05 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 263.95
[32m[20230114 18:45:05 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 263.95
[32m[20230114 18:45:05 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 263.95
[32m[20230114 18:45:05 @agent_ppo2.py:147][0m Total time:       3.02 min
[32m[20230114 18:45:05 @agent_ppo2.py:149][0m 212992 total steps have happened
[32m[20230114 18:45:05 @agent_ppo2.py:125][0m #------------------------ Iteration 104 --------------------------#
[32m[20230114 18:45:06 @agent_ppo2.py:131][0m Sampling time: 0.30 s by 4 slaves
[32m[20230114 18:45:06 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:45:06 @agent_ppo2.py:189][0m |          -0.0014 |          11.5910 |           3.4361 |
[32m[20230114 18:45:06 @agent_ppo2.py:189][0m |          -0.0054 |           6.0090 |           3.4347 |
[32m[20230114 18:45:06 @agent_ppo2.py:189][0m |          -0.0058 |           5.4509 |           3.4363 |
[32m[20230114 18:45:06 @agent_ppo2.py:189][0m |          -0.0070 |           5.2424 |           3.4366 |
[32m[20230114 18:45:06 @agent_ppo2.py:189][0m |          -0.0081 |           5.0991 |           3.4406 |
[32m[20230114 18:45:06 @agent_ppo2.py:189][0m |          -0.0091 |           4.9727 |           3.4412 |
[32m[20230114 18:45:06 @agent_ppo2.py:189][0m |          -0.0091 |           4.8840 |           3.4426 |
[32m[20230114 18:45:06 @agent_ppo2.py:189][0m |          -0.0100 |           4.8042 |           3.4441 |
[32m[20230114 18:45:07 @agent_ppo2.py:189][0m |          -0.0095 |           4.7949 |           3.4453 |
[32m[20230114 18:45:07 @agent_ppo2.py:189][0m |          -0.0101 |           4.7088 |           3.4444 |
[32m[20230114 18:45:07 @agent_ppo2.py:134][0m Policy update time: 0.99 s
[32m[20230114 18:45:07 @agent_ppo2.py:142][0m Average TRAINING episode reward: 189.36
[32m[20230114 18:45:07 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 235.67
[32m[20230114 18:45:07 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 262.52
[32m[20230114 18:45:07 @agent_ppo2.py:147][0m Total time:       3.05 min
[32m[20230114 18:45:07 @agent_ppo2.py:149][0m 215040 total steps have happened
[32m[20230114 18:45:07 @agent_ppo2.py:125][0m #------------------------ Iteration 105 --------------------------#
[32m[20230114 18:45:07 @agent_ppo2.py:131][0m Sampling time: 0.30 s by 4 slaves
[32m[20230114 18:45:07 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:45:07 @agent_ppo2.py:189][0m |           0.0010 |           9.9586 |           3.5226 |
[32m[20230114 18:45:07 @agent_ppo2.py:189][0m |          -0.0037 |           6.4976 |           3.5160 |
[32m[20230114 18:45:08 @agent_ppo2.py:189][0m |          -0.0045 |           5.9361 |           3.5164 |
[32m[20230114 18:45:08 @agent_ppo2.py:189][0m |          -0.0043 |           5.7412 |           3.5138 |
[32m[20230114 18:45:08 @agent_ppo2.py:189][0m |          -0.0068 |           5.6133 |           3.5106 |
[32m[20230114 18:45:08 @agent_ppo2.py:189][0m |          -0.0059 |           5.4186 |           3.5129 |
[32m[20230114 18:45:08 @agent_ppo2.py:189][0m |          -0.0075 |           5.3234 |           3.5094 |
[32m[20230114 18:45:08 @agent_ppo2.py:189][0m |          -0.0072 |           5.2928 |           3.5087 |
[32m[20230114 18:45:08 @agent_ppo2.py:189][0m |          -0.0074 |           5.2564 |           3.5034 |
[32m[20230114 18:45:08 @agent_ppo2.py:189][0m |          -0.0078 |           5.1775 |           3.5049 |
[32m[20230114 18:45:08 @agent_ppo2.py:134][0m Policy update time: 0.91 s
[32m[20230114 18:45:08 @agent_ppo2.py:142][0m Average TRAINING episode reward: 175.13
[32m[20230114 18:45:08 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 236.24
[32m[20230114 18:45:08 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 267.31
[32m[20230114 18:45:08 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 267.31
[32m[20230114 18:45:08 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 267.31
[32m[20230114 18:45:08 @agent_ppo2.py:147][0m Total time:       3.07 min
[32m[20230114 18:45:08 @agent_ppo2.py:149][0m 217088 total steps have happened
[32m[20230114 18:45:08 @agent_ppo2.py:125][0m #------------------------ Iteration 106 --------------------------#
[32m[20230114 18:45:09 @agent_ppo2.py:131][0m Sampling time: 0.30 s by 4 slaves
[32m[20230114 18:45:09 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:45:09 @agent_ppo2.py:189][0m |          -0.0016 |           5.8512 |           3.4449 |
[32m[20230114 18:45:09 @agent_ppo2.py:189][0m |          -0.0054 |           5.4453 |           3.4453 |
[32m[20230114 18:45:09 @agent_ppo2.py:189][0m |          -0.0069 |           5.3756 |           3.4462 |
[32m[20230114 18:45:09 @agent_ppo2.py:189][0m |          -0.0078 |           5.2461 |           3.4495 |
[32m[20230114 18:45:09 @agent_ppo2.py:189][0m |          -0.0085 |           5.2002 |           3.4506 |
[32m[20230114 18:45:09 @agent_ppo2.py:189][0m |          -0.0087 |           5.1420 |           3.4521 |
[32m[20230114 18:45:09 @agent_ppo2.py:189][0m |          -0.0100 |           5.1019 |           3.4568 |
[32m[20230114 18:45:09 @agent_ppo2.py:189][0m |          -0.0094 |           5.0469 |           3.4613 |
[32m[20230114 18:45:10 @agent_ppo2.py:189][0m |          -0.0093 |           5.0636 |           3.4574 |
[32m[20230114 18:45:10 @agent_ppo2.py:189][0m |          -0.0115 |           4.9918 |           3.4601 |
[32m[20230114 18:45:10 @agent_ppo2.py:134][0m Policy update time: 1.04 s
[32m[20230114 18:45:10 @agent_ppo2.py:142][0m Average TRAINING episode reward: 233.40
[32m[20230114 18:45:10 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 239.03
[32m[20230114 18:45:10 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 267.01
[32m[20230114 18:45:10 @agent_ppo2.py:147][0m Total time:       3.10 min
[32m[20230114 18:45:10 @agent_ppo2.py:149][0m 219136 total steps have happened
[32m[20230114 18:45:10 @agent_ppo2.py:125][0m #------------------------ Iteration 107 --------------------------#
[32m[20230114 18:45:10 @agent_ppo2.py:131][0m Sampling time: 0.32 s by 4 slaves
[32m[20230114 18:45:10 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:45:10 @agent_ppo2.py:189][0m |          -0.0002 |          16.3292 |           3.5575 |
[32m[20230114 18:45:11 @agent_ppo2.py:189][0m |          -0.0032 |          11.6784 |           3.5501 |
[32m[20230114 18:45:11 @agent_ppo2.py:189][0m |          -0.0043 |           9.7425 |           3.5510 |
[32m[20230114 18:45:11 @agent_ppo2.py:189][0m |          -0.0053 |           8.7327 |           3.5491 |
[32m[20230114 18:45:11 @agent_ppo2.py:189][0m |          -0.0057 |           7.9735 |           3.5478 |
[32m[20230114 18:45:11 @agent_ppo2.py:189][0m |          -0.0073 |           7.7648 |           3.5448 |
[32m[20230114 18:45:11 @agent_ppo2.py:189][0m |          -0.0066 |           7.2557 |           3.5411 |
[32m[20230114 18:45:11 @agent_ppo2.py:189][0m |          -0.0073 |           6.9567 |           3.5425 |
[32m[20230114 18:45:11 @agent_ppo2.py:189][0m |          -0.0083 |           6.7150 |           3.5395 |
[32m[20230114 18:45:11 @agent_ppo2.py:189][0m |          -0.0079 |           6.4488 |           3.5393 |
[32m[20230114 18:45:11 @agent_ppo2.py:134][0m Policy update time: 1.00 s
[32m[20230114 18:45:12 @agent_ppo2.py:142][0m Average TRAINING episode reward: 194.99
[32m[20230114 18:45:12 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 236.39
[32m[20230114 18:45:12 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 268.35
[32m[20230114 18:45:12 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 268.35
[32m[20230114 18:45:12 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 268.35
[32m[20230114 18:45:12 @agent_ppo2.py:147][0m Total time:       3.12 min
[32m[20230114 18:45:12 @agent_ppo2.py:149][0m 221184 total steps have happened
[32m[20230114 18:45:12 @agent_ppo2.py:125][0m #------------------------ Iteration 108 --------------------------#
[32m[20230114 18:45:12 @agent_ppo2.py:131][0m Sampling time: 0.29 s by 4 slaves
[32m[20230114 18:45:12 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:45:12 @agent_ppo2.py:189][0m |           0.0013 |           5.4432 |           3.5203 |
[32m[20230114 18:45:12 @agent_ppo2.py:189][0m |          -0.0026 |           4.8297 |           3.5163 |
[32m[20230114 18:45:12 @agent_ppo2.py:189][0m |          -0.0032 |           4.7069 |           3.5180 |
[32m[20230114 18:45:12 @agent_ppo2.py:189][0m |          -0.0053 |           4.6109 |           3.5196 |
[32m[20230114 18:45:12 @agent_ppo2.py:189][0m |          -0.0051 |           4.5403 |           3.5206 |
[32m[20230114 18:45:12 @agent_ppo2.py:189][0m |          -0.0062 |           4.4714 |           3.5237 |
[32m[20230114 18:45:13 @agent_ppo2.py:189][0m |          -0.0071 |           4.4151 |           3.5244 |
[32m[20230114 18:45:13 @agent_ppo2.py:189][0m |          -0.0073 |           4.3654 |           3.5235 |
[32m[20230114 18:45:13 @agent_ppo2.py:189][0m |          -0.0074 |           4.3130 |           3.5224 |
[32m[20230114 18:45:13 @agent_ppo2.py:189][0m |          -0.0076 |           4.2684 |           3.5248 |
[32m[20230114 18:45:13 @agent_ppo2.py:134][0m Policy update time: 1.02 s
[32m[20230114 18:45:13 @agent_ppo2.py:142][0m Average TRAINING episode reward: 237.71
[32m[20230114 18:45:13 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 242.60
[32m[20230114 18:45:13 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 266.73
[32m[20230114 18:45:13 @agent_ppo2.py:147][0m Total time:       3.15 min
[32m[20230114 18:45:13 @agent_ppo2.py:149][0m 223232 total steps have happened
[32m[20230114 18:45:13 @agent_ppo2.py:125][0m #------------------------ Iteration 109 --------------------------#
[32m[20230114 18:45:13 @agent_ppo2.py:131][0m Sampling time: 0.30 s by 4 slaves
[32m[20230114 18:45:13 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:45:14 @agent_ppo2.py:189][0m |           0.0006 |           5.9781 |           3.5161 |
[32m[20230114 18:45:14 @agent_ppo2.py:189][0m |          -0.0068 |           5.5012 |           3.5077 |
[32m[20230114 18:45:14 @agent_ppo2.py:189][0m |          -0.0049 |           5.2558 |           3.5093 |
[32m[20230114 18:45:14 @agent_ppo2.py:189][0m |          -0.0031 |           5.1314 |           3.5048 |
[32m[20230114 18:45:14 @agent_ppo2.py:189][0m |          -0.0091 |           5.0567 |           3.5056 |
[32m[20230114 18:45:14 @agent_ppo2.py:189][0m |          -0.0098 |           4.9830 |           3.5057 |
[32m[20230114 18:45:14 @agent_ppo2.py:189][0m |          -0.0075 |           4.9116 |           3.5024 |
[32m[20230114 18:45:14 @agent_ppo2.py:189][0m |          -0.0053 |           4.9952 |           3.5017 |
[32m[20230114 18:45:14 @agent_ppo2.py:189][0m |          -0.0100 |           4.8385 |           3.5024 |
[32m[20230114 18:45:14 @agent_ppo2.py:189][0m |          -0.0086 |           4.7955 |           3.5009 |
[32m[20230114 18:45:14 @agent_ppo2.py:134][0m Policy update time: 1.03 s
[32m[20230114 18:45:15 @agent_ppo2.py:142][0m Average TRAINING episode reward: 235.21
[32m[20230114 18:45:15 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 241.92
[32m[20230114 18:45:15 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 269.90
[32m[20230114 18:45:15 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 269.90
[32m[20230114 18:45:15 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 269.90
[32m[20230114 18:45:15 @agent_ppo2.py:147][0m Total time:       3.18 min
[32m[20230114 18:45:15 @agent_ppo2.py:149][0m 225280 total steps have happened
[32m[20230114 18:45:15 @agent_ppo2.py:125][0m #------------------------ Iteration 110 --------------------------#
[32m[20230114 18:45:15 @agent_ppo2.py:131][0m Sampling time: 0.30 s by 4 slaves
[32m[20230114 18:45:15 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:45:15 @agent_ppo2.py:189][0m |           0.0016 |           4.9421 |           3.5062 |
[32m[20230114 18:45:15 @agent_ppo2.py:189][0m |          -0.0011 |           4.9185 |           3.5064 |
[32m[20230114 18:45:15 @agent_ppo2.py:189][0m |          -0.0016 |           4.8384 |           3.5040 |
[32m[20230114 18:45:15 @agent_ppo2.py:189][0m |          -0.0047 |           4.6825 |           3.5036 |
[32m[20230114 18:45:16 @agent_ppo2.py:189][0m |          -0.0058 |           4.6390 |           3.5045 |
[32m[20230114 18:45:16 @agent_ppo2.py:189][0m |          -0.0069 |           4.6879 |           3.5037 |
[32m[20230114 18:45:16 @agent_ppo2.py:189][0m |          -0.0068 |           4.6051 |           3.5019 |
[32m[20230114 18:45:16 @agent_ppo2.py:189][0m |          -0.0072 |           4.6039 |           3.5048 |
[32m[20230114 18:45:16 @agent_ppo2.py:189][0m |          -0.0081 |           4.5561 |           3.5045 |
[32m[20230114 18:45:16 @agent_ppo2.py:189][0m |          -0.0058 |           4.5585 |           3.5043 |
[32m[20230114 18:45:16 @agent_ppo2.py:134][0m Policy update time: 1.03 s
[32m[20230114 18:45:16 @agent_ppo2.py:142][0m Average TRAINING episode reward: 235.14
[32m[20230114 18:45:16 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 240.93
[32m[20230114 18:45:16 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 265.18
[32m[20230114 18:45:16 @agent_ppo2.py:147][0m Total time:       3.20 min
[32m[20230114 18:45:16 @agent_ppo2.py:149][0m 227328 total steps have happened
[32m[20230114 18:45:16 @agent_ppo2.py:125][0m #------------------------ Iteration 111 --------------------------#
[32m[20230114 18:45:17 @agent_ppo2.py:131][0m Sampling time: 0.30 s by 4 slaves
[32m[20230114 18:45:17 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:45:17 @agent_ppo2.py:189][0m |          -0.0000 |           4.4709 |           3.5633 |
[32m[20230114 18:45:17 @agent_ppo2.py:189][0m |          -0.0025 |           4.2607 |           3.5580 |
[32m[20230114 18:45:17 @agent_ppo2.py:189][0m |          -0.0038 |           4.1938 |           3.5583 |
[32m[20230114 18:45:17 @agent_ppo2.py:189][0m |          -0.0049 |           4.1561 |           3.5587 |
[32m[20230114 18:45:17 @agent_ppo2.py:189][0m |          -0.0059 |           4.1329 |           3.5597 |
[32m[20230114 18:45:17 @agent_ppo2.py:189][0m |          -0.0060 |           4.1214 |           3.5602 |
[32m[20230114 18:45:17 @agent_ppo2.py:189][0m |          -0.0063 |           4.1081 |           3.5576 |
[32m[20230114 18:45:17 @agent_ppo2.py:189][0m |          -0.0068 |           4.0601 |           3.5618 |
[32m[20230114 18:45:17 @agent_ppo2.py:189][0m |          -0.0072 |           4.0351 |           3.5612 |
[32m[20230114 18:45:18 @agent_ppo2.py:189][0m |          -0.0071 |           4.0376 |           3.5621 |
[32m[20230114 18:45:18 @agent_ppo2.py:134][0m Policy update time: 1.04 s
[32m[20230114 18:45:18 @agent_ppo2.py:142][0m Average TRAINING episode reward: 228.70
[32m[20230114 18:45:18 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 237.64
[32m[20230114 18:45:18 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 263.89
[32m[20230114 18:45:18 @agent_ppo2.py:147][0m Total time:       3.23 min
[32m[20230114 18:45:18 @agent_ppo2.py:149][0m 229376 total steps have happened
[32m[20230114 18:45:18 @agent_ppo2.py:125][0m #------------------------ Iteration 112 --------------------------#
[32m[20230114 18:45:18 @agent_ppo2.py:131][0m Sampling time: 0.32 s by 4 slaves
[32m[20230114 18:45:18 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:45:18 @agent_ppo2.py:189][0m |           0.0013 |           9.4098 |           3.4966 |
[32m[20230114 18:45:18 @agent_ppo2.py:189][0m |          -0.0025 |           7.4967 |           3.4907 |
[32m[20230114 18:45:19 @agent_ppo2.py:189][0m |          -0.0065 |           6.9395 |           3.4917 |
[32m[20230114 18:45:19 @agent_ppo2.py:189][0m |          -0.0071 |           6.6361 |           3.4892 |
[32m[20230114 18:45:19 @agent_ppo2.py:189][0m |          -0.0072 |           6.4185 |           3.4875 |
[32m[20230114 18:45:19 @agent_ppo2.py:189][0m |          -0.0087 |           6.2652 |           3.4883 |
[32m[20230114 18:45:19 @agent_ppo2.py:189][0m |          -0.0087 |           6.2004 |           3.4893 |
[32m[20230114 18:45:19 @agent_ppo2.py:189][0m |          -0.0086 |           5.9479 |           3.4904 |
[32m[20230114 18:45:19 @agent_ppo2.py:189][0m |          -0.0101 |           5.8162 |           3.4908 |
[32m[20230114 18:45:19 @agent_ppo2.py:189][0m |          -0.0101 |           5.7767 |           3.4901 |
[32m[20230114 18:45:19 @agent_ppo2.py:134][0m Policy update time: 1.06 s
[32m[20230114 18:45:19 @agent_ppo2.py:142][0m Average TRAINING episode reward: 171.09
[32m[20230114 18:45:19 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 238.65
[32m[20230114 18:45:19 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 270.11
[32m[20230114 18:45:19 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 270.11
[32m[20230114 18:45:19 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 270.11
[32m[20230114 18:45:19 @agent_ppo2.py:147][0m Total time:       3.26 min
[32m[20230114 18:45:19 @agent_ppo2.py:149][0m 231424 total steps have happened
[32m[20230114 18:45:19 @agent_ppo2.py:125][0m #------------------------ Iteration 113 --------------------------#
[32m[20230114 18:45:20 @agent_ppo2.py:131][0m Sampling time: 0.29 s by 4 slaves
[32m[20230114 18:45:20 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:45:20 @agent_ppo2.py:189][0m |           0.0010 |           5.9953 |           3.5467 |
[32m[20230114 18:45:20 @agent_ppo2.py:189][0m |          -0.0034 |           5.7100 |           3.5406 |
[32m[20230114 18:45:20 @agent_ppo2.py:189][0m |          -0.0046 |           5.6144 |           3.5410 |
[32m[20230114 18:45:20 @agent_ppo2.py:189][0m |          -0.0053 |           5.5383 |           3.5371 |
[32m[20230114 18:45:20 @agent_ppo2.py:189][0m |          -0.0064 |           5.5021 |           3.5378 |
[32m[20230114 18:45:20 @agent_ppo2.py:189][0m |          -0.0046 |           5.4796 |           3.5357 |
[32m[20230114 18:45:21 @agent_ppo2.py:189][0m |          -0.0076 |           5.4326 |           3.5349 |
[32m[20230114 18:45:21 @agent_ppo2.py:189][0m |          -0.0091 |           5.3992 |           3.5320 |
[32m[20230114 18:45:21 @agent_ppo2.py:189][0m |          -0.0079 |           5.3847 |           3.5313 |
[32m[20230114 18:45:21 @agent_ppo2.py:189][0m |          -0.0081 |           5.3603 |           3.5308 |
[32m[20230114 18:45:21 @agent_ppo2.py:134][0m Policy update time: 1.02 s
[32m[20230114 18:45:21 @agent_ppo2.py:142][0m Average TRAINING episode reward: 238.25
[32m[20230114 18:45:21 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 243.38
[32m[20230114 18:45:21 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 269.50
[32m[20230114 18:45:21 @agent_ppo2.py:147][0m Total time:       3.28 min
[32m[20230114 18:45:21 @agent_ppo2.py:149][0m 233472 total steps have happened
[32m[20230114 18:45:21 @agent_ppo2.py:125][0m #------------------------ Iteration 114 --------------------------#
[32m[20230114 18:45:21 @agent_ppo2.py:131][0m Sampling time: 0.31 s by 4 slaves
[32m[20230114 18:45:21 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:45:21 @agent_ppo2.py:189][0m |           0.0002 |          16.2006 |           3.5190 |
[32m[20230114 18:45:22 @agent_ppo2.py:189][0m |          -0.0041 |           8.0012 |           3.5169 |
[32m[20230114 18:45:22 @agent_ppo2.py:189][0m |          -0.0062 |           7.1111 |           3.5117 |
[32m[20230114 18:45:22 @agent_ppo2.py:189][0m |          -0.0068 |           6.8640 |           3.5131 |
[32m[20230114 18:45:22 @agent_ppo2.py:189][0m |          -0.0087 |           6.4888 |           3.5117 |
[32m[20230114 18:45:22 @agent_ppo2.py:189][0m |          -0.0088 |           6.3132 |           3.5100 |
[32m[20230114 18:45:22 @agent_ppo2.py:189][0m |          -0.0092 |           6.1706 |           3.5114 |
[32m[20230114 18:45:22 @agent_ppo2.py:189][0m |          -0.0106 |           5.9264 |           3.5073 |
[32m[20230114 18:45:22 @agent_ppo2.py:189][0m |          -0.0109 |           5.8314 |           3.5060 |
[32m[20230114 18:45:22 @agent_ppo2.py:189][0m |          -0.0100 |           5.6108 |           3.5073 |
[32m[20230114 18:45:22 @agent_ppo2.py:134][0m Policy update time: 0.83 s
[32m[20230114 18:45:22 @agent_ppo2.py:142][0m Average TRAINING episode reward: 123.44
[32m[20230114 18:45:22 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 234.15
[32m[20230114 18:45:22 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 64.49
[32m[20230114 18:45:22 @agent_ppo2.py:147][0m Total time:       3.31 min
[32m[20230114 18:45:22 @agent_ppo2.py:149][0m 235520 total steps have happened
[32m[20230114 18:45:23 @agent_ppo2.py:125][0m #------------------------ Iteration 115 --------------------------#
[32m[20230114 18:45:23 @agent_ppo2.py:131][0m Sampling time: 0.29 s by 4 slaves
[32m[20230114 18:45:23 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:45:23 @agent_ppo2.py:189][0m |           0.0017 |           6.5012 |           3.5721 |
[32m[20230114 18:45:23 @agent_ppo2.py:189][0m |          -0.0028 |           5.6696 |           3.5704 |
[32m[20230114 18:45:23 @agent_ppo2.py:189][0m |          -0.0038 |           5.4836 |           3.5685 |
[32m[20230114 18:45:23 @agent_ppo2.py:189][0m |          -0.0040 |           5.3481 |           3.5729 |
[32m[20230114 18:45:23 @agent_ppo2.py:189][0m |          -0.0055 |           5.2259 |           3.5719 |
[32m[20230114 18:45:23 @agent_ppo2.py:189][0m |          -0.0068 |           5.1360 |           3.5719 |
[32m[20230114 18:45:24 @agent_ppo2.py:189][0m |          -0.0058 |           5.0401 |           3.5712 |
[32m[20230114 18:45:24 @agent_ppo2.py:189][0m |          -0.0076 |           4.9630 |           3.5706 |
[32m[20230114 18:45:24 @agent_ppo2.py:189][0m |          -0.0080 |           4.8999 |           3.5706 |
[32m[20230114 18:45:24 @agent_ppo2.py:189][0m |          -0.0086 |           4.8333 |           3.5740 |
[32m[20230114 18:45:24 @agent_ppo2.py:134][0m Policy update time: 0.98 s
[32m[20230114 18:45:24 @agent_ppo2.py:142][0m Average TRAINING episode reward: 242.05
[32m[20230114 18:45:24 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 244.78
[32m[20230114 18:45:24 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 271.07
[32m[20230114 18:45:24 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 271.07
[32m[20230114 18:45:24 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 271.07
[32m[20230114 18:45:24 @agent_ppo2.py:147][0m Total time:       3.33 min
[32m[20230114 18:45:24 @agent_ppo2.py:149][0m 237568 total steps have happened
[32m[20230114 18:45:24 @agent_ppo2.py:125][0m #------------------------ Iteration 116 --------------------------#
[32m[20230114 18:45:24 @agent_ppo2.py:131][0m Sampling time: 0.31 s by 4 slaves
[32m[20230114 18:45:24 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:45:25 @agent_ppo2.py:189][0m |           0.0002 |           6.7995 |           3.5121 |
[32m[20230114 18:45:25 @agent_ppo2.py:189][0m |          -0.0024 |           6.4991 |           3.5101 |
[32m[20230114 18:45:25 @agent_ppo2.py:189][0m |          -0.0052 |           6.3892 |           3.5052 |
[32m[20230114 18:45:25 @agent_ppo2.py:189][0m |          -0.0071 |           6.3190 |           3.5080 |
[32m[20230114 18:45:25 @agent_ppo2.py:189][0m |          -0.0057 |           6.3010 |           3.5108 |
[32m[20230114 18:45:25 @agent_ppo2.py:189][0m |          -0.0076 |           6.2414 |           3.5097 |
[32m[20230114 18:45:25 @agent_ppo2.py:189][0m |          -0.0078 |           6.2252 |           3.5074 |
[32m[20230114 18:45:25 @agent_ppo2.py:189][0m |          -0.0079 |           6.1808 |           3.5056 |
[32m[20230114 18:45:25 @agent_ppo2.py:189][0m |          -0.0087 |           6.1600 |           3.5101 |
[32m[20230114 18:45:25 @agent_ppo2.py:189][0m |          -0.0082 |           6.1828 |           3.5097 |
[32m[20230114 18:45:25 @agent_ppo2.py:134][0m Policy update time: 0.99 s
[32m[20230114 18:45:26 @agent_ppo2.py:142][0m Average TRAINING episode reward: 241.01
[32m[20230114 18:45:26 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 247.53
[32m[20230114 18:45:26 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 267.97
[32m[20230114 18:45:26 @agent_ppo2.py:147][0m Total time:       3.36 min
[32m[20230114 18:45:26 @agent_ppo2.py:149][0m 239616 total steps have happened
[32m[20230114 18:45:26 @agent_ppo2.py:125][0m #------------------------ Iteration 117 --------------------------#
[32m[20230114 18:45:26 @agent_ppo2.py:131][0m Sampling time: 0.28 s by 4 slaves
[32m[20230114 18:45:26 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:45:26 @agent_ppo2.py:189][0m |          -0.0006 |          14.4943 |           3.5501 |
[32m[20230114 18:45:26 @agent_ppo2.py:189][0m |          -0.0048 |           8.9293 |           3.5463 |
[32m[20230114 18:45:26 @agent_ppo2.py:189][0m |          -0.0057 |           7.9986 |           3.5430 |
[32m[20230114 18:45:26 @agent_ppo2.py:189][0m |          -0.0076 |           7.4914 |           3.5441 |
[32m[20230114 18:45:26 @agent_ppo2.py:189][0m |          -0.0074 |           7.1229 |           3.5449 |
[32m[20230114 18:45:26 @agent_ppo2.py:189][0m |          -0.0101 |           6.9009 |           3.5423 |
[32m[20230114 18:45:27 @agent_ppo2.py:189][0m |          -0.0033 |           6.6877 |           3.5421 |
[32m[20230114 18:45:27 @agent_ppo2.py:189][0m |          -0.0091 |           6.6173 |           3.5389 |
[32m[20230114 18:45:27 @agent_ppo2.py:189][0m |          -0.0109 |           6.3926 |           3.5393 |
[32m[20230114 18:45:27 @agent_ppo2.py:189][0m |          -0.0118 |           6.3528 |           3.5400 |
[32m[20230114 18:45:27 @agent_ppo2.py:134][0m Policy update time: 0.92 s
[32m[20230114 18:45:27 @agent_ppo2.py:142][0m Average TRAINING episode reward: 161.18
[32m[20230114 18:45:27 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 241.34
[32m[20230114 18:45:27 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 272.72
[32m[20230114 18:45:27 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 272.72
[32m[20230114 18:45:27 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 272.72
[32m[20230114 18:45:27 @agent_ppo2.py:147][0m Total time:       3.38 min
[32m[20230114 18:45:27 @agent_ppo2.py:149][0m 241664 total steps have happened
[32m[20230114 18:45:27 @agent_ppo2.py:125][0m #------------------------ Iteration 118 --------------------------#
[32m[20230114 18:45:27 @agent_ppo2.py:131][0m Sampling time: 0.29 s by 4 slaves
[32m[20230114 18:45:27 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:45:27 @agent_ppo2.py:189][0m |          -0.0012 |          20.1998 |           3.5782 |
[32m[20230114 18:45:28 @agent_ppo2.py:189][0m |          -0.0051 |          12.0989 |           3.5729 |
[32m[20230114 18:45:28 @agent_ppo2.py:189][0m |          -0.0070 |           9.8141 |           3.5743 |
[32m[20230114 18:45:28 @agent_ppo2.py:189][0m |          -0.0081 |           8.7433 |           3.5738 |
[32m[20230114 18:45:28 @agent_ppo2.py:189][0m |          -0.0091 |           8.3947 |           3.5710 |
[32m[20230114 18:45:28 @agent_ppo2.py:189][0m |          -0.0102 |           8.1961 |           3.5744 |
[32m[20230114 18:45:28 @agent_ppo2.py:189][0m |          -0.0097 |           7.7014 |           3.5743 |
[32m[20230114 18:45:28 @agent_ppo2.py:189][0m |          -0.0111 |           7.5554 |           3.5730 |
[32m[20230114 18:45:28 @agent_ppo2.py:189][0m |          -0.0111 |           7.4480 |           3.5738 |
[32m[20230114 18:45:28 @agent_ppo2.py:189][0m |          -0.0120 |           7.3212 |           3.5733 |
[32m[20230114 18:45:28 @agent_ppo2.py:134][0m Policy update time: 0.86 s
[32m[20230114 18:45:28 @agent_ppo2.py:142][0m Average TRAINING episode reward: 182.75
[32m[20230114 18:45:28 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 244.48
[32m[20230114 18:45:28 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 269.45
[32m[20230114 18:45:28 @agent_ppo2.py:147][0m Total time:       3.40 min
[32m[20230114 18:45:28 @agent_ppo2.py:149][0m 243712 total steps have happened
[32m[20230114 18:45:28 @agent_ppo2.py:125][0m #------------------------ Iteration 119 --------------------------#
[32m[20230114 18:45:29 @agent_ppo2.py:131][0m Sampling time: 0.28 s by 4 slaves
[32m[20230114 18:45:29 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:45:29 @agent_ppo2.py:189][0m |           0.0024 |           6.5180 |           3.5664 |
[32m[20230114 18:45:29 @agent_ppo2.py:189][0m |          -0.0031 |           5.7191 |           3.5659 |
[32m[20230114 18:45:29 @agent_ppo2.py:189][0m |          -0.0055 |           5.4676 |           3.5696 |
[32m[20230114 18:45:29 @agent_ppo2.py:189][0m |          -0.0075 |           5.2999 |           3.5672 |
[32m[20230114 18:45:29 @agent_ppo2.py:189][0m |          -0.0081 |           5.2171 |           3.5695 |
[32m[20230114 18:45:29 @agent_ppo2.py:189][0m |          -0.0081 |           5.0817 |           3.5682 |
[32m[20230114 18:45:29 @agent_ppo2.py:189][0m |          -0.0092 |           4.9966 |           3.5708 |
[32m[20230114 18:45:30 @agent_ppo2.py:189][0m |          -0.0102 |           4.9100 |           3.5714 |
[32m[20230114 18:45:30 @agent_ppo2.py:189][0m |          -0.0096 |           4.8355 |           3.5726 |
[32m[20230114 18:45:30 @agent_ppo2.py:189][0m |          -0.0112 |           4.7629 |           3.5723 |
[32m[20230114 18:45:30 @agent_ppo2.py:134][0m Policy update time: 0.97 s
[32m[20230114 18:45:30 @agent_ppo2.py:142][0m Average TRAINING episode reward: 242.61
[32m[20230114 18:45:30 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 245.80
[32m[20230114 18:45:30 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 267.36
[32m[20230114 18:45:30 @agent_ppo2.py:147][0m Total time:       3.43 min
[32m[20230114 18:45:30 @agent_ppo2.py:149][0m 245760 total steps have happened
[32m[20230114 18:45:30 @agent_ppo2.py:125][0m #------------------------ Iteration 120 --------------------------#
[32m[20230114 18:45:30 @agent_ppo2.py:131][0m Sampling time: 0.28 s by 4 slaves
[32m[20230114 18:45:30 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:45:30 @agent_ppo2.py:189][0m |           0.0008 |           6.6060 |           3.6370 |
[32m[20230114 18:45:30 @agent_ppo2.py:189][0m |          -0.0019 |           6.4318 |           3.6367 |
[32m[20230114 18:45:31 @agent_ppo2.py:189][0m |          -0.0038 |           6.3292 |           3.6326 |
[32m[20230114 18:45:31 @agent_ppo2.py:189][0m |          -0.0044 |           6.2834 |           3.6320 |
[32m[20230114 18:45:31 @agent_ppo2.py:189][0m |          -0.0047 |           6.2689 |           3.6329 |
[32m[20230114 18:45:31 @agent_ppo2.py:189][0m |          -0.0057 |           6.2449 |           3.6305 |
[32m[20230114 18:45:31 @agent_ppo2.py:189][0m |          -0.0059 |           6.2160 |           3.6308 |
[32m[20230114 18:45:31 @agent_ppo2.py:189][0m |          -0.0068 |           6.2074 |           3.6305 |
[32m[20230114 18:45:31 @agent_ppo2.py:189][0m |          -0.0069 |           6.1861 |           3.6287 |
[32m[20230114 18:45:31 @agent_ppo2.py:189][0m |          -0.0074 |           6.1693 |           3.6290 |
[32m[20230114 18:45:31 @agent_ppo2.py:134][0m Policy update time: 0.98 s
[32m[20230114 18:45:31 @agent_ppo2.py:142][0m Average TRAINING episode reward: 242.81
[32m[20230114 18:45:31 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 248.76
[32m[20230114 18:45:31 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 264.90
[32m[20230114 18:45:31 @agent_ppo2.py:147][0m Total time:       3.45 min
[32m[20230114 18:45:31 @agent_ppo2.py:149][0m 247808 total steps have happened
[32m[20230114 18:45:31 @agent_ppo2.py:125][0m #------------------------ Iteration 121 --------------------------#
[32m[20230114 18:45:32 @agent_ppo2.py:131][0m Sampling time: 0.28 s by 4 slaves
[32m[20230114 18:45:32 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:45:32 @agent_ppo2.py:189][0m |          -0.0010 |           6.7553 |           3.5645 |
[32m[20230114 18:45:32 @agent_ppo2.py:189][0m |          -0.0047 |           6.4164 |           3.5586 |
[32m[20230114 18:45:32 @agent_ppo2.py:189][0m |          -0.0078 |           6.2603 |           3.5571 |
[32m[20230114 18:45:32 @agent_ppo2.py:189][0m |          -0.0070 |           6.1411 |           3.5580 |
[32m[20230114 18:45:32 @agent_ppo2.py:189][0m |          -0.0085 |           6.0015 |           3.5514 |
[32m[20230114 18:45:32 @agent_ppo2.py:189][0m |          -0.0089 |           5.8420 |           3.5556 |
[32m[20230114 18:45:32 @agent_ppo2.py:189][0m |          -0.0076 |           5.7688 |           3.5566 |
[32m[20230114 18:45:33 @agent_ppo2.py:189][0m |          -0.0106 |           5.6183 |           3.5547 |
[32m[20230114 18:45:33 @agent_ppo2.py:189][0m |          -0.0113 |           5.5294 |           3.5581 |
[32m[20230114 18:45:33 @agent_ppo2.py:189][0m |          -0.0110 |           5.4481 |           3.5546 |
[32m[20230114 18:45:33 @agent_ppo2.py:134][0m Policy update time: 0.99 s
[32m[20230114 18:45:33 @agent_ppo2.py:142][0m Average TRAINING episode reward: 242.27
[32m[20230114 18:45:33 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 247.66
[32m[20230114 18:45:33 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 269.63
[32m[20230114 18:45:33 @agent_ppo2.py:147][0m Total time:       3.48 min
[32m[20230114 18:45:33 @agent_ppo2.py:149][0m 249856 total steps have happened
[32m[20230114 18:45:33 @agent_ppo2.py:125][0m #------------------------ Iteration 122 --------------------------#
[32m[20230114 18:45:33 @agent_ppo2.py:131][0m Sampling time: 0.30 s by 4 slaves
[32m[20230114 18:45:33 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:45:33 @agent_ppo2.py:189][0m |           0.0006 |          12.5313 |           3.5882 |
[32m[20230114 18:45:34 @agent_ppo2.py:189][0m |          -0.0026 |           8.3896 |           3.5822 |
[32m[20230114 18:45:34 @agent_ppo2.py:189][0m |          -0.0042 |           7.7739 |           3.5772 |
[32m[20230114 18:45:34 @agent_ppo2.py:189][0m |          -0.0055 |           7.4582 |           3.5755 |
[32m[20230114 18:45:34 @agent_ppo2.py:189][0m |          -0.0066 |           7.2349 |           3.5724 |
[32m[20230114 18:45:34 @agent_ppo2.py:189][0m |          -0.0066 |           7.1997 |           3.5685 |
[32m[20230114 18:45:34 @agent_ppo2.py:189][0m |          -0.0077 |           7.0936 |           3.5666 |
[32m[20230114 18:45:34 @agent_ppo2.py:189][0m |          -0.0075 |           6.8458 |           3.5668 |
[32m[20230114 18:45:34 @agent_ppo2.py:189][0m |          -0.0085 |           6.7486 |           3.5661 |
[32m[20230114 18:45:34 @agent_ppo2.py:189][0m |          -0.0088 |           6.6200 |           3.5638 |
[32m[20230114 18:45:34 @agent_ppo2.py:134][0m Policy update time: 1.02 s
[32m[20230114 18:45:35 @agent_ppo2.py:142][0m Average TRAINING episode reward: 166.34
[32m[20230114 18:45:35 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 239.47
[32m[20230114 18:45:35 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 263.40
[32m[20230114 18:45:35 @agent_ppo2.py:147][0m Total time:       3.51 min
[32m[20230114 18:45:35 @agent_ppo2.py:149][0m 251904 total steps have happened
[32m[20230114 18:45:35 @agent_ppo2.py:125][0m #------------------------ Iteration 123 --------------------------#
[32m[20230114 18:45:35 @agent_ppo2.py:131][0m Sampling time: 0.28 s by 4 slaves
[32m[20230114 18:45:35 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:45:35 @agent_ppo2.py:189][0m |          -0.0013 |           6.2502 |           3.5533 |
[32m[20230114 18:45:35 @agent_ppo2.py:189][0m |          -0.0049 |           6.0425 |           3.5452 |
[32m[20230114 18:45:35 @agent_ppo2.py:189][0m |          -0.0067 |           5.9337 |           3.5449 |
[32m[20230114 18:45:35 @agent_ppo2.py:189][0m |          -0.0077 |           5.8669 |           3.5448 |
[32m[20230114 18:45:35 @agent_ppo2.py:189][0m |          -0.0087 |           5.8146 |           3.5415 |
[32m[20230114 18:45:35 @agent_ppo2.py:189][0m |          -0.0079 |           5.8044 |           3.5451 |
[32m[20230114 18:45:36 @agent_ppo2.py:189][0m |          -0.0085 |           5.7894 |           3.5430 |
[32m[20230114 18:45:36 @agent_ppo2.py:189][0m |          -0.0088 |           5.7664 |           3.5440 |
[32m[20230114 18:45:36 @agent_ppo2.py:189][0m |          -0.0101 |           5.6660 |           3.5429 |
[32m[20230114 18:45:36 @agent_ppo2.py:189][0m |          -0.0095 |           5.6413 |           3.5416 |
[32m[20230114 18:45:36 @agent_ppo2.py:134][0m Policy update time: 0.98 s
[32m[20230114 18:45:36 @agent_ppo2.py:142][0m Average TRAINING episode reward: 242.18
[32m[20230114 18:45:36 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 247.55
[32m[20230114 18:45:36 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 114.02
[32m[20230114 18:45:36 @agent_ppo2.py:147][0m Total time:       3.53 min
[32m[20230114 18:45:36 @agent_ppo2.py:149][0m 253952 total steps have happened
[32m[20230114 18:45:36 @agent_ppo2.py:125][0m #------------------------ Iteration 124 --------------------------#
[32m[20230114 18:45:36 @agent_ppo2.py:131][0m Sampling time: 0.26 s by 4 slaves
[32m[20230114 18:45:36 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:45:37 @agent_ppo2.py:189][0m |           0.0014 |          12.8939 |           3.5583 |
[32m[20230114 18:45:37 @agent_ppo2.py:189][0m |          -0.0043 |           9.9039 |           3.5552 |
[32m[20230114 18:45:37 @agent_ppo2.py:189][0m |          -0.0043 |           8.9384 |           3.5537 |
[32m[20230114 18:45:37 @agent_ppo2.py:189][0m |          -0.0058 |           8.2476 |           3.5497 |
[32m[20230114 18:45:37 @agent_ppo2.py:189][0m |          -0.0063 |           7.8977 |           3.5454 |
[32m[20230114 18:45:37 @agent_ppo2.py:189][0m |          -0.0076 |           7.6243 |           3.5454 |
[32m[20230114 18:45:37 @agent_ppo2.py:189][0m |          -0.0081 |           7.4293 |           3.5452 |
[32m[20230114 18:45:37 @agent_ppo2.py:189][0m |          -0.0079 |           7.2795 |           3.5398 |
[32m[20230114 18:45:37 @agent_ppo2.py:189][0m |          -0.0095 |           7.1479 |           3.5393 |
[32m[20230114 18:45:37 @agent_ppo2.py:189][0m |          -0.0095 |           7.0275 |           3.5391 |
[32m[20230114 18:45:37 @agent_ppo2.py:134][0m Policy update time: 0.86 s
[32m[20230114 18:45:38 @agent_ppo2.py:142][0m Average TRAINING episode reward: 200.98
[32m[20230114 18:45:38 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 247.35
[32m[20230114 18:45:38 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 270.11
[32m[20230114 18:45:38 @agent_ppo2.py:147][0m Total time:       3.56 min
[32m[20230114 18:45:38 @agent_ppo2.py:149][0m 256000 total steps have happened
[32m[20230114 18:45:38 @agent_ppo2.py:125][0m #------------------------ Iteration 125 --------------------------#
[32m[20230114 18:45:38 @agent_ppo2.py:131][0m Sampling time: 0.29 s by 4 slaves
[32m[20230114 18:45:38 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:45:38 @agent_ppo2.py:189][0m |          -0.0001 |          11.9379 |           3.5605 |
[32m[20230114 18:45:38 @agent_ppo2.py:189][0m |          -0.0044 |           8.2104 |           3.5553 |
[32m[20230114 18:45:38 @agent_ppo2.py:189][0m |          -0.0055 |           7.5090 |           3.5545 |
[32m[20230114 18:45:38 @agent_ppo2.py:189][0m |          -0.0058 |           7.0978 |           3.5544 |
[32m[20230114 18:45:38 @agent_ppo2.py:189][0m |          -0.0072 |           6.8922 |           3.5526 |
[32m[20230114 18:45:38 @agent_ppo2.py:189][0m |          -0.0080 |           6.7662 |           3.5531 |
[32m[20230114 18:45:39 @agent_ppo2.py:189][0m |          -0.0081 |           6.6195 |           3.5513 |
[32m[20230114 18:45:39 @agent_ppo2.py:189][0m |          -0.0085 |           6.5062 |           3.5528 |
[32m[20230114 18:45:39 @agent_ppo2.py:189][0m |          -0.0088 |           6.4101 |           3.5516 |
[32m[20230114 18:45:39 @agent_ppo2.py:189][0m |          -0.0098 |           6.3434 |           3.5513 |
[32m[20230114 18:45:39 @agent_ppo2.py:134][0m Policy update time: 0.99 s
[32m[20230114 18:45:39 @agent_ppo2.py:142][0m Average TRAINING episode reward: 197.51
[32m[20230114 18:45:39 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 245.68
[32m[20230114 18:45:39 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 254.64
[32m[20230114 18:45:39 @agent_ppo2.py:147][0m Total time:       3.58 min
[32m[20230114 18:45:39 @agent_ppo2.py:149][0m 258048 total steps have happened
[32m[20230114 18:45:39 @agent_ppo2.py:125][0m #------------------------ Iteration 126 --------------------------#
[32m[20230114 18:45:39 @agent_ppo2.py:131][0m Sampling time: 0.28 s by 4 slaves
[32m[20230114 18:45:39 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:45:40 @agent_ppo2.py:189][0m |           0.0014 |           7.4881 |           3.5862 |
[32m[20230114 18:45:40 @agent_ppo2.py:189][0m |          -0.0023 |           7.0855 |           3.5884 |
[32m[20230114 18:45:40 @agent_ppo2.py:189][0m |          -0.0038 |           6.8814 |           3.5856 |
[32m[20230114 18:45:40 @agent_ppo2.py:189][0m |          -0.0046 |           6.7280 |           3.5881 |
[32m[20230114 18:45:40 @agent_ppo2.py:189][0m |          -0.0058 |           6.6026 |           3.5869 |
[32m[20230114 18:45:40 @agent_ppo2.py:189][0m |          -0.0062 |           6.4786 |           3.5859 |
[32m[20230114 18:45:40 @agent_ppo2.py:189][0m |          -0.0072 |           6.3979 |           3.5889 |
[32m[20230114 18:45:40 @agent_ppo2.py:189][0m |          -0.0076 |           6.2738 |           3.5890 |
[32m[20230114 18:45:40 @agent_ppo2.py:189][0m |          -0.0080 |           6.1741 |           3.5895 |
[32m[20230114 18:45:40 @agent_ppo2.py:189][0m |          -0.0087 |           6.0649 |           3.5918 |
[32m[20230114 18:45:40 @agent_ppo2.py:134][0m Policy update time: 0.98 s
[32m[20230114 18:45:41 @agent_ppo2.py:142][0m Average TRAINING episode reward: 241.67
[32m[20230114 18:45:41 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 249.88
[32m[20230114 18:45:41 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 266.41
[32m[20230114 18:45:41 @agent_ppo2.py:147][0m Total time:       3.61 min
[32m[20230114 18:45:41 @agent_ppo2.py:149][0m 260096 total steps have happened
[32m[20230114 18:45:41 @agent_ppo2.py:125][0m #------------------------ Iteration 127 --------------------------#
[32m[20230114 18:45:41 @agent_ppo2.py:131][0m Sampling time: 0.33 s by 4 slaves
[32m[20230114 18:45:41 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:45:41 @agent_ppo2.py:189][0m |          -0.0016 |          12.7672 |           3.5515 |
[32m[20230114 18:45:41 @agent_ppo2.py:189][0m |          -0.0043 |           9.2346 |           3.5463 |
[32m[20230114 18:45:41 @agent_ppo2.py:189][0m |          -0.0049 |           8.7071 |           3.5441 |
[32m[20230114 18:45:41 @agent_ppo2.py:189][0m |          -0.0072 |           8.5103 |           3.5438 |
[32m[20230114 18:45:41 @agent_ppo2.py:189][0m |          -0.0082 |           8.2940 |           3.5412 |
[32m[20230114 18:45:41 @agent_ppo2.py:189][0m |           0.0003 |           8.1194 |           3.5381 |
[32m[20230114 18:45:42 @agent_ppo2.py:189][0m |          -0.0095 |           8.0323 |           3.5354 |
[32m[20230114 18:45:42 @agent_ppo2.py:189][0m |          -0.0107 |           7.9285 |           3.5345 |
[32m[20230114 18:45:42 @agent_ppo2.py:189][0m |          -0.0075 |           7.9997 |           3.5336 |
[32m[20230114 18:45:42 @agent_ppo2.py:189][0m |          -0.0084 |           8.3064 |           3.5318 |
[32m[20230114 18:45:42 @agent_ppo2.py:134][0m Policy update time: 0.93 s
[32m[20230114 18:45:42 @agent_ppo2.py:142][0m Average TRAINING episode reward: 199.12
[32m[20230114 18:45:42 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 242.13
[32m[20230114 18:45:42 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 272.46
[32m[20230114 18:45:42 @agent_ppo2.py:147][0m Total time:       3.63 min
[32m[20230114 18:45:42 @agent_ppo2.py:149][0m 262144 total steps have happened
[32m[20230114 18:45:42 @agent_ppo2.py:125][0m #------------------------ Iteration 128 --------------------------#
[32m[20230114 18:45:42 @agent_ppo2.py:131][0m Sampling time: 0.28 s by 4 slaves
[32m[20230114 18:45:42 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:45:43 @agent_ppo2.py:189][0m |           0.0003 |           8.7796 |           3.4755 |
[32m[20230114 18:45:43 @agent_ppo2.py:189][0m |          -0.0055 |           8.0013 |           3.4748 |
[32m[20230114 18:45:43 @agent_ppo2.py:189][0m |          -0.0086 |           7.5934 |           3.4749 |
[32m[20230114 18:45:43 @agent_ppo2.py:189][0m |          -0.0051 |           7.4123 |           3.4763 |
[32m[20230114 18:45:43 @agent_ppo2.py:189][0m |          -0.0136 |           7.2735 |           3.4771 |
[32m[20230114 18:45:43 @agent_ppo2.py:189][0m |          -0.0134 |           7.1196 |           3.4766 |
[32m[20230114 18:45:43 @agent_ppo2.py:189][0m |          -0.0163 |           7.0194 |           3.4791 |
[32m[20230114 18:45:43 @agent_ppo2.py:189][0m |          -0.0203 |           6.9218 |           3.4803 |
[32m[20230114 18:45:43 @agent_ppo2.py:189][0m |          -0.0132 |           6.7940 |           3.4820 |
[32m[20230114 18:45:43 @agent_ppo2.py:189][0m |          -0.0096 |           6.7545 |           3.4783 |
[32m[20230114 18:45:43 @agent_ppo2.py:134][0m Policy update time: 0.97 s
[32m[20230114 18:45:44 @agent_ppo2.py:142][0m Average TRAINING episode reward: 244.04
[32m[20230114 18:45:44 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 248.70
[32m[20230114 18:45:44 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 272.74
[32m[20230114 18:45:44 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 272.74
[32m[20230114 18:45:44 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 272.74
[32m[20230114 18:45:44 @agent_ppo2.py:147][0m Total time:       3.66 min
[32m[20230114 18:45:44 @agent_ppo2.py:149][0m 264192 total steps have happened
[32m[20230114 18:45:44 @agent_ppo2.py:125][0m #------------------------ Iteration 129 --------------------------#
[32m[20230114 18:45:44 @agent_ppo2.py:131][0m Sampling time: 0.29 s by 4 slaves
[32m[20230114 18:45:44 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:45:44 @agent_ppo2.py:189][0m |          -0.0035 |           7.1235 |           3.5046 |
[32m[20230114 18:45:44 @agent_ppo2.py:189][0m |          -0.0021 |           6.7783 |           3.5034 |
[32m[20230114 18:45:44 @agent_ppo2.py:189][0m |          -0.0016 |           6.6513 |           3.5019 |
[32m[20230114 18:45:44 @agent_ppo2.py:189][0m |          -0.0064 |           6.5061 |           3.5002 |
[32m[20230114 18:45:44 @agent_ppo2.py:189][0m |          -0.0087 |           6.4654 |           3.4974 |
[32m[20230114 18:45:44 @agent_ppo2.py:189][0m |          -0.0074 |           6.3351 |           3.4963 |
[32m[20230114 18:45:44 @agent_ppo2.py:189][0m |          -0.0057 |           6.3096 |           3.4957 |
[32m[20230114 18:45:45 @agent_ppo2.py:189][0m |          -0.0063 |           6.2469 |           3.4926 |
[32m[20230114 18:45:45 @agent_ppo2.py:189][0m |          -0.0030 |           6.1915 |           3.4912 |
[32m[20230114 18:45:45 @agent_ppo2.py:189][0m |          -0.0050 |           6.1232 |           3.4913 |
[32m[20230114 18:45:45 @agent_ppo2.py:134][0m Policy update time: 0.92 s
[32m[20230114 18:45:45 @agent_ppo2.py:142][0m Average TRAINING episode reward: 247.93
[32m[20230114 18:45:45 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 250.55
[32m[20230114 18:45:45 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 274.28
[32m[20230114 18:45:45 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 274.28
[32m[20230114 18:45:45 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 274.28
[32m[20230114 18:45:45 @agent_ppo2.py:147][0m Total time:       3.68 min
[32m[20230114 18:45:45 @agent_ppo2.py:149][0m 266240 total steps have happened
[32m[20230114 18:45:45 @agent_ppo2.py:125][0m #------------------------ Iteration 130 --------------------------#
[32m[20230114 18:45:45 @agent_ppo2.py:131][0m Sampling time: 0.29 s by 4 slaves
[32m[20230114 18:45:45 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:45:45 @agent_ppo2.py:189][0m |           0.0023 |          13.6475 |           3.5155 |
[32m[20230114 18:45:46 @agent_ppo2.py:189][0m |          -0.0013 |          10.5096 |           3.5192 |
[32m[20230114 18:45:46 @agent_ppo2.py:189][0m |          -0.0030 |           9.7573 |           3.5182 |
[32m[20230114 18:45:46 @agent_ppo2.py:189][0m |          -0.0031 |           9.4178 |           3.5155 |
[32m[20230114 18:45:46 @agent_ppo2.py:189][0m |          -0.0049 |           9.1298 |           3.5169 |
[32m[20230114 18:45:46 @agent_ppo2.py:189][0m |          -0.0047 |           8.7305 |           3.5186 |
[32m[20230114 18:45:46 @agent_ppo2.py:189][0m |          -0.0059 |           8.5901 |           3.5182 |
[32m[20230114 18:45:46 @agent_ppo2.py:189][0m |          -0.0055 |           8.3457 |           3.5186 |
[32m[20230114 18:45:46 @agent_ppo2.py:189][0m |          -0.0067 |           8.2362 |           3.5187 |
[32m[20230114 18:45:46 @agent_ppo2.py:189][0m |          -0.0062 |           8.0135 |           3.5186 |
[32m[20230114 18:45:46 @agent_ppo2.py:134][0m Policy update time: 0.95 s
[32m[20230114 18:45:46 @agent_ppo2.py:142][0m Average TRAINING episode reward: 178.62
[32m[20230114 18:45:46 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 249.07
[32m[20230114 18:45:46 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 269.03
[32m[20230114 18:45:46 @agent_ppo2.py:147][0m Total time:       3.70 min
[32m[20230114 18:45:46 @agent_ppo2.py:149][0m 268288 total steps have happened
[32m[20230114 18:45:46 @agent_ppo2.py:125][0m #------------------------ Iteration 131 --------------------------#
[32m[20230114 18:45:47 @agent_ppo2.py:131][0m Sampling time: 0.27 s by 4 slaves
[32m[20230114 18:45:47 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:45:47 @agent_ppo2.py:189][0m |           0.0010 |          13.7408 |           3.5484 |
[32m[20230114 18:45:47 @agent_ppo2.py:189][0m |          -0.0029 |          10.4494 |           3.5413 |
[32m[20230114 18:45:47 @agent_ppo2.py:189][0m |          -0.0040 |           9.6919 |           3.5398 |
[32m[20230114 18:45:47 @agent_ppo2.py:189][0m |          -0.0051 |           9.3404 |           3.5379 |
[32m[20230114 18:45:47 @agent_ppo2.py:189][0m |          -0.0057 |           9.0915 |           3.5391 |
[32m[20230114 18:45:47 @agent_ppo2.py:189][0m |          -0.0070 |           8.8537 |           3.5379 |
[32m[20230114 18:45:47 @agent_ppo2.py:189][0m |          -0.0071 |           8.7188 |           3.5388 |
[32m[20230114 18:45:47 @agent_ppo2.py:189][0m |          -0.0078 |           8.5633 |           3.5398 |
[32m[20230114 18:45:48 @agent_ppo2.py:189][0m |          -0.0079 |           8.3979 |           3.5399 |
[32m[20230114 18:45:48 @agent_ppo2.py:189][0m |          -0.0083 |           8.2996 |           3.5383 |
[32m[20230114 18:45:48 @agent_ppo2.py:134][0m Policy update time: 0.95 s
[32m[20230114 18:45:48 @agent_ppo2.py:142][0m Average TRAINING episode reward: 243.92
[32m[20230114 18:45:48 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 246.30
[32m[20230114 18:45:48 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 62.49
[32m[20230114 18:45:48 @agent_ppo2.py:147][0m Total time:       3.73 min
[32m[20230114 18:45:48 @agent_ppo2.py:149][0m 270336 total steps have happened
[32m[20230114 18:45:48 @agent_ppo2.py:125][0m #------------------------ Iteration 132 --------------------------#
[32m[20230114 18:45:48 @agent_ppo2.py:131][0m Sampling time: 0.25 s by 4 slaves
[32m[20230114 18:45:48 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:45:48 @agent_ppo2.py:189][0m |           0.0008 |          20.4566 |           3.5298 |
[32m[20230114 18:45:48 @agent_ppo2.py:189][0m |          -0.0040 |          15.4309 |           3.5305 |
[32m[20230114 18:45:48 @agent_ppo2.py:189][0m |          -0.0037 |          14.1181 |           3.5301 |
[32m[20230114 18:45:48 @agent_ppo2.py:189][0m |          -0.0055 |          12.9035 |           3.5286 |
[32m[20230114 18:45:49 @agent_ppo2.py:189][0m |           0.0025 |          13.0195 |           3.5289 |
[32m[20230114 18:45:49 @agent_ppo2.py:189][0m |          -0.0089 |          11.7153 |           3.5258 |
[32m[20230114 18:45:49 @agent_ppo2.py:189][0m |          -0.0089 |          10.6053 |           3.5290 |
[32m[20230114 18:45:49 @agent_ppo2.py:189][0m |          -0.0102 |          10.2888 |           3.5295 |
[32m[20230114 18:45:49 @agent_ppo2.py:189][0m |          -0.0111 |           9.9459 |           3.5288 |
[32m[20230114 18:45:49 @agent_ppo2.py:189][0m |          -0.0112 |           9.5977 |           3.5317 |
[32m[20230114 18:45:49 @agent_ppo2.py:134][0m Policy update time: 0.83 s
[32m[20230114 18:45:49 @agent_ppo2.py:142][0m Average TRAINING episode reward: 199.78
[32m[20230114 18:45:49 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 251.88
[32m[20230114 18:45:49 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 272.80
[32m[20230114 18:45:49 @agent_ppo2.py:147][0m Total time:       3.75 min
[32m[20230114 18:45:49 @agent_ppo2.py:149][0m 272384 total steps have happened
[32m[20230114 18:45:49 @agent_ppo2.py:125][0m #------------------------ Iteration 133 --------------------------#
[32m[20230114 18:45:49 @agent_ppo2.py:131][0m Sampling time: 0.33 s by 4 slaves
[32m[20230114 18:45:50 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:45:50 @agent_ppo2.py:189][0m |           0.0004 |          19.9998 |           3.5059 |
[32m[20230114 18:45:50 @agent_ppo2.py:189][0m |          -0.0048 |          14.7756 |           3.4974 |
[32m[20230114 18:45:50 @agent_ppo2.py:189][0m |          -0.0065 |          13.0469 |           3.4996 |
[32m[20230114 18:45:50 @agent_ppo2.py:189][0m |          -0.0077 |          12.2861 |           3.4984 |
[32m[20230114 18:45:50 @agent_ppo2.py:189][0m |          -0.0084 |          11.6289 |           3.5001 |
[32m[20230114 18:45:50 @agent_ppo2.py:189][0m |          -0.0100 |          11.1765 |           3.5017 |
[32m[20230114 18:45:50 @agent_ppo2.py:189][0m |          -0.0103 |          10.7854 |           3.5041 |
[32m[20230114 18:45:50 @agent_ppo2.py:189][0m |          -0.0113 |          10.4997 |           3.5023 |
[32m[20230114 18:45:50 @agent_ppo2.py:189][0m |          -0.0115 |          10.3529 |           3.5048 |
[32m[20230114 18:45:50 @agent_ppo2.py:189][0m |          -0.0118 |          10.1344 |           3.5057 |
[32m[20230114 18:45:50 @agent_ppo2.py:134][0m Policy update time: 0.99 s
[32m[20230114 18:45:51 @agent_ppo2.py:142][0m Average TRAINING episode reward: 186.99
[32m[20230114 18:45:51 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 249.65
[32m[20230114 18:45:51 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 273.12
[32m[20230114 18:45:51 @agent_ppo2.py:147][0m Total time:       3.78 min
[32m[20230114 18:45:51 @agent_ppo2.py:149][0m 274432 total steps have happened
[32m[20230114 18:45:51 @agent_ppo2.py:125][0m #------------------------ Iteration 134 --------------------------#
[32m[20230114 18:45:51 @agent_ppo2.py:131][0m Sampling time: 0.26 s by 4 slaves
[32m[20230114 18:45:51 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:45:51 @agent_ppo2.py:189][0m |           0.0005 |           8.1081 |           3.6070 |
[32m[20230114 18:45:51 @agent_ppo2.py:189][0m |          -0.0052 |           7.6330 |           3.6088 |
[32m[20230114 18:45:51 @agent_ppo2.py:189][0m |          -0.0063 |           7.5013 |           3.6084 |
[32m[20230114 18:45:51 @agent_ppo2.py:189][0m |          -0.0075 |           7.3126 |           3.6118 |
[32m[20230114 18:45:51 @agent_ppo2.py:189][0m |          -0.0086 |           7.2088 |           3.6111 |
[32m[20230114 18:45:52 @agent_ppo2.py:189][0m |          -0.0088 |           7.1524 |           3.6150 |
[32m[20230114 18:45:52 @agent_ppo2.py:189][0m |          -0.0094 |           7.0964 |           3.6176 |
[32m[20230114 18:45:52 @agent_ppo2.py:189][0m |          -0.0102 |           7.0669 |           3.6177 |
[32m[20230114 18:45:52 @agent_ppo2.py:189][0m |          -0.0113 |           6.9492 |           3.6156 |
[32m[20230114 18:45:52 @agent_ppo2.py:189][0m |          -0.0118 |           6.9192 |           3.6161 |
[32m[20230114 18:45:52 @agent_ppo2.py:134][0m Policy update time: 0.94 s
[32m[20230114 18:45:52 @agent_ppo2.py:142][0m Average TRAINING episode reward: 246.44
[32m[20230114 18:45:52 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 251.40
[32m[20230114 18:45:52 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 272.55
[32m[20230114 18:45:52 @agent_ppo2.py:147][0m Total time:       3.80 min
[32m[20230114 18:45:52 @agent_ppo2.py:149][0m 276480 total steps have happened
[32m[20230114 18:45:52 @agent_ppo2.py:125][0m #------------------------ Iteration 135 --------------------------#
[32m[20230114 18:45:52 @agent_ppo2.py:131][0m Sampling time: 0.27 s by 4 slaves
[32m[20230114 18:45:52 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:45:53 @agent_ppo2.py:189][0m |           0.0012 |          10.6800 |           3.6888 |
[32m[20230114 18:45:53 @agent_ppo2.py:189][0m |          -0.0042 |           8.1423 |           3.6809 |
[32m[20230114 18:45:53 @agent_ppo2.py:189][0m |          -0.0064 |           7.6260 |           3.6798 |
[32m[20230114 18:45:53 @agent_ppo2.py:189][0m |          -0.0066 |           7.3526 |           3.6733 |
[32m[20230114 18:45:53 @agent_ppo2.py:189][0m |          -0.0076 |           7.2418 |           3.6752 |
[32m[20230114 18:45:53 @agent_ppo2.py:189][0m |          -0.0087 |           7.1117 |           3.6742 |
[32m[20230114 18:45:53 @agent_ppo2.py:189][0m |          -0.0093 |           7.0710 |           3.6720 |
[32m[20230114 18:45:53 @agent_ppo2.py:189][0m |          -0.0100 |           6.9585 |           3.6714 |
[32m[20230114 18:45:53 @agent_ppo2.py:189][0m |          -0.0104 |           6.9858 |           3.6703 |
[32m[20230114 18:45:53 @agent_ppo2.py:189][0m |          -0.0108 |           6.8432 |           3.6698 |
[32m[20230114 18:45:53 @agent_ppo2.py:134][0m Policy update time: 0.90 s
[32m[20230114 18:45:54 @agent_ppo2.py:142][0m Average TRAINING episode reward: 208.88
[32m[20230114 18:45:54 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 247.27
[32m[20230114 18:45:54 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 267.29
[32m[20230114 18:45:54 @agent_ppo2.py:147][0m Total time:       3.82 min
[32m[20230114 18:45:54 @agent_ppo2.py:149][0m 278528 total steps have happened
[32m[20230114 18:45:54 @agent_ppo2.py:125][0m #------------------------ Iteration 136 --------------------------#
[32m[20230114 18:45:54 @agent_ppo2.py:131][0m Sampling time: 0.31 s by 4 slaves
[32m[20230114 18:45:54 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:45:54 @agent_ppo2.py:189][0m |           0.0010 |          13.0762 |           3.7071 |
[32m[20230114 18:45:54 @agent_ppo2.py:189][0m |          -0.0016 |          10.8198 |           3.7048 |
[32m[20230114 18:45:54 @agent_ppo2.py:189][0m |          -0.0021 |          10.0968 |           3.7062 |
[32m[20230114 18:45:54 @agent_ppo2.py:189][0m |          -0.0049 |           9.5483 |           3.7025 |
[32m[20230114 18:45:54 @agent_ppo2.py:189][0m |          -0.0055 |           9.2168 |           3.7023 |
[32m[20230114 18:45:54 @agent_ppo2.py:189][0m |          -0.0062 |           9.0088 |           3.7037 |
[32m[20230114 18:45:55 @agent_ppo2.py:189][0m |          -0.0067 |           8.8360 |           3.7035 |
[32m[20230114 18:45:55 @agent_ppo2.py:189][0m |          -0.0068 |           8.7528 |           3.7022 |
[32m[20230114 18:45:55 @agent_ppo2.py:189][0m |          -0.0046 |           8.6088 |           3.7016 |
[32m[20230114 18:45:55 @agent_ppo2.py:189][0m |          -0.0065 |           8.4636 |           3.7014 |
[32m[20230114 18:45:55 @agent_ppo2.py:134][0m Policy update time: 0.98 s
[32m[20230114 18:45:55 @agent_ppo2.py:142][0m Average TRAINING episode reward: 185.83
[32m[20230114 18:45:55 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 247.84
[32m[20230114 18:45:55 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 271.29
[32m[20230114 18:45:55 @agent_ppo2.py:147][0m Total time:       3.85 min
[32m[20230114 18:45:55 @agent_ppo2.py:149][0m 280576 total steps have happened
[32m[20230114 18:45:55 @agent_ppo2.py:125][0m #------------------------ Iteration 137 --------------------------#
[32m[20230114 18:45:55 @agent_ppo2.py:131][0m Sampling time: 0.29 s by 4 slaves
[32m[20230114 18:45:55 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:45:56 @agent_ppo2.py:189][0m |          -0.0004 |           8.7956 |           3.6464 |
[32m[20230114 18:45:56 @agent_ppo2.py:189][0m |          -0.0045 |           8.2567 |           3.6436 |
[32m[20230114 18:45:56 @agent_ppo2.py:189][0m |          -0.0072 |           8.0837 |           3.6419 |
[32m[20230114 18:45:56 @agent_ppo2.py:189][0m |          -0.0082 |           7.9737 |           3.6464 |
[32m[20230114 18:45:56 @agent_ppo2.py:189][0m |          -0.0079 |           7.8946 |           3.6453 |
[32m[20230114 18:45:56 @agent_ppo2.py:189][0m |          -0.0088 |           7.8299 |           3.6490 |
[32m[20230114 18:45:56 @agent_ppo2.py:189][0m |          -0.0098 |           7.7859 |           3.6484 |
[32m[20230114 18:45:56 @agent_ppo2.py:189][0m |          -0.0090 |           7.7847 |           3.6511 |
[32m[20230114 18:45:56 @agent_ppo2.py:189][0m |          -0.0094 |           7.7080 |           3.6507 |
[32m[20230114 18:45:56 @agent_ppo2.py:189][0m |          -0.0112 |           7.6656 |           3.6536 |
[32m[20230114 18:45:56 @agent_ppo2.py:134][0m Policy update time: 0.89 s
[32m[20230114 18:45:56 @agent_ppo2.py:142][0m Average TRAINING episode reward: 250.18
[32m[20230114 18:45:56 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 256.36
[32m[20230114 18:45:56 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 270.30
[32m[20230114 18:45:56 @agent_ppo2.py:147][0m Total time:       3.87 min
[32m[20230114 18:45:56 @agent_ppo2.py:149][0m 282624 total steps have happened
[32m[20230114 18:45:56 @agent_ppo2.py:125][0m #------------------------ Iteration 138 --------------------------#
[32m[20230114 18:45:57 @agent_ppo2.py:131][0m Sampling time: 0.26 s by 4 slaves
[32m[20230114 18:45:57 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:45:57 @agent_ppo2.py:189][0m |           0.0004 |           7.3296 |           3.6937 |
[32m[20230114 18:45:57 @agent_ppo2.py:189][0m |          -0.0041 |           6.9986 |           3.6914 |
[32m[20230114 18:45:57 @agent_ppo2.py:189][0m |          -0.0055 |           6.9123 |           3.6875 |
[32m[20230114 18:45:57 @agent_ppo2.py:189][0m |          -0.0064 |           6.8101 |           3.6909 |
[32m[20230114 18:45:57 @agent_ppo2.py:189][0m |          -0.0087 |           6.7101 |           3.6904 |
[32m[20230114 18:45:57 @agent_ppo2.py:189][0m |          -0.0085 |           6.6463 |           3.6880 |
[32m[20230114 18:45:57 @agent_ppo2.py:189][0m |          -0.0089 |           6.5870 |           3.6894 |
[32m[20230114 18:45:57 @agent_ppo2.py:189][0m |          -0.0091 |           6.5309 |           3.6881 |
[32m[20230114 18:45:58 @agent_ppo2.py:189][0m |          -0.0106 |           6.4980 |           3.6895 |
[32m[20230114 18:45:58 @agent_ppo2.py:189][0m |          -0.0107 |           6.4440 |           3.6899 |
[32m[20230114 18:45:58 @agent_ppo2.py:134][0m Policy update time: 0.92 s
[32m[20230114 18:45:58 @agent_ppo2.py:142][0m Average TRAINING episode reward: 249.49
[32m[20230114 18:45:58 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 252.29
[32m[20230114 18:45:58 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 272.37
[32m[20230114 18:45:58 @agent_ppo2.py:147][0m Total time:       3.90 min
[32m[20230114 18:45:58 @agent_ppo2.py:149][0m 284672 total steps have happened
[32m[20230114 18:45:58 @agent_ppo2.py:125][0m #------------------------ Iteration 139 --------------------------#
[32m[20230114 18:45:58 @agent_ppo2.py:131][0m Sampling time: 0.29 s by 4 slaves
[32m[20230114 18:45:58 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:45:58 @agent_ppo2.py:189][0m |           0.0023 |           8.4850 |           3.7238 |
[32m[20230114 18:45:58 @agent_ppo2.py:189][0m |          -0.0005 |           8.2495 |           3.7223 |
[32m[20230114 18:45:58 @agent_ppo2.py:189][0m |          -0.0023 |           8.1285 |           3.7213 |
[32m[20230114 18:45:59 @agent_ppo2.py:189][0m |          -0.0036 |           8.0713 |           3.7217 |
[32m[20230114 18:45:59 @agent_ppo2.py:189][0m |          -0.0036 |           8.0027 |           3.7184 |
[32m[20230114 18:45:59 @agent_ppo2.py:189][0m |          -0.0047 |           7.9471 |           3.7203 |
[32m[20230114 18:45:59 @agent_ppo2.py:189][0m |          -0.0035 |           7.9425 |           3.7194 |
[32m[20230114 18:45:59 @agent_ppo2.py:189][0m |          -0.0064 |           7.8611 |           3.7170 |
[32m[20230114 18:45:59 @agent_ppo2.py:189][0m |          -0.0063 |           7.8113 |           3.7227 |
[32m[20230114 18:45:59 @agent_ppo2.py:189][0m |          -0.0061 |           7.7606 |           3.7175 |
[32m[20230114 18:45:59 @agent_ppo2.py:134][0m Policy update time: 0.91 s
[32m[20230114 18:45:59 @agent_ppo2.py:142][0m Average TRAINING episode reward: 247.67
[32m[20230114 18:45:59 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 251.59
[32m[20230114 18:45:59 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 269.66
[32m[20230114 18:45:59 @agent_ppo2.py:147][0m Total time:       3.92 min
[32m[20230114 18:45:59 @agent_ppo2.py:149][0m 286720 total steps have happened
[32m[20230114 18:45:59 @agent_ppo2.py:125][0m #------------------------ Iteration 140 --------------------------#
[32m[20230114 18:46:00 @agent_ppo2.py:131][0m Sampling time: 0.26 s by 4 slaves
[32m[20230114 18:46:00 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:46:00 @agent_ppo2.py:189][0m |           0.0023 |           8.4554 |           3.7282 |
[32m[20230114 18:46:00 @agent_ppo2.py:189][0m |          -0.0037 |           8.1186 |           3.7250 |
[32m[20230114 18:46:00 @agent_ppo2.py:189][0m |          -0.0055 |           8.0049 |           3.7215 |
[32m[20230114 18:46:00 @agent_ppo2.py:189][0m |          -0.0118 |           7.9636 |           3.7203 |
[32m[20230114 18:46:00 @agent_ppo2.py:189][0m |          -0.0080 |           7.8908 |           3.7179 |
[32m[20230114 18:46:00 @agent_ppo2.py:189][0m |          -0.0096 |           7.8709 |           3.7176 |
[32m[20230114 18:46:00 @agent_ppo2.py:189][0m |          -0.0084 |           7.8130 |           3.7179 |
[32m[20230114 18:46:00 @agent_ppo2.py:189][0m |          -0.0077 |           7.7576 |           3.7142 |
[32m[20230114 18:46:00 @agent_ppo2.py:189][0m |          -0.0045 |           7.7516 |           3.7171 |
[32m[20230114 18:46:00 @agent_ppo2.py:189][0m |           0.0003 |           7.6871 |           3.7161 |
[32m[20230114 18:46:00 @agent_ppo2.py:134][0m Policy update time: 0.88 s
[32m[20230114 18:46:01 @agent_ppo2.py:142][0m Average TRAINING episode reward: 251.44
[32m[20230114 18:46:01 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 254.25
[32m[20230114 18:46:01 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 273.48
[32m[20230114 18:46:01 @agent_ppo2.py:147][0m Total time:       3.94 min
[32m[20230114 18:46:01 @agent_ppo2.py:149][0m 288768 total steps have happened
[32m[20230114 18:46:01 @agent_ppo2.py:125][0m #------------------------ Iteration 141 --------------------------#
[32m[20230114 18:46:01 @agent_ppo2.py:131][0m Sampling time: 0.26 s by 4 slaves
[32m[20230114 18:46:01 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:46:01 @agent_ppo2.py:189][0m |          -0.0007 |           8.2537 |           3.6939 |
[32m[20230114 18:46:01 @agent_ppo2.py:189][0m |          -0.0036 |           7.9323 |           3.6928 |
[32m[20230114 18:46:01 @agent_ppo2.py:189][0m |          -0.0056 |           7.6995 |           3.6878 |
[32m[20230114 18:46:01 @agent_ppo2.py:189][0m |          -0.0086 |           7.4856 |           3.6890 |
[32m[20230114 18:46:01 @agent_ppo2.py:189][0m |          -0.0085 |           7.3698 |           3.6892 |
[32m[20230114 18:46:01 @agent_ppo2.py:189][0m |          -0.0093 |           7.2886 |           3.6851 |
[32m[20230114 18:46:02 @agent_ppo2.py:189][0m |          -0.0101 |           7.1894 |           3.6866 |
[32m[20230114 18:46:02 @agent_ppo2.py:189][0m |          -0.0112 |           7.0705 |           3.6863 |
[32m[20230114 18:46:02 @agent_ppo2.py:189][0m |          -0.0108 |           6.9900 |           3.6839 |
[32m[20230114 18:46:02 @agent_ppo2.py:189][0m |          -0.0113 |           6.9243 |           3.6798 |
[32m[20230114 18:46:02 @agent_ppo2.py:134][0m Policy update time: 0.89 s
[32m[20230114 18:46:02 @agent_ppo2.py:142][0m Average TRAINING episode reward: 250.33
[32m[20230114 18:46:02 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 254.49
[32m[20230114 18:46:02 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 271.80
[32m[20230114 18:46:02 @agent_ppo2.py:147][0m Total time:       3.97 min
[32m[20230114 18:46:02 @agent_ppo2.py:149][0m 290816 total steps have happened
[32m[20230114 18:46:02 @agent_ppo2.py:125][0m #------------------------ Iteration 142 --------------------------#
[32m[20230114 18:46:02 @agent_ppo2.py:131][0m Sampling time: 0.26 s by 4 slaves
[32m[20230114 18:46:02 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:46:02 @agent_ppo2.py:189][0m |           0.0010 |           8.3787 |           3.7129 |
[32m[20230114 18:46:03 @agent_ppo2.py:189][0m |          -0.0038 |           7.5190 |           3.7069 |
[32m[20230114 18:46:03 @agent_ppo2.py:189][0m |          -0.0046 |           7.2829 |           3.7113 |
[32m[20230114 18:46:03 @agent_ppo2.py:189][0m |          -0.0049 |           7.1340 |           3.7087 |
[32m[20230114 18:46:03 @agent_ppo2.py:189][0m |          -0.0065 |           6.9494 |           3.7113 |
[32m[20230114 18:46:03 @agent_ppo2.py:189][0m |          -0.0062 |           6.7677 |           3.7134 |
[32m[20230114 18:46:03 @agent_ppo2.py:189][0m |          -0.0069 |           6.6544 |           3.7141 |
[32m[20230114 18:46:03 @agent_ppo2.py:189][0m |          -0.0076 |           6.4923 |           3.7194 |
[32m[20230114 18:46:03 @agent_ppo2.py:189][0m |          -0.0083 |           6.4344 |           3.7205 |
[32m[20230114 18:46:03 @agent_ppo2.py:189][0m |          -0.0087 |           6.3348 |           3.7209 |
[32m[20230114 18:46:03 @agent_ppo2.py:134][0m Policy update time: 0.93 s
[32m[20230114 18:46:03 @agent_ppo2.py:142][0m Average TRAINING episode reward: 250.19
[32m[20230114 18:46:03 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 255.17
[32m[20230114 18:46:03 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 268.28
[32m[20230114 18:46:03 @agent_ppo2.py:147][0m Total time:       3.99 min
[32m[20230114 18:46:03 @agent_ppo2.py:149][0m 292864 total steps have happened
[32m[20230114 18:46:03 @agent_ppo2.py:125][0m #------------------------ Iteration 143 --------------------------#
[32m[20230114 18:46:04 @agent_ppo2.py:131][0m Sampling time: 0.29 s by 4 slaves
[32m[20230114 18:46:04 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:46:04 @agent_ppo2.py:189][0m |           0.0007 |           8.9686 |           3.8253 |
[32m[20230114 18:46:04 @agent_ppo2.py:189][0m |          -0.0019 |           8.3720 |           3.8230 |
[32m[20230114 18:46:04 @agent_ppo2.py:189][0m |          -0.0035 |           8.1906 |           3.8190 |
[32m[20230114 18:46:04 @agent_ppo2.py:189][0m |          -0.0037 |           8.0697 |           3.8175 |
[32m[20230114 18:46:04 @agent_ppo2.py:189][0m |          -0.0041 |           7.9785 |           3.8184 |
[32m[20230114 18:46:04 @agent_ppo2.py:189][0m |          -0.0048 |           7.8694 |           3.8161 |
[32m[20230114 18:46:04 @agent_ppo2.py:189][0m |          -0.0052 |           7.7985 |           3.8114 |
[32m[20230114 18:46:05 @agent_ppo2.py:189][0m |          -0.0060 |           7.7085 |           3.8130 |
[32m[20230114 18:46:05 @agent_ppo2.py:189][0m |          -0.0065 |           7.6321 |           3.8114 |
[32m[20230114 18:46:05 @agent_ppo2.py:189][0m |          -0.0071 |           7.5591 |           3.8096 |
[32m[20230114 18:46:05 @agent_ppo2.py:134][0m Policy update time: 0.96 s
[32m[20230114 18:46:05 @agent_ppo2.py:142][0m Average TRAINING episode reward: 242.75
[32m[20230114 18:46:05 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 247.80
[32m[20230114 18:46:05 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 273.55
[32m[20230114 18:46:05 @agent_ppo2.py:147][0m Total time:       4.01 min
[32m[20230114 18:46:05 @agent_ppo2.py:149][0m 294912 total steps have happened
[32m[20230114 18:46:05 @agent_ppo2.py:125][0m #------------------------ Iteration 144 --------------------------#
[32m[20230114 18:46:05 @agent_ppo2.py:131][0m Sampling time: 0.27 s by 4 slaves
[32m[20230114 18:46:05 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:46:05 @agent_ppo2.py:189][0m |          -0.0006 |           8.5759 |           3.7001 |
[32m[20230114 18:46:06 @agent_ppo2.py:189][0m |          -0.0063 |           8.2303 |           3.6967 |
[32m[20230114 18:46:06 @agent_ppo2.py:189][0m |          -0.0084 |           8.1024 |           3.6985 |
[32m[20230114 18:46:06 @agent_ppo2.py:189][0m |          -0.0098 |           8.0231 |           3.6958 |
[32m[20230114 18:46:06 @agent_ppo2.py:189][0m |          -0.0107 |           7.9185 |           3.6974 |
[32m[20230114 18:46:06 @agent_ppo2.py:189][0m |          -0.0112 |           7.8721 |           3.6955 |
[32m[20230114 18:46:06 @agent_ppo2.py:189][0m |          -0.0113 |           7.8111 |           3.6959 |
[32m[20230114 18:46:06 @agent_ppo2.py:189][0m |          -0.0127 |           7.7817 |           3.6969 |
[32m[20230114 18:46:06 @agent_ppo2.py:189][0m |          -0.0117 |           7.7359 |           3.6975 |
[32m[20230114 18:46:06 @agent_ppo2.py:189][0m |          -0.0126 |           7.6986 |           3.6988 |
[32m[20230114 18:46:06 @agent_ppo2.py:134][0m Policy update time: 1.00 s
[32m[20230114 18:46:06 @agent_ppo2.py:142][0m Average TRAINING episode reward: 250.91
[32m[20230114 18:46:06 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 256.36
[32m[20230114 18:46:06 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 274.91
[32m[20230114 18:46:06 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 274.91
[32m[20230114 18:46:06 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 274.91
[32m[20230114 18:46:06 @agent_ppo2.py:147][0m Total time:       4.04 min
[32m[20230114 18:46:06 @agent_ppo2.py:149][0m 296960 total steps have happened
[32m[20230114 18:46:06 @agent_ppo2.py:125][0m #------------------------ Iteration 145 --------------------------#
[32m[20230114 18:46:07 @agent_ppo2.py:131][0m Sampling time: 0.29 s by 4 slaves
[32m[20230114 18:46:07 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:46:07 @agent_ppo2.py:189][0m |           0.0005 |           8.9242 |           3.7522 |
[32m[20230114 18:46:07 @agent_ppo2.py:189][0m |          -0.0026 |           8.5974 |           3.7453 |
[32m[20230114 18:46:07 @agent_ppo2.py:189][0m |          -0.0049 |           8.4419 |           3.7513 |
[32m[20230114 18:46:07 @agent_ppo2.py:189][0m |          -0.0062 |           8.3437 |           3.7552 |
[32m[20230114 18:46:07 @agent_ppo2.py:189][0m |          -0.0060 |           8.2670 |           3.7591 |
[32m[20230114 18:46:07 @agent_ppo2.py:189][0m |          -0.0066 |           8.2148 |           3.7548 |
[32m[20230114 18:46:08 @agent_ppo2.py:189][0m |          -0.0077 |           8.1503 |           3.7626 |
[32m[20230114 18:46:08 @agent_ppo2.py:189][0m |          -0.0069 |           8.1442 |           3.7596 |
[32m[20230114 18:46:08 @agent_ppo2.py:189][0m |          -0.0086 |           8.0615 |           3.7610 |
[32m[20230114 18:46:08 @agent_ppo2.py:189][0m |          -0.0077 |           8.0299 |           3.7639 |
[32m[20230114 18:46:08 @agent_ppo2.py:134][0m Policy update time: 1.04 s
[32m[20230114 18:46:08 @agent_ppo2.py:142][0m Average TRAINING episode reward: 248.05
[32m[20230114 18:46:08 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 250.98
[32m[20230114 18:46:08 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 35.86
[32m[20230114 18:46:08 @agent_ppo2.py:147][0m Total time:       4.06 min
[32m[20230114 18:46:08 @agent_ppo2.py:149][0m 299008 total steps have happened
[32m[20230114 18:46:08 @agent_ppo2.py:125][0m #------------------------ Iteration 146 --------------------------#
[32m[20230114 18:46:08 @agent_ppo2.py:131][0m Sampling time: 0.33 s by 4 slaves
[32m[20230114 18:46:08 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:46:08 @agent_ppo2.py:189][0m |           0.0074 |          12.8917 |           3.7501 |
[32m[20230114 18:46:09 @agent_ppo2.py:189][0m |          -0.0056 |           9.8449 |           3.7440 |
[32m[20230114 18:46:09 @agent_ppo2.py:189][0m |          -0.0059 |           9.5581 |           3.7444 |
[32m[20230114 18:46:09 @agent_ppo2.py:189][0m |          -0.0018 |           9.4158 |           3.7445 |
[32m[20230114 18:46:09 @agent_ppo2.py:189][0m |          -0.0065 |           9.2146 |           3.7413 |
[32m[20230114 18:46:09 @agent_ppo2.py:189][0m |          -0.0087 |           9.1041 |           3.7421 |
[32m[20230114 18:46:09 @agent_ppo2.py:189][0m |          -0.0089 |           9.0146 |           3.7407 |
[32m[20230114 18:46:09 @agent_ppo2.py:189][0m |          -0.0091 |           9.0003 |           3.7405 |
[32m[20230114 18:46:09 @agent_ppo2.py:189][0m |          -0.0095 |           8.8312 |           3.7410 |
[32m[20230114 18:46:09 @agent_ppo2.py:189][0m |          -0.0028 |           9.0661 |           3.7373 |
[32m[20230114 18:46:09 @agent_ppo2.py:134][0m Policy update time: 0.97 s
[32m[20230114 18:46:09 @agent_ppo2.py:142][0m Average TRAINING episode reward: 188.40
[32m[20230114 18:46:09 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 253.76
[32m[20230114 18:46:09 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 277.22
[32m[20230114 18:46:09 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 277.22
[32m[20230114 18:46:09 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 277.22
[32m[20230114 18:46:09 @agent_ppo2.py:147][0m Total time:       4.09 min
[32m[20230114 18:46:09 @agent_ppo2.py:149][0m 301056 total steps have happened
[32m[20230114 18:46:09 @agent_ppo2.py:125][0m #------------------------ Iteration 147 --------------------------#
[32m[20230114 18:46:10 @agent_ppo2.py:131][0m Sampling time: 0.29 s by 4 slaves
[32m[20230114 18:46:10 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:46:10 @agent_ppo2.py:189][0m |           0.0020 |          14.6308 |           3.7817 |
[32m[20230114 18:46:10 @agent_ppo2.py:189][0m |          -0.0017 |          12.4628 |           3.7790 |
[32m[20230114 18:46:10 @agent_ppo2.py:189][0m |          -0.0045 |          11.7872 |           3.7797 |
[32m[20230114 18:46:10 @agent_ppo2.py:189][0m |          -0.0054 |          11.2386 |           3.7803 |
[32m[20230114 18:46:10 @agent_ppo2.py:189][0m |          -0.0067 |          10.9329 |           3.7768 |
[32m[20230114 18:46:10 @agent_ppo2.py:189][0m |          -0.0069 |          10.6775 |           3.7811 |
[32m[20230114 18:46:10 @agent_ppo2.py:189][0m |          -0.0080 |          10.5245 |           3.7812 |
[32m[20230114 18:46:11 @agent_ppo2.py:189][0m |          -0.0080 |          10.2660 |           3.7818 |
[32m[20230114 18:46:11 @agent_ppo2.py:189][0m |          -0.0094 |          10.0963 |           3.7831 |
[32m[20230114 18:46:11 @agent_ppo2.py:189][0m |          -0.0080 |           9.9981 |           3.7847 |
[32m[20230114 18:46:11 @agent_ppo2.py:134][0m Policy update time: 0.94 s
[32m[20230114 18:46:11 @agent_ppo2.py:142][0m Average TRAINING episode reward: 184.06
[32m[20230114 18:46:11 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 252.42
[32m[20230114 18:46:11 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 276.86
[32m[20230114 18:46:11 @agent_ppo2.py:147][0m Total time:       4.11 min
[32m[20230114 18:46:11 @agent_ppo2.py:149][0m 303104 total steps have happened
[32m[20230114 18:46:11 @agent_ppo2.py:125][0m #------------------------ Iteration 148 --------------------------#
[32m[20230114 18:46:11 @agent_ppo2.py:131][0m Sampling time: 0.26 s by 4 slaves
[32m[20230114 18:46:11 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:46:11 @agent_ppo2.py:189][0m |          -0.0023 |           9.1994 |           3.8088 |
[32m[20230114 18:46:11 @agent_ppo2.py:189][0m |          -0.0045 |           8.5356 |           3.8053 |
[32m[20230114 18:46:12 @agent_ppo2.py:189][0m |          -0.0059 |           8.3257 |           3.8079 |
[32m[20230114 18:46:12 @agent_ppo2.py:189][0m |          -0.0075 |           8.1466 |           3.8061 |
[32m[20230114 18:46:12 @agent_ppo2.py:189][0m |          -0.0076 |           7.9918 |           3.8081 |
[32m[20230114 18:46:12 @agent_ppo2.py:189][0m |          -0.0088 |           8.0596 |           3.8043 |
[32m[20230114 18:46:12 @agent_ppo2.py:189][0m |          -0.0114 |           7.7749 |           3.8087 |
[32m[20230114 18:46:12 @agent_ppo2.py:189][0m |          -0.0069 |           7.7936 |           3.8070 |
[32m[20230114 18:46:12 @agent_ppo2.py:189][0m |          -0.0116 |           7.6393 |           3.8065 |
[32m[20230114 18:46:12 @agent_ppo2.py:189][0m |          -0.0130 |           7.5442 |           3.8094 |
[32m[20230114 18:46:12 @agent_ppo2.py:134][0m Policy update time: 0.91 s
[32m[20230114 18:46:12 @agent_ppo2.py:142][0m Average TRAINING episode reward: 249.38
[32m[20230114 18:46:12 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 257.02
[32m[20230114 18:46:12 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 278.32
[32m[20230114 18:46:12 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 278.32
[32m[20230114 18:46:12 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 278.32
[32m[20230114 18:46:12 @agent_ppo2.py:147][0m Total time:       4.14 min
[32m[20230114 18:46:12 @agent_ppo2.py:149][0m 305152 total steps have happened
[32m[20230114 18:46:12 @agent_ppo2.py:125][0m #------------------------ Iteration 149 --------------------------#
[32m[20230114 18:46:13 @agent_ppo2.py:131][0m Sampling time: 0.26 s by 4 slaves
[32m[20230114 18:46:13 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:46:13 @agent_ppo2.py:189][0m |          -0.0005 |           6.8513 |           3.8427 |
[32m[20230114 18:46:13 @agent_ppo2.py:189][0m |          -0.0036 |           5.7091 |           3.8410 |
[32m[20230114 18:46:13 @agent_ppo2.py:189][0m |          -0.0056 |           5.3049 |           3.8386 |
[32m[20230114 18:46:13 @agent_ppo2.py:189][0m |          -0.0068 |           4.9615 |           3.8390 |
[32m[20230114 18:46:13 @agent_ppo2.py:189][0m |          -0.0085 |           4.7347 |           3.8340 |
[32m[20230114 18:46:13 @agent_ppo2.py:189][0m |          -0.0076 |           4.5390 |           3.8345 |
[32m[20230114 18:46:13 @agent_ppo2.py:189][0m |          -0.0073 |           4.4431 |           3.8327 |
[32m[20230114 18:46:13 @agent_ppo2.py:189][0m |          -0.0091 |           4.2357 |           3.8305 |
[32m[20230114 18:46:13 @agent_ppo2.py:189][0m |          -0.0069 |           4.1450 |           3.8314 |
[32m[20230114 18:46:14 @agent_ppo2.py:189][0m |          -0.0105 |           3.9423 |           3.8281 |
[32m[20230114 18:46:14 @agent_ppo2.py:134][0m Policy update time: 0.89 s
[32m[20230114 18:46:14 @agent_ppo2.py:142][0m Average TRAINING episode reward: 253.64
[32m[20230114 18:46:14 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 257.09
[32m[20230114 18:46:14 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 281.48
[32m[20230114 18:46:14 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 281.48
[32m[20230114 18:46:14 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 281.48
[32m[20230114 18:46:14 @agent_ppo2.py:147][0m Total time:       4.16 min
[32m[20230114 18:46:14 @agent_ppo2.py:149][0m 307200 total steps have happened
[32m[20230114 18:46:14 @agent_ppo2.py:125][0m #------------------------ Iteration 150 --------------------------#
[32m[20230114 18:46:14 @agent_ppo2.py:131][0m Sampling time: 0.26 s by 4 slaves
[32m[20230114 18:46:14 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:46:14 @agent_ppo2.py:189][0m |           0.0005 |          14.8253 |           3.7669 |
[32m[20230114 18:46:14 @agent_ppo2.py:189][0m |          -0.0029 |          11.7277 |           3.7564 |
[32m[20230114 18:46:14 @agent_ppo2.py:189][0m |          -0.0064 |          11.1960 |           3.7635 |
[32m[20230114 18:46:14 @agent_ppo2.py:189][0m |          -0.0065 |          10.9272 |           3.7616 |
[32m[20230114 18:46:14 @agent_ppo2.py:189][0m |          -0.0078 |          10.6305 |           3.7581 |
[32m[20230114 18:46:15 @agent_ppo2.py:189][0m |          -0.0083 |          10.5070 |           3.7608 |
[32m[20230114 18:46:15 @agent_ppo2.py:189][0m |          -0.0092 |          10.3823 |           3.7549 |
[32m[20230114 18:46:15 @agent_ppo2.py:189][0m |          -0.0114 |          10.1616 |           3.7629 |
[32m[20230114 18:46:15 @agent_ppo2.py:189][0m |          -0.0108 |          10.0692 |           3.7599 |
[32m[20230114 18:46:15 @agent_ppo2.py:189][0m |          -0.0122 |           9.9249 |           3.7595 |
[32m[20230114 18:46:15 @agent_ppo2.py:134][0m Policy update time: 0.86 s
[32m[20230114 18:46:15 @agent_ppo2.py:142][0m Average TRAINING episode reward: 222.13
[32m[20230114 18:46:15 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 260.26
[32m[20230114 18:46:15 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 115.39
[32m[20230114 18:46:15 @agent_ppo2.py:147][0m Total time:       4.18 min
[32m[20230114 18:46:15 @agent_ppo2.py:149][0m 309248 total steps have happened
[32m[20230114 18:46:15 @agent_ppo2.py:125][0m #------------------------ Iteration 151 --------------------------#
[32m[20230114 18:46:15 @agent_ppo2.py:131][0m Sampling time: 0.25 s by 4 slaves
[32m[20230114 18:46:15 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:46:16 @agent_ppo2.py:189][0m |           0.0007 |           9.3220 |           3.8421 |
[32m[20230114 18:46:16 @agent_ppo2.py:189][0m |          -0.0032 |           8.6774 |           3.8423 |
[32m[20230114 18:46:16 @agent_ppo2.py:189][0m |          -0.0044 |           8.3530 |           3.8437 |
[32m[20230114 18:46:16 @agent_ppo2.py:189][0m |          -0.0057 |           8.1203 |           3.8441 |
[32m[20230114 18:46:16 @agent_ppo2.py:189][0m |          -0.0064 |           7.9284 |           3.8410 |
[32m[20230114 18:46:16 @agent_ppo2.py:189][0m |          -0.0075 |           7.7825 |           3.8449 |
[32m[20230114 18:46:16 @agent_ppo2.py:189][0m |          -0.0089 |           7.6002 |           3.8465 |
[32m[20230114 18:46:16 @agent_ppo2.py:189][0m |          -0.0086 |           7.5000 |           3.8465 |
[32m[20230114 18:46:16 @agent_ppo2.py:189][0m |          -0.0096 |           7.3227 |           3.8477 |
[32m[20230114 18:46:16 @agent_ppo2.py:189][0m |          -0.0098 |           7.2161 |           3.8482 |
[32m[20230114 18:46:16 @agent_ppo2.py:134][0m Policy update time: 0.88 s
[32m[20230114 18:46:17 @agent_ppo2.py:142][0m Average TRAINING episode reward: 252.83
[32m[20230114 18:46:17 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 256.58
[32m[20230114 18:46:17 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 269.64
[32m[20230114 18:46:17 @agent_ppo2.py:147][0m Total time:       4.21 min
[32m[20230114 18:46:17 @agent_ppo2.py:149][0m 311296 total steps have happened
[32m[20230114 18:46:17 @agent_ppo2.py:125][0m #------------------------ Iteration 152 --------------------------#
[32m[20230114 18:46:17 @agent_ppo2.py:131][0m Sampling time: 0.27 s by 4 slaves
[32m[20230114 18:46:17 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:46:17 @agent_ppo2.py:189][0m |           0.0005 |          10.4503 |           3.8837 |
[32m[20230114 18:46:17 @agent_ppo2.py:189][0m |          -0.0034 |           9.4815 |           3.8783 |
[32m[20230114 18:46:17 @agent_ppo2.py:189][0m |          -0.0057 |           9.1426 |           3.8766 |
[32m[20230114 18:46:17 @agent_ppo2.py:189][0m |          -0.0072 |           8.9239 |           3.8789 |
[32m[20230114 18:46:17 @agent_ppo2.py:189][0m |          -0.0082 |           8.7753 |           3.8795 |
[32m[20230114 18:46:17 @agent_ppo2.py:189][0m |          -0.0093 |           8.6381 |           3.8770 |
[32m[20230114 18:46:17 @agent_ppo2.py:189][0m |          -0.0098 |           8.5072 |           3.8804 |
[32m[20230114 18:46:18 @agent_ppo2.py:189][0m |          -0.0103 |           8.4216 |           3.8797 |
[32m[20230114 18:46:18 @agent_ppo2.py:189][0m |          -0.0112 |           8.3273 |           3.8812 |
[32m[20230114 18:46:18 @agent_ppo2.py:189][0m |          -0.0118 |           8.2362 |           3.8809 |
[32m[20230114 18:46:18 @agent_ppo2.py:134][0m Policy update time: 0.91 s
[32m[20230114 18:46:18 @agent_ppo2.py:142][0m Average TRAINING episode reward: 249.59
[32m[20230114 18:46:18 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 253.59
[32m[20230114 18:46:18 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 274.36
[32m[20230114 18:46:18 @agent_ppo2.py:147][0m Total time:       4.23 min
[32m[20230114 18:46:18 @agent_ppo2.py:149][0m 313344 total steps have happened
[32m[20230114 18:46:18 @agent_ppo2.py:125][0m #------------------------ Iteration 153 --------------------------#
[32m[20230114 18:46:18 @agent_ppo2.py:131][0m Sampling time: 0.26 s by 4 slaves
[32m[20230114 18:46:18 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:46:18 @agent_ppo2.py:189][0m |           0.0021 |           9.1271 |           3.8614 |
[32m[20230114 18:46:18 @agent_ppo2.py:189][0m |          -0.0026 |           8.5216 |           3.8649 |
[32m[20230114 18:46:19 @agent_ppo2.py:189][0m |          -0.0043 |           8.3092 |           3.8644 |
[32m[20230114 18:46:19 @agent_ppo2.py:189][0m |          -0.0049 |           8.2470 |           3.8644 |
[32m[20230114 18:46:19 @agent_ppo2.py:189][0m |           0.0024 |           8.6606 |           3.8670 |
[32m[20230114 18:46:19 @agent_ppo2.py:189][0m |          -0.0068 |           8.0658 |           3.8659 |
[32m[20230114 18:46:19 @agent_ppo2.py:189][0m |          -0.0074 |           8.0027 |           3.8679 |
[32m[20230114 18:46:19 @agent_ppo2.py:189][0m |          -0.0069 |           7.9434 |           3.8686 |
[32m[20230114 18:46:19 @agent_ppo2.py:189][0m |          -0.0082 |           7.8927 |           3.8697 |
[32m[20230114 18:46:19 @agent_ppo2.py:189][0m |          -0.0072 |           7.9089 |           3.8698 |
[32m[20230114 18:46:19 @agent_ppo2.py:134][0m Policy update time: 0.92 s
[32m[20230114 18:46:19 @agent_ppo2.py:142][0m Average TRAINING episode reward: 249.15
[32m[20230114 18:46:19 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 253.97
[32m[20230114 18:46:19 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 276.40
[32m[20230114 18:46:19 @agent_ppo2.py:147][0m Total time:       4.25 min
[32m[20230114 18:46:19 @agent_ppo2.py:149][0m 315392 total steps have happened
[32m[20230114 18:46:19 @agent_ppo2.py:125][0m #------------------------ Iteration 154 --------------------------#
[32m[20230114 18:46:20 @agent_ppo2.py:131][0m Sampling time: 0.32 s by 4 slaves
[32m[20230114 18:46:20 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:46:20 @agent_ppo2.py:189][0m |          -0.0012 |          15.2745 |           3.9240 |
[32m[20230114 18:46:20 @agent_ppo2.py:189][0m |          -0.0070 |          12.1070 |           3.9214 |
[32m[20230114 18:46:20 @agent_ppo2.py:189][0m |          -0.0042 |          11.3613 |           3.9187 |
[32m[20230114 18:46:20 @agent_ppo2.py:189][0m |          -0.0023 |          11.4561 |           3.9138 |
[32m[20230114 18:46:20 @agent_ppo2.py:189][0m |          -0.0025 |          10.1808 |           3.9130 |
[32m[20230114 18:46:20 @agent_ppo2.py:189][0m |          -0.0099 |           9.7477 |           3.9174 |
[32m[20230114 18:46:20 @agent_ppo2.py:189][0m |          -0.0093 |           9.3526 |           3.9148 |
[32m[20230114 18:46:20 @agent_ppo2.py:189][0m |          -0.0108 |           8.9683 |           3.9162 |
[32m[20230114 18:46:21 @agent_ppo2.py:189][0m |          -0.0116 |           8.7833 |           3.9107 |
[32m[20230114 18:46:21 @agent_ppo2.py:189][0m |          -0.0096 |           8.7450 |           3.9156 |
[32m[20230114 18:46:21 @agent_ppo2.py:134][0m Policy update time: 0.95 s
[32m[20230114 18:46:21 @agent_ppo2.py:142][0m Average TRAINING episode reward: 197.00
[32m[20230114 18:46:21 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 257.93
[32m[20230114 18:46:21 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 276.69
[32m[20230114 18:46:21 @agent_ppo2.py:147][0m Total time:       4.28 min
[32m[20230114 18:46:21 @agent_ppo2.py:149][0m 317440 total steps have happened
[32m[20230114 18:46:21 @agent_ppo2.py:125][0m #------------------------ Iteration 155 --------------------------#
[32m[20230114 18:46:21 @agent_ppo2.py:131][0m Sampling time: 0.27 s by 4 slaves
[32m[20230114 18:46:21 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:46:21 @agent_ppo2.py:189][0m |           0.0005 |          10.7545 |           3.9174 |
[32m[20230114 18:46:21 @agent_ppo2.py:189][0m |          -0.0051 |          10.1737 |           3.9163 |
[32m[20230114 18:46:21 @agent_ppo2.py:189][0m |          -0.0049 |           9.9300 |           3.9120 |
[32m[20230114 18:46:22 @agent_ppo2.py:189][0m |          -0.0058 |           9.7866 |           3.9112 |
[32m[20230114 18:46:22 @agent_ppo2.py:189][0m |          -0.0089 |           9.6438 |           3.9060 |
[32m[20230114 18:46:22 @agent_ppo2.py:189][0m |          -0.0096 |           9.5116 |           3.9082 |
[32m[20230114 18:46:22 @agent_ppo2.py:189][0m |          -0.0086 |           9.4071 |           3.9067 |
[32m[20230114 18:46:22 @agent_ppo2.py:189][0m |          -0.0105 |           9.3399 |           3.9055 |
[32m[20230114 18:46:22 @agent_ppo2.py:189][0m |          -0.0137 |           9.2579 |           3.9035 |
[32m[20230114 18:46:22 @agent_ppo2.py:189][0m |          -0.0062 |           9.2004 |           3.9055 |
[32m[20230114 18:46:22 @agent_ppo2.py:134][0m Policy update time: 0.91 s
[32m[20230114 18:46:22 @agent_ppo2.py:142][0m Average TRAINING episode reward: 250.64
[32m[20230114 18:46:22 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 258.24
[32m[20230114 18:46:22 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 266.37
[32m[20230114 18:46:22 @agent_ppo2.py:147][0m Total time:       4.30 min
[32m[20230114 18:46:22 @agent_ppo2.py:149][0m 319488 total steps have happened
[32m[20230114 18:46:22 @agent_ppo2.py:125][0m #------------------------ Iteration 156 --------------------------#
[32m[20230114 18:46:23 @agent_ppo2.py:131][0m Sampling time: 0.31 s by 4 slaves
[32m[20230114 18:46:23 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:46:23 @agent_ppo2.py:189][0m |           0.0012 |           9.2497 |           3.9011 |
[32m[20230114 18:46:23 @agent_ppo2.py:189][0m |          -0.0001 |           8.7911 |           3.9020 |
[32m[20230114 18:46:23 @agent_ppo2.py:189][0m |          -0.0048 |           8.5198 |           3.9039 |
[32m[20230114 18:46:23 @agent_ppo2.py:189][0m |          -0.0058 |           8.2627 |           3.9077 |
[32m[20230114 18:46:23 @agent_ppo2.py:189][0m |          -0.0062 |           8.1316 |           3.9065 |
[32m[20230114 18:46:23 @agent_ppo2.py:189][0m |          -0.0091 |           8.0348 |           3.9084 |
[32m[20230114 18:46:23 @agent_ppo2.py:189][0m |          -0.0085 |           7.9399 |           3.9089 |
[32m[20230114 18:46:23 @agent_ppo2.py:189][0m |          -0.0079 |           7.9354 |           3.9115 |
[32m[20230114 18:46:23 @agent_ppo2.py:189][0m |          -0.0103 |           7.7741 |           3.9134 |
[32m[20230114 18:46:23 @agent_ppo2.py:189][0m |          -0.0088 |           7.7278 |           3.9134 |
[32m[20230114 18:46:23 @agent_ppo2.py:134][0m Policy update time: 0.92 s
[32m[20230114 18:46:24 @agent_ppo2.py:142][0m Average TRAINING episode reward: 250.40
[32m[20230114 18:46:24 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 257.89
[32m[20230114 18:46:24 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 278.97
[32m[20230114 18:46:24 @agent_ppo2.py:147][0m Total time:       4.33 min
[32m[20230114 18:46:24 @agent_ppo2.py:149][0m 321536 total steps have happened
[32m[20230114 18:46:24 @agent_ppo2.py:125][0m #------------------------ Iteration 157 --------------------------#
[32m[20230114 18:46:24 @agent_ppo2.py:131][0m Sampling time: 0.26 s by 4 slaves
[32m[20230114 18:46:24 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:46:24 @agent_ppo2.py:189][0m |          -0.0014 |          10.1891 |           3.9711 |
[32m[20230114 18:46:24 @agent_ppo2.py:189][0m |          -0.0042 |           9.7193 |           3.9720 |
[32m[20230114 18:46:24 @agent_ppo2.py:189][0m |          -0.0043 |           9.5944 |           3.9684 |
[32m[20230114 18:46:24 @agent_ppo2.py:189][0m |           0.0023 |          10.2330 |           3.9698 |
[32m[20230114 18:46:24 @agent_ppo2.py:189][0m |          -0.0082 |           9.4415 |           3.9646 |
[32m[20230114 18:46:25 @agent_ppo2.py:189][0m |          -0.0067 |           9.4692 |           3.9684 |
[32m[20230114 18:46:25 @agent_ppo2.py:189][0m |          -0.0044 |           9.4961 |           3.9711 |
[32m[20230114 18:46:25 @agent_ppo2.py:189][0m |           0.0005 |           9.9351 |           3.9678 |
[32m[20230114 18:46:25 @agent_ppo2.py:189][0m |          -0.0020 |           9.5328 |           3.9660 |
[32m[20230114 18:46:25 @agent_ppo2.py:189][0m |          -0.0123 |           9.1824 |           3.9702 |
[32m[20230114 18:46:25 @agent_ppo2.py:134][0m Policy update time: 0.89 s
[32m[20230114 18:46:25 @agent_ppo2.py:142][0m Average TRAINING episode reward: 253.47
[32m[20230114 18:46:25 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 260.68
[32m[20230114 18:46:25 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 278.89
[32m[20230114 18:46:25 @agent_ppo2.py:147][0m Total time:       4.35 min
[32m[20230114 18:46:25 @agent_ppo2.py:149][0m 323584 total steps have happened
[32m[20230114 18:46:25 @agent_ppo2.py:125][0m #------------------------ Iteration 158 --------------------------#
[32m[20230114 18:46:25 @agent_ppo2.py:131][0m Sampling time: 0.27 s by 4 slaves
[32m[20230114 18:46:25 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:46:26 @agent_ppo2.py:189][0m |           0.0035 |           9.2768 |           4.0262 |
[32m[20230114 18:46:26 @agent_ppo2.py:189][0m |          -0.0056 |           8.6259 |           4.0252 |
[32m[20230114 18:46:26 @agent_ppo2.py:189][0m |          -0.0039 |           8.5973 |           4.0245 |
[32m[20230114 18:46:26 @agent_ppo2.py:189][0m |          -0.0056 |           8.3531 |           4.0199 |
[32m[20230114 18:46:26 @agent_ppo2.py:189][0m |          -0.0057 |           8.2665 |           4.0236 |
[32m[20230114 18:46:26 @agent_ppo2.py:189][0m |          -0.0061 |           8.2505 |           4.0184 |
[32m[20230114 18:46:26 @agent_ppo2.py:189][0m |          -0.0069 |           8.1976 |           4.0213 |
[32m[20230114 18:46:26 @agent_ppo2.py:189][0m |          -0.0085 |           7.9649 |           4.0190 |
[32m[20230114 18:46:26 @agent_ppo2.py:189][0m |          -0.0089 |           7.9146 |           4.0182 |
[32m[20230114 18:46:26 @agent_ppo2.py:189][0m |          -0.0051 |           8.1024 |           4.0208 |
[32m[20230114 18:46:26 @agent_ppo2.py:134][0m Policy update time: 0.87 s
[32m[20230114 18:46:26 @agent_ppo2.py:142][0m Average TRAINING episode reward: 254.41
[32m[20230114 18:46:26 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 258.19
[32m[20230114 18:46:26 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 274.83
[32m[20230114 18:46:26 @agent_ppo2.py:147][0m Total time:       4.37 min
[32m[20230114 18:46:26 @agent_ppo2.py:149][0m 325632 total steps have happened
[32m[20230114 18:46:26 @agent_ppo2.py:125][0m #------------------------ Iteration 159 --------------------------#
[32m[20230114 18:46:27 @agent_ppo2.py:131][0m Sampling time: 0.25 s by 4 slaves
[32m[20230114 18:46:27 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:46:27 @agent_ppo2.py:189][0m |          -0.0011 |           9.4047 |           4.0366 |
[32m[20230114 18:46:27 @agent_ppo2.py:189][0m |          -0.0062 |           8.8839 |           4.0366 |
[32m[20230114 18:46:27 @agent_ppo2.py:189][0m |          -0.0076 |           8.7041 |           4.0288 |
[32m[20230114 18:46:27 @agent_ppo2.py:189][0m |          -0.0071 |           8.6319 |           4.0237 |
[32m[20230114 18:46:27 @agent_ppo2.py:189][0m |          -0.0081 |           8.5429 |           4.0243 |
[32m[20230114 18:46:27 @agent_ppo2.py:189][0m |          -0.0101 |           8.4517 |           4.0216 |
[32m[20230114 18:46:27 @agent_ppo2.py:189][0m |          -0.0099 |           8.3945 |           4.0161 |
[32m[20230114 18:46:27 @agent_ppo2.py:189][0m |          -0.0067 |           8.5619 |           4.0161 |
[32m[20230114 18:46:27 @agent_ppo2.py:189][0m |          -0.0099 |           8.3207 |           4.0142 |
[32m[20230114 18:46:28 @agent_ppo2.py:189][0m |          -0.0116 |           8.2607 |           4.0124 |
[32m[20230114 18:46:28 @agent_ppo2.py:134][0m Policy update time: 0.87 s
[32m[20230114 18:46:28 @agent_ppo2.py:142][0m Average TRAINING episode reward: 254.62
[32m[20230114 18:46:28 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 256.30
[32m[20230114 18:46:28 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 277.07
[32m[20230114 18:46:28 @agent_ppo2.py:147][0m Total time:       4.39 min
[32m[20230114 18:46:28 @agent_ppo2.py:149][0m 327680 total steps have happened
[32m[20230114 18:46:28 @agent_ppo2.py:125][0m #------------------------ Iteration 160 --------------------------#
[32m[20230114 18:46:28 @agent_ppo2.py:131][0m Sampling time: 0.25 s by 4 slaves
[32m[20230114 18:46:28 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:46:28 @agent_ppo2.py:189][0m |          -0.0002 |           9.0481 |           3.9835 |
[32m[20230114 18:46:28 @agent_ppo2.py:189][0m |          -0.0044 |           8.8031 |           3.9771 |
[32m[20230114 18:46:28 @agent_ppo2.py:189][0m |          -0.0061 |           8.7017 |           3.9729 |
[32m[20230114 18:46:28 @agent_ppo2.py:189][0m |          -0.0081 |           8.6377 |           3.9670 |
[32m[20230114 18:46:29 @agent_ppo2.py:189][0m |          -0.0071 |           8.5905 |           3.9665 |
[32m[20230114 18:46:29 @agent_ppo2.py:189][0m |          -0.0091 |           8.5647 |           3.9670 |
[32m[20230114 18:46:29 @agent_ppo2.py:189][0m |          -0.0088 |           8.5033 |           3.9627 |
[32m[20230114 18:46:29 @agent_ppo2.py:189][0m |          -0.0034 |           9.0192 |           3.9621 |
[32m[20230114 18:46:29 @agent_ppo2.py:189][0m |          -0.0106 |           8.4205 |           3.9514 |
[32m[20230114 18:46:29 @agent_ppo2.py:189][0m |          -0.0117 |           8.3945 |           3.9543 |
[32m[20230114 18:46:29 @agent_ppo2.py:134][0m Policy update time: 0.87 s
[32m[20230114 18:46:29 @agent_ppo2.py:142][0m Average TRAINING episode reward: 255.15
[32m[20230114 18:46:29 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 256.79
[32m[20230114 18:46:29 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 268.97
[32m[20230114 18:46:29 @agent_ppo2.py:147][0m Total time:       4.42 min
[32m[20230114 18:46:29 @agent_ppo2.py:149][0m 329728 total steps have happened
[32m[20230114 18:46:29 @agent_ppo2.py:125][0m #------------------------ Iteration 161 --------------------------#
[32m[20230114 18:46:29 @agent_ppo2.py:131][0m Sampling time: 0.25 s by 4 slaves
[32m[20230114 18:46:30 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:46:30 @agent_ppo2.py:189][0m |          -0.0004 |           8.6975 |           3.9072 |
[32m[20230114 18:46:30 @agent_ppo2.py:189][0m |          -0.0057 |           8.2165 |           3.9027 |
[32m[20230114 18:46:30 @agent_ppo2.py:189][0m |          -0.0060 |           8.0853 |           3.8995 |
[32m[20230114 18:46:30 @agent_ppo2.py:189][0m |          -0.0090 |           7.8217 |           3.8954 |
[32m[20230114 18:46:30 @agent_ppo2.py:189][0m |          -0.0086 |           7.8032 |           3.8922 |
[32m[20230114 18:46:30 @agent_ppo2.py:189][0m |          -0.0111 |           7.5449 |           3.8904 |
[32m[20230114 18:46:30 @agent_ppo2.py:189][0m |          -0.0120 |           7.4477 |           3.8902 |
[32m[20230114 18:46:30 @agent_ppo2.py:189][0m |          -0.0115 |           7.3522 |           3.8898 |
[32m[20230114 18:46:30 @agent_ppo2.py:189][0m |          -0.0121 |           7.2340 |           3.8874 |
[32m[20230114 18:46:30 @agent_ppo2.py:189][0m |          -0.0126 |           7.1401 |           3.8849 |
[32m[20230114 18:46:30 @agent_ppo2.py:134][0m Policy update time: 0.89 s
[32m[20230114 18:46:31 @agent_ppo2.py:142][0m Average TRAINING episode reward: 253.02
[32m[20230114 18:46:31 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 257.68
[32m[20230114 18:46:31 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 278.32
[32m[20230114 18:46:31 @agent_ppo2.py:147][0m Total time:       4.44 min
[32m[20230114 18:46:31 @agent_ppo2.py:149][0m 331776 total steps have happened
[32m[20230114 18:46:31 @agent_ppo2.py:125][0m #------------------------ Iteration 162 --------------------------#
[32m[20230114 18:46:31 @agent_ppo2.py:131][0m Sampling time: 0.25 s by 4 slaves
[32m[20230114 18:46:31 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:46:31 @agent_ppo2.py:189][0m |           0.0020 |           9.9940 |           3.9688 |
[32m[20230114 18:46:31 @agent_ppo2.py:189][0m |          -0.0025 |           9.4398 |           3.9674 |
[32m[20230114 18:46:31 @agent_ppo2.py:189][0m |          -0.0059 |           9.1978 |           3.9631 |
[32m[20230114 18:46:31 @agent_ppo2.py:189][0m |          -0.0044 |           9.1956 |           3.9616 |
[32m[20230114 18:46:31 @agent_ppo2.py:189][0m |          -0.0073 |           9.0032 |           3.9630 |
[32m[20230114 18:46:31 @agent_ppo2.py:189][0m |          -0.0084 |           8.9351 |           3.9627 |
[32m[20230114 18:46:31 @agent_ppo2.py:189][0m |          -0.0072 |           8.9032 |           3.9615 |
[32m[20230114 18:46:32 @agent_ppo2.py:189][0m |          -0.0059 |           9.0203 |           3.9645 |
[32m[20230114 18:46:32 @agent_ppo2.py:189][0m |          -0.0087 |           8.7823 |           3.9657 |
[32m[20230114 18:46:32 @agent_ppo2.py:189][0m |          -0.0086 |           8.7471 |           3.9642 |
[32m[20230114 18:46:32 @agent_ppo2.py:134][0m Policy update time: 0.88 s
[32m[20230114 18:46:32 @agent_ppo2.py:142][0m Average TRAINING episode reward: 254.18
[32m[20230114 18:46:32 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 255.70
[32m[20230114 18:46:32 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 277.54
[32m[20230114 18:46:32 @agent_ppo2.py:147][0m Total time:       4.46 min
[32m[20230114 18:46:32 @agent_ppo2.py:149][0m 333824 total steps have happened
[32m[20230114 18:46:32 @agent_ppo2.py:125][0m #------------------------ Iteration 163 --------------------------#
[32m[20230114 18:46:32 @agent_ppo2.py:131][0m Sampling time: 0.26 s by 4 slaves
[32m[20230114 18:46:32 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:46:32 @agent_ppo2.py:189][0m |           0.0025 |           8.9432 |           3.8708 |
[32m[20230114 18:46:32 @agent_ppo2.py:189][0m |          -0.0056 |           8.0004 |           3.8711 |
[32m[20230114 18:46:32 @agent_ppo2.py:189][0m |          -0.0070 |           7.7002 |           3.8680 |
[32m[20230114 18:46:33 @agent_ppo2.py:189][0m |          -0.0080 |           7.5051 |           3.8661 |
[32m[20230114 18:46:33 @agent_ppo2.py:189][0m |          -0.0070 |           7.4098 |           3.8668 |
[32m[20230114 18:46:33 @agent_ppo2.py:189][0m |          -0.0076 |           7.2473 |           3.8683 |
[32m[20230114 18:46:33 @agent_ppo2.py:189][0m |          -0.0091 |           7.1461 |           3.8676 |
[32m[20230114 18:46:33 @agent_ppo2.py:189][0m |          -0.0099 |           7.0346 |           3.8678 |
[32m[20230114 18:46:33 @agent_ppo2.py:189][0m |          -0.0118 |           6.9306 |           3.8653 |
[32m[20230114 18:46:33 @agent_ppo2.py:189][0m |          -0.0107 |           6.8360 |           3.8665 |
[32m[20230114 18:46:33 @agent_ppo2.py:134][0m Policy update time: 0.91 s
[32m[20230114 18:46:33 @agent_ppo2.py:142][0m Average TRAINING episode reward: 249.89
[32m[20230114 18:46:33 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 255.39
[32m[20230114 18:46:33 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 278.44
[32m[20230114 18:46:33 @agent_ppo2.py:147][0m Total time:       4.49 min
[32m[20230114 18:46:33 @agent_ppo2.py:149][0m 335872 total steps have happened
[32m[20230114 18:46:33 @agent_ppo2.py:125][0m #------------------------ Iteration 164 --------------------------#
[32m[20230114 18:46:34 @agent_ppo2.py:131][0m Sampling time: 0.26 s by 4 slaves
[32m[20230114 18:46:34 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:46:34 @agent_ppo2.py:189][0m |           0.0001 |           9.1807 |           3.9353 |
[32m[20230114 18:46:34 @agent_ppo2.py:189][0m |          -0.0046 |           8.7825 |           3.9325 |
[32m[20230114 18:46:34 @agent_ppo2.py:189][0m |          -0.0061 |           8.6147 |           3.9313 |
[32m[20230114 18:46:34 @agent_ppo2.py:189][0m |          -0.0077 |           8.5176 |           3.9346 |
[32m[20230114 18:46:34 @agent_ppo2.py:189][0m |          -0.0085 |           8.4352 |           3.9332 |
[32m[20230114 18:46:34 @agent_ppo2.py:189][0m |          -0.0083 |           8.3665 |           3.9309 |
[32m[20230114 18:46:34 @agent_ppo2.py:189][0m |          -0.0097 |           8.2986 |           3.9315 |
[32m[20230114 18:46:34 @agent_ppo2.py:189][0m |          -0.0083 |           8.2903 |           3.9298 |
[32m[20230114 18:46:34 @agent_ppo2.py:189][0m |          -0.0088 |           8.2452 |           3.9311 |
[32m[20230114 18:46:34 @agent_ppo2.py:189][0m |          -0.0084 |           8.2172 |           3.9314 |
[32m[20230114 18:46:34 @agent_ppo2.py:134][0m Policy update time: 0.92 s
[32m[20230114 18:46:35 @agent_ppo2.py:142][0m Average TRAINING episode reward: 249.28
[32m[20230114 18:46:35 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 251.98
[32m[20230114 18:46:35 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 282.53
[32m[20230114 18:46:35 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 282.53
[32m[20230114 18:46:35 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 282.53
[32m[20230114 18:46:35 @agent_ppo2.py:147][0m Total time:       4.51 min
[32m[20230114 18:46:35 @agent_ppo2.py:149][0m 337920 total steps have happened
[32m[20230114 18:46:35 @agent_ppo2.py:125][0m #------------------------ Iteration 165 --------------------------#
[32m[20230114 18:46:35 @agent_ppo2.py:131][0m Sampling time: 0.26 s by 4 slaves
[32m[20230114 18:46:35 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:46:35 @agent_ppo2.py:189][0m |          -0.0004 |           9.7036 |           3.9073 |
[32m[20230114 18:46:35 @agent_ppo2.py:189][0m |          -0.0033 |           9.3870 |           3.9034 |
[32m[20230114 18:46:35 @agent_ppo2.py:189][0m |          -0.0051 |           9.2182 |           3.8988 |
[32m[20230114 18:46:35 @agent_ppo2.py:189][0m |          -0.0069 |           9.0623 |           3.9060 |
[32m[20230114 18:46:35 @agent_ppo2.py:189][0m |          -0.0063 |           8.9973 |           3.9059 |
[32m[20230114 18:46:35 @agent_ppo2.py:189][0m |          -0.0080 |           8.8042 |           3.9069 |
[32m[20230114 18:46:36 @agent_ppo2.py:189][0m |          -0.0081 |           8.8042 |           3.9124 |
[32m[20230114 18:46:36 @agent_ppo2.py:189][0m |          -0.0095 |           8.6649 |           3.9110 |
[32m[20230114 18:46:36 @agent_ppo2.py:189][0m |          -0.0087 |           8.5941 |           3.9109 |
[32m[20230114 18:46:36 @agent_ppo2.py:189][0m |          -0.0098 |           8.4726 |           3.9096 |
[32m[20230114 18:46:36 @agent_ppo2.py:134][0m Policy update time: 0.87 s
[32m[20230114 18:46:36 @agent_ppo2.py:142][0m Average TRAINING episode reward: 254.17
[32m[20230114 18:46:36 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 257.95
[32m[20230114 18:46:36 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 280.27
[32m[20230114 18:46:36 @agent_ppo2.py:147][0m Total time:       4.53 min
[32m[20230114 18:46:36 @agent_ppo2.py:149][0m 339968 total steps have happened
[32m[20230114 18:46:36 @agent_ppo2.py:125][0m #------------------------ Iteration 166 --------------------------#
[32m[20230114 18:46:36 @agent_ppo2.py:131][0m Sampling time: 0.32 s by 4 slaves
[32m[20230114 18:46:36 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:46:36 @agent_ppo2.py:189][0m |          -0.0025 |          17.6667 |           3.9990 |
[32m[20230114 18:46:37 @agent_ppo2.py:189][0m |           0.0004 |          13.5140 |           3.9992 |
[32m[20230114 18:46:37 @agent_ppo2.py:189][0m |          -0.0081 |          11.8016 |           3.9998 |
[32m[20230114 18:46:37 @agent_ppo2.py:189][0m |          -0.0104 |          11.0473 |           3.9969 |
[32m[20230114 18:46:37 @agent_ppo2.py:189][0m |          -0.0108 |          10.6810 |           3.9916 |
[32m[20230114 18:46:37 @agent_ppo2.py:189][0m |          -0.0111 |          10.3379 |           3.9913 |
[32m[20230114 18:46:37 @agent_ppo2.py:189][0m |          -0.0119 |          10.1436 |           3.9907 |
[32m[20230114 18:46:37 @agent_ppo2.py:189][0m |          -0.0134 |          10.0283 |           3.9909 |
[32m[20230114 18:46:37 @agent_ppo2.py:189][0m |          -0.0131 |           9.8791 |           3.9889 |
[32m[20230114 18:46:37 @agent_ppo2.py:189][0m |          -0.0130 |           9.5562 |           3.9885 |
[32m[20230114 18:46:37 @agent_ppo2.py:134][0m Policy update time: 0.87 s
[32m[20230114 18:46:37 @agent_ppo2.py:142][0m Average TRAINING episode reward: 166.59
[32m[20230114 18:46:37 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 258.06
[32m[20230114 18:46:37 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 279.39
[32m[20230114 18:46:37 @agent_ppo2.py:147][0m Total time:       4.55 min
[32m[20230114 18:46:37 @agent_ppo2.py:149][0m 342016 total steps have happened
[32m[20230114 18:46:37 @agent_ppo2.py:125][0m #------------------------ Iteration 167 --------------------------#
[32m[20230114 18:46:38 @agent_ppo2.py:131][0m Sampling time: 0.25 s by 4 slaves
[32m[20230114 18:46:38 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:46:38 @agent_ppo2.py:189][0m |           0.0018 |          10.8738 |           3.9958 |
[32m[20230114 18:46:38 @agent_ppo2.py:189][0m |          -0.0053 |          10.0943 |           4.0009 |
[32m[20230114 18:46:38 @agent_ppo2.py:189][0m |          -0.0046 |           9.9261 |           4.0008 |
[32m[20230114 18:46:38 @agent_ppo2.py:189][0m |          -0.0025 |           9.8233 |           3.9943 |
[32m[20230114 18:46:38 @agent_ppo2.py:189][0m |          -0.0070 |           9.7252 |           3.9972 |
[32m[20230114 18:46:38 @agent_ppo2.py:189][0m |          -0.0101 |           9.6864 |           3.9978 |
[32m[20230114 18:46:38 @agent_ppo2.py:189][0m |          -0.0064 |           9.6330 |           3.9963 |
[32m[20230114 18:46:38 @agent_ppo2.py:189][0m |          -0.0076 |           9.6430 |           3.9977 |
[32m[20230114 18:46:38 @agent_ppo2.py:189][0m |          -0.0110 |           9.5326 |           3.9960 |
[32m[20230114 18:46:39 @agent_ppo2.py:189][0m |          -0.0092 |           9.5813 |           3.9959 |
[32m[20230114 18:46:39 @agent_ppo2.py:134][0m Policy update time: 0.88 s
[32m[20230114 18:46:39 @agent_ppo2.py:142][0m Average TRAINING episode reward: 253.33
[32m[20230114 18:46:39 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 255.30
[32m[20230114 18:46:39 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 277.48
[32m[20230114 18:46:39 @agent_ppo2.py:147][0m Total time:       4.58 min
[32m[20230114 18:46:39 @agent_ppo2.py:149][0m 344064 total steps have happened
[32m[20230114 18:46:39 @agent_ppo2.py:125][0m #------------------------ Iteration 168 --------------------------#
[32m[20230114 18:46:39 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:46:39 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:46:39 @agent_ppo2.py:189][0m |           0.0008 |          32.3917 |           4.0196 |
[32m[20230114 18:46:39 @agent_ppo2.py:189][0m |          -0.0042 |          21.9890 |           4.0168 |
[32m[20230114 18:46:39 @agent_ppo2.py:189][0m |          -0.0041 |          20.3487 |           4.0125 |
[32m[20230114 18:46:39 @agent_ppo2.py:189][0m |          -0.0068 |          18.8024 |           4.0121 |
[32m[20230114 18:46:39 @agent_ppo2.py:189][0m |          -0.0083 |          18.3714 |           4.0123 |
[32m[20230114 18:46:39 @agent_ppo2.py:189][0m |          -0.0086 |          17.2946 |           4.0119 |
[32m[20230114 18:46:39 @agent_ppo2.py:189][0m |          -0.0081 |          16.9473 |           4.0099 |
[32m[20230114 18:46:40 @agent_ppo2.py:189][0m |          -0.0083 |          16.3801 |           4.0107 |
[32m[20230114 18:46:40 @agent_ppo2.py:189][0m |          -0.0098 |          16.0746 |           4.0117 |
[32m[20230114 18:46:40 @agent_ppo2.py:189][0m |          -0.0113 |          15.7524 |           4.0099 |
[32m[20230114 18:46:40 @agent_ppo2.py:134][0m Policy update time: 0.64 s
[32m[20230114 18:46:40 @agent_ppo2.py:142][0m Average TRAINING episode reward: 131.54
[32m[20230114 18:46:40 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 257.18
[32m[20230114 18:46:40 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 276.29
[32m[20230114 18:46:40 @agent_ppo2.py:147][0m Total time:       4.60 min
[32m[20230114 18:46:40 @agent_ppo2.py:149][0m 346112 total steps have happened
[32m[20230114 18:46:40 @agent_ppo2.py:125][0m #------------------------ Iteration 169 --------------------------#
[32m[20230114 18:46:40 @agent_ppo2.py:131][0m Sampling time: 0.26 s by 4 slaves
[32m[20230114 18:46:40 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:46:40 @agent_ppo2.py:189][0m |           0.0003 |          28.5528 |           4.0347 |
[32m[20230114 18:46:40 @agent_ppo2.py:189][0m |          -0.0049 |          22.3509 |           4.0332 |
[32m[20230114 18:46:40 @agent_ppo2.py:189][0m |          -0.0060 |          19.6384 |           4.0318 |
[32m[20230114 18:46:41 @agent_ppo2.py:189][0m |          -0.0083 |          17.7758 |           4.0300 |
[32m[20230114 18:46:41 @agent_ppo2.py:189][0m |          -0.0099 |          16.4914 |           4.0324 |
[32m[20230114 18:46:41 @agent_ppo2.py:189][0m |          -0.0108 |          15.5891 |           4.0277 |
[32m[20230114 18:46:41 @agent_ppo2.py:189][0m |          -0.0113 |          14.9175 |           4.0260 |
[32m[20230114 18:46:41 @agent_ppo2.py:189][0m |          -0.0122 |          14.4431 |           4.0263 |
[32m[20230114 18:46:41 @agent_ppo2.py:189][0m |          -0.0125 |          13.8762 |           4.0248 |
[32m[20230114 18:46:41 @agent_ppo2.py:189][0m |          -0.0138 |          13.3486 |           4.0258 |
[32m[20230114 18:46:41 @agent_ppo2.py:134][0m Policy update time: 0.78 s
[32m[20230114 18:46:41 @agent_ppo2.py:142][0m Average TRAINING episode reward: 189.64
[32m[20230114 18:46:41 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 253.81
[32m[20230114 18:46:41 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 151.96
[32m[20230114 18:46:41 @agent_ppo2.py:147][0m Total time:       4.62 min
[32m[20230114 18:46:41 @agent_ppo2.py:149][0m 348160 total steps have happened
[32m[20230114 18:46:41 @agent_ppo2.py:125][0m #------------------------ Iteration 170 --------------------------#
[32m[20230114 18:46:41 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:46:41 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:46:42 @agent_ppo2.py:189][0m |          -0.0011 |          10.8305 |           3.9772 |
[32m[20230114 18:46:42 @agent_ppo2.py:189][0m |          -0.0059 |          10.0200 |           3.9695 |
[32m[20230114 18:46:42 @agent_ppo2.py:189][0m |          -0.0058 |           9.7137 |           3.9724 |
[32m[20230114 18:46:42 @agent_ppo2.py:189][0m |          -0.0071 |           9.5609 |           3.9715 |
[32m[20230114 18:46:42 @agent_ppo2.py:189][0m |          -0.0084 |           9.3958 |           3.9729 |
[32m[20230114 18:46:42 @agent_ppo2.py:189][0m |          -0.0094 |           9.2150 |           3.9721 |
[32m[20230114 18:46:42 @agent_ppo2.py:189][0m |          -0.0088 |           9.1409 |           3.9692 |
[32m[20230114 18:46:42 @agent_ppo2.py:189][0m |          -0.0101 |           9.0413 |           3.9697 |
[32m[20230114 18:46:42 @agent_ppo2.py:189][0m |          -0.0091 |           8.9535 |           3.9706 |
[32m[20230114 18:46:42 @agent_ppo2.py:189][0m |          -0.0107 |           8.9091 |           3.9703 |
[32m[20230114 18:46:42 @agent_ppo2.py:134][0m Policy update time: 0.87 s
[32m[20230114 18:46:42 @agent_ppo2.py:142][0m Average TRAINING episode reward: 254.05
[32m[20230114 18:46:42 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 257.29
[32m[20230114 18:46:42 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 280.16
[32m[20230114 18:46:42 @agent_ppo2.py:147][0m Total time:       4.64 min
[32m[20230114 18:46:42 @agent_ppo2.py:149][0m 350208 total steps have happened
[32m[20230114 18:46:42 @agent_ppo2.py:125][0m #------------------------ Iteration 171 --------------------------#
[32m[20230114 18:46:43 @agent_ppo2.py:131][0m Sampling time: 0.28 s by 4 slaves
[32m[20230114 18:46:43 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:46:43 @agent_ppo2.py:189][0m |          -0.0005 |           9.9054 |           3.9052 |
[32m[20230114 18:46:43 @agent_ppo2.py:189][0m |          -0.0041 |           9.4050 |           3.9029 |
[32m[20230114 18:46:43 @agent_ppo2.py:189][0m |          -0.0054 |           9.1275 |           3.9060 |
[32m[20230114 18:46:43 @agent_ppo2.py:189][0m |          -0.0059 |           8.9392 |           3.9117 |
[32m[20230114 18:46:43 @agent_ppo2.py:189][0m |          -0.0059 |           8.8056 |           3.9075 |
[32m[20230114 18:46:43 @agent_ppo2.py:189][0m |          -0.0067 |           8.6960 |           3.9087 |
[32m[20230114 18:46:43 @agent_ppo2.py:189][0m |          -0.0058 |           8.6147 |           3.9107 |
[32m[20230114 18:46:43 @agent_ppo2.py:189][0m |          -0.0102 |           8.5825 |           3.9108 |
[32m[20230114 18:46:44 @agent_ppo2.py:189][0m |          -0.0113 |           8.4682 |           3.9139 |
[32m[20230114 18:46:44 @agent_ppo2.py:189][0m |          -0.0082 |           8.3948 |           3.9115 |
[32m[20230114 18:46:44 @agent_ppo2.py:134][0m Policy update time: 0.92 s
[32m[20230114 18:46:44 @agent_ppo2.py:142][0m Average TRAINING episode reward: 251.60
[32m[20230114 18:46:44 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 255.29
[32m[20230114 18:46:44 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 276.88
[32m[20230114 18:46:44 @agent_ppo2.py:147][0m Total time:       4.66 min
[32m[20230114 18:46:44 @agent_ppo2.py:149][0m 352256 total steps have happened
[32m[20230114 18:46:44 @agent_ppo2.py:125][0m #------------------------ Iteration 172 --------------------------#
[32m[20230114 18:46:44 @agent_ppo2.py:131][0m Sampling time: 0.26 s by 4 slaves
[32m[20230114 18:46:44 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:46:44 @agent_ppo2.py:189][0m |          -0.0008 |          10.5410 |           4.0382 |
[32m[20230114 18:46:44 @agent_ppo2.py:189][0m |          -0.0059 |          10.2894 |           4.0328 |
[32m[20230114 18:46:45 @agent_ppo2.py:189][0m |          -0.0063 |          10.1671 |           4.0352 |
[32m[20230114 18:46:45 @agent_ppo2.py:189][0m |          -0.0076 |          10.0921 |           4.0332 |
[32m[20230114 18:46:45 @agent_ppo2.py:189][0m |          -0.0087 |          10.0210 |           4.0361 |
[32m[20230114 18:46:45 @agent_ppo2.py:189][0m |          -0.0096 |           9.9540 |           4.0338 |
[32m[20230114 18:46:45 @agent_ppo2.py:189][0m |          -0.0098 |           9.9499 |           4.0310 |
[32m[20230114 18:46:45 @agent_ppo2.py:189][0m |          -0.0101 |           9.8915 |           4.0312 |
[32m[20230114 18:46:45 @agent_ppo2.py:189][0m |          -0.0108 |           9.8347 |           4.0359 |
[32m[20230114 18:46:45 @agent_ppo2.py:189][0m |          -0.0117 |           9.8205 |           4.0372 |
[32m[20230114 18:46:45 @agent_ppo2.py:134][0m Policy update time: 1.01 s
[32m[20230114 18:46:45 @agent_ppo2.py:142][0m Average TRAINING episode reward: 253.77
[32m[20230114 18:46:45 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 255.86
[32m[20230114 18:46:45 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 157.83
[32m[20230114 18:46:45 @agent_ppo2.py:147][0m Total time:       4.69 min
[32m[20230114 18:46:45 @agent_ppo2.py:149][0m 354304 total steps have happened
[32m[20230114 18:46:45 @agent_ppo2.py:125][0m #------------------------ Iteration 173 --------------------------#
[32m[20230114 18:46:46 @agent_ppo2.py:131][0m Sampling time: 0.25 s by 4 slaves
[32m[20230114 18:46:46 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:46:46 @agent_ppo2.py:189][0m |          -0.0004 |          16.3786 |           4.0675 |
[32m[20230114 18:46:46 @agent_ppo2.py:189][0m |          -0.0062 |          11.9076 |           4.0630 |
[32m[20230114 18:46:46 @agent_ppo2.py:189][0m |          -0.0087 |          11.3180 |           4.0610 |
[32m[20230114 18:46:46 @agent_ppo2.py:189][0m |          -0.0092 |          10.9766 |           4.0662 |
[32m[20230114 18:46:46 @agent_ppo2.py:189][0m |          -0.0101 |          10.6244 |           4.0664 |
[32m[20230114 18:46:46 @agent_ppo2.py:189][0m |          -0.0115 |          10.4025 |           4.0629 |
[32m[20230114 18:46:46 @agent_ppo2.py:189][0m |          -0.0130 |          10.1575 |           4.0647 |
[32m[20230114 18:46:46 @agent_ppo2.py:189][0m |          -0.0121 |          10.0086 |           4.0669 |
[32m[20230114 18:46:46 @agent_ppo2.py:189][0m |          -0.0135 |           9.8892 |           4.0687 |
[32m[20230114 18:46:46 @agent_ppo2.py:189][0m |          -0.0135 |           9.7873 |           4.0687 |
[32m[20230114 18:46:46 @agent_ppo2.py:134][0m Policy update time: 0.86 s
[32m[20230114 18:46:47 @agent_ppo2.py:142][0m Average TRAINING episode reward: 219.18
[32m[20230114 18:46:47 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 258.26
[32m[20230114 18:46:47 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 278.14
[32m[20230114 18:46:47 @agent_ppo2.py:147][0m Total time:       4.71 min
[32m[20230114 18:46:47 @agent_ppo2.py:149][0m 356352 total steps have happened
[32m[20230114 18:46:47 @agent_ppo2.py:125][0m #------------------------ Iteration 174 --------------------------#
[32m[20230114 18:46:47 @agent_ppo2.py:131][0m Sampling time: 0.25 s by 4 slaves
[32m[20230114 18:46:47 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:46:47 @agent_ppo2.py:189][0m |          -0.0017 |           9.6123 |           4.0144 |
[32m[20230114 18:46:47 @agent_ppo2.py:189][0m |          -0.0063 |           8.1490 |           4.0128 |
[32m[20230114 18:46:47 @agent_ppo2.py:189][0m |          -0.0104 |           7.4064 |           4.0132 |
[32m[20230114 18:46:47 @agent_ppo2.py:189][0m |          -0.0112 |           6.8071 |           4.0070 |
[32m[20230114 18:46:47 @agent_ppo2.py:189][0m |          -0.0144 |           6.3502 |           4.0085 |
[32m[20230114 18:46:47 @agent_ppo2.py:189][0m |          -0.0141 |           5.9102 |           4.0144 |
[32m[20230114 18:46:48 @agent_ppo2.py:189][0m |          -0.0141 |           5.5515 |           4.0131 |
[32m[20230114 18:46:48 @agent_ppo2.py:189][0m |          -0.0166 |           5.2763 |           4.0126 |
[32m[20230114 18:46:48 @agent_ppo2.py:189][0m |          -0.0061 |           5.7325 |           4.0042 |
[32m[20230114 18:46:48 @agent_ppo2.py:189][0m |          -0.0123 |           4.9400 |           4.0138 |
[32m[20230114 18:46:48 @agent_ppo2.py:134][0m Policy update time: 0.87 s
[32m[20230114 18:46:48 @agent_ppo2.py:142][0m Average TRAINING episode reward: 251.86
[32m[20230114 18:46:48 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 252.83
[32m[20230114 18:46:48 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 278.14
[32m[20230114 18:46:48 @agent_ppo2.py:147][0m Total time:       4.73 min
[32m[20230114 18:46:48 @agent_ppo2.py:149][0m 358400 total steps have happened
[32m[20230114 18:46:48 @agent_ppo2.py:125][0m #------------------------ Iteration 175 --------------------------#
[32m[20230114 18:46:48 @agent_ppo2.py:131][0m Sampling time: 0.25 s by 4 slaves
[32m[20230114 18:46:48 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:46:48 @agent_ppo2.py:189][0m |           0.0022 |          10.6490 |           4.1117 |
[32m[20230114 18:46:49 @agent_ppo2.py:189][0m |          -0.0055 |           9.8925 |           4.1039 |
[32m[20230114 18:46:49 @agent_ppo2.py:189][0m |          -0.0078 |           9.6633 |           4.1018 |
[32m[20230114 18:46:49 @agent_ppo2.py:189][0m |          -0.0081 |           9.4493 |           4.1028 |
[32m[20230114 18:46:49 @agent_ppo2.py:189][0m |          -0.0092 |           9.3425 |           4.1056 |
[32m[20230114 18:46:49 @agent_ppo2.py:189][0m |          -0.0106 |           9.1655 |           4.0997 |
[32m[20230114 18:46:49 @agent_ppo2.py:189][0m |          -0.0070 |           9.0529 |           4.0983 |
[32m[20230114 18:46:49 @agent_ppo2.py:189][0m |          -0.0092 |           8.9158 |           4.1006 |
[32m[20230114 18:46:49 @agent_ppo2.py:189][0m |          -0.0098 |           8.8169 |           4.1053 |
[32m[20230114 18:46:49 @agent_ppo2.py:189][0m |          -0.0099 |           8.7259 |           4.1075 |
[32m[20230114 18:46:49 @agent_ppo2.py:134][0m Policy update time: 0.90 s
[32m[20230114 18:46:49 @agent_ppo2.py:142][0m Average TRAINING episode reward: 250.49
[32m[20230114 18:46:49 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 255.17
[32m[20230114 18:46:49 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 277.15
[32m[20230114 18:46:49 @agent_ppo2.py:147][0m Total time:       4.75 min
[32m[20230114 18:46:49 @agent_ppo2.py:149][0m 360448 total steps have happened
[32m[20230114 18:46:49 @agent_ppo2.py:125][0m #------------------------ Iteration 176 --------------------------#
[32m[20230114 18:46:50 @agent_ppo2.py:131][0m Sampling time: 0.25 s by 4 slaves
[32m[20230114 18:46:50 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:46:50 @agent_ppo2.py:189][0m |          -0.0001 |          18.2632 |           4.0583 |
[32m[20230114 18:46:50 @agent_ppo2.py:189][0m |          -0.0053 |          15.3833 |           4.0547 |
[32m[20230114 18:46:50 @agent_ppo2.py:189][0m |          -0.0078 |          14.2974 |           4.0542 |
[32m[20230114 18:46:50 @agent_ppo2.py:189][0m |          -0.0101 |          13.6914 |           4.0519 |
[32m[20230114 18:46:50 @agent_ppo2.py:189][0m |          -0.0128 |          13.3649 |           4.0501 |
[32m[20230114 18:46:50 @agent_ppo2.py:189][0m |          -0.0130 |          13.1538 |           4.0465 |
[32m[20230114 18:46:50 @agent_ppo2.py:189][0m |          -0.0042 |          13.4653 |           4.0449 |
[32m[20230114 18:46:50 @agent_ppo2.py:189][0m |          -0.0141 |          12.8433 |           4.0449 |
[32m[20230114 18:46:50 @agent_ppo2.py:189][0m |          -0.0135 |          12.5817 |           4.0452 |
[32m[20230114 18:46:50 @agent_ppo2.py:189][0m |          -0.0148 |          12.3493 |           4.0416 |
[32m[20230114 18:46:50 @agent_ppo2.py:134][0m Policy update time: 0.75 s
[32m[20230114 18:46:51 @agent_ppo2.py:142][0m Average TRAINING episode reward: 189.42
[32m[20230114 18:46:51 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 253.40
[32m[20230114 18:46:51 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 275.61
[32m[20230114 18:46:51 @agent_ppo2.py:147][0m Total time:       4.77 min
[32m[20230114 18:46:51 @agent_ppo2.py:149][0m 362496 total steps have happened
[32m[20230114 18:46:51 @agent_ppo2.py:125][0m #------------------------ Iteration 177 --------------------------#
[32m[20230114 18:46:51 @agent_ppo2.py:131][0m Sampling time: 0.27 s by 4 slaves
[32m[20230114 18:46:51 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:46:51 @agent_ppo2.py:189][0m |          -0.0002 |          10.8227 |           4.1574 |
[32m[20230114 18:46:51 @agent_ppo2.py:189][0m |          -0.0066 |          10.3022 |           4.1541 |
[32m[20230114 18:46:51 @agent_ppo2.py:189][0m |          -0.0076 |          10.1097 |           4.1569 |
[32m[20230114 18:46:51 @agent_ppo2.py:189][0m |          -0.0086 |           9.9216 |           4.1550 |
[32m[20230114 18:46:51 @agent_ppo2.py:189][0m |          -0.0099 |           9.7974 |           4.1551 |
[32m[20230114 18:46:51 @agent_ppo2.py:189][0m |          -0.0099 |           9.7028 |           4.1555 |
[32m[20230114 18:46:52 @agent_ppo2.py:189][0m |          -0.0102 |           9.6106 |           4.1558 |
[32m[20230114 18:46:52 @agent_ppo2.py:189][0m |          -0.0106 |           9.5266 |           4.1545 |
[32m[20230114 18:46:52 @agent_ppo2.py:189][0m |          -0.0101 |           9.5436 |           4.1577 |
[32m[20230114 18:46:52 @agent_ppo2.py:189][0m |          -0.0111 |           9.4362 |           4.1556 |
[32m[20230114 18:46:52 @agent_ppo2.py:134][0m Policy update time: 0.87 s
[32m[20230114 18:46:52 @agent_ppo2.py:142][0m Average TRAINING episode reward: 253.33
[32m[20230114 18:46:52 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 256.84
[32m[20230114 18:46:52 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 274.55
[32m[20230114 18:46:52 @agent_ppo2.py:147][0m Total time:       4.80 min
[32m[20230114 18:46:52 @agent_ppo2.py:149][0m 364544 total steps have happened
[32m[20230114 18:46:52 @agent_ppo2.py:125][0m #------------------------ Iteration 178 --------------------------#
[32m[20230114 18:46:52 @agent_ppo2.py:131][0m Sampling time: 0.25 s by 4 slaves
[32m[20230114 18:46:52 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:46:52 @agent_ppo2.py:189][0m |          -0.0001 |          10.7389 |           4.0901 |
[32m[20230114 18:46:52 @agent_ppo2.py:189][0m |          -0.0030 |          10.4652 |           4.0752 |
[32m[20230114 18:46:53 @agent_ppo2.py:189][0m |          -0.0056 |          10.3317 |           4.0792 |
[32m[20230114 18:46:53 @agent_ppo2.py:189][0m |          -0.0064 |          10.2515 |           4.0725 |
[32m[20230114 18:46:53 @agent_ppo2.py:189][0m |          -0.0071 |          10.1498 |           4.0687 |
[32m[20230114 18:46:53 @agent_ppo2.py:189][0m |          -0.0071 |          10.0477 |           4.0695 |
[32m[20230114 18:46:53 @agent_ppo2.py:189][0m |          -0.0066 |           9.9513 |           4.0697 |
[32m[20230114 18:46:53 @agent_ppo2.py:189][0m |          -0.0079 |           9.8630 |           4.0688 |
[32m[20230114 18:46:53 @agent_ppo2.py:189][0m |          -0.0084 |           9.7644 |           4.0660 |
[32m[20230114 18:46:53 @agent_ppo2.py:189][0m |          -0.0087 |           9.6892 |           4.0674 |
[32m[20230114 18:46:53 @agent_ppo2.py:134][0m Policy update time: 0.85 s
[32m[20230114 18:46:53 @agent_ppo2.py:142][0m Average TRAINING episode reward: 252.44
[32m[20230114 18:46:53 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 255.80
[32m[20230114 18:46:53 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 275.78
[32m[20230114 18:46:53 @agent_ppo2.py:147][0m Total time:       4.82 min
[32m[20230114 18:46:53 @agent_ppo2.py:149][0m 366592 total steps have happened
[32m[20230114 18:46:53 @agent_ppo2.py:125][0m #------------------------ Iteration 179 --------------------------#
[32m[20230114 18:46:54 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:46:54 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:46:54 @agent_ppo2.py:189][0m |           0.0036 |           9.9889 |           4.1086 |
[32m[20230114 18:46:54 @agent_ppo2.py:189][0m |          -0.0049 |           9.3968 |           4.0990 |
[32m[20230114 18:46:54 @agent_ppo2.py:189][0m |          -0.0063 |           9.2186 |           4.0996 |
[32m[20230114 18:46:54 @agent_ppo2.py:189][0m |          -0.0076 |           9.0883 |           4.0947 |
[32m[20230114 18:46:54 @agent_ppo2.py:189][0m |          -0.0080 |           8.9585 |           4.0976 |
[32m[20230114 18:46:54 @agent_ppo2.py:189][0m |          -0.0090 |           8.9036 |           4.0926 |
[32m[20230114 18:46:54 @agent_ppo2.py:189][0m |          -0.0097 |           8.8406 |           4.0962 |
[32m[20230114 18:46:54 @agent_ppo2.py:189][0m |          -0.0084 |           8.8039 |           4.0905 |
[32m[20230114 18:46:54 @agent_ppo2.py:189][0m |          -0.0092 |           8.8201 |           4.0950 |
[32m[20230114 18:46:54 @agent_ppo2.py:189][0m |          -0.0112 |           8.6402 |           4.0966 |
[32m[20230114 18:46:54 @agent_ppo2.py:134][0m Policy update time: 0.84 s
[32m[20230114 18:46:55 @agent_ppo2.py:142][0m Average TRAINING episode reward: 256.25
[32m[20230114 18:46:55 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 258.87
[32m[20230114 18:46:55 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 273.56
[32m[20230114 18:46:55 @agent_ppo2.py:147][0m Total time:       4.84 min
[32m[20230114 18:46:55 @agent_ppo2.py:149][0m 368640 total steps have happened
[32m[20230114 18:46:55 @agent_ppo2.py:125][0m #------------------------ Iteration 180 --------------------------#
[32m[20230114 18:46:55 @agent_ppo2.py:131][0m Sampling time: 0.30 s by 4 slaves
[32m[20230114 18:46:55 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:46:55 @agent_ppo2.py:189][0m |          -0.0010 |           9.8704 |           4.0545 |
[32m[20230114 18:46:55 @agent_ppo2.py:189][0m |          -0.0044 |           9.0295 |           4.0553 |
[32m[20230114 18:46:55 @agent_ppo2.py:189][0m |          -0.0070 |           8.7041 |           4.0489 |
[32m[20230114 18:46:55 @agent_ppo2.py:189][0m |          -0.0017 |           8.4490 |           4.0550 |
[32m[20230114 18:46:55 @agent_ppo2.py:189][0m |          -0.0029 |           8.2827 |           4.0540 |
[32m[20230114 18:46:55 @agent_ppo2.py:189][0m |          -0.0042 |           8.1165 |           4.0535 |
[32m[20230114 18:46:56 @agent_ppo2.py:189][0m |          -0.0115 |           8.0058 |           4.0486 |
[32m[20230114 18:46:56 @agent_ppo2.py:189][0m |          -0.0111 |           7.8958 |           4.0588 |
[32m[20230114 18:46:56 @agent_ppo2.py:189][0m |          -0.0107 |           7.8036 |           4.0571 |
[32m[20230114 18:46:56 @agent_ppo2.py:189][0m |          -0.0108 |           7.6945 |           4.0568 |
[32m[20230114 18:46:56 @agent_ppo2.py:134][0m Policy update time: 0.86 s
[32m[20230114 18:46:56 @agent_ppo2.py:142][0m Average TRAINING episode reward: 253.60
[32m[20230114 18:46:56 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 257.05
[32m[20230114 18:46:56 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 272.73
[32m[20230114 18:46:56 @agent_ppo2.py:147][0m Total time:       4.86 min
[32m[20230114 18:46:56 @agent_ppo2.py:149][0m 370688 total steps have happened
[32m[20230114 18:46:56 @agent_ppo2.py:125][0m #------------------------ Iteration 181 --------------------------#
[32m[20230114 18:46:56 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:46:56 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:46:56 @agent_ppo2.py:189][0m |           0.0010 |          10.8234 |           4.1346 |
[32m[20230114 18:46:56 @agent_ppo2.py:189][0m |          -0.0023 |          10.2232 |           4.1263 |
[32m[20230114 18:46:57 @agent_ppo2.py:189][0m |          -0.0119 |           9.9732 |           4.1222 |
[32m[20230114 18:46:57 @agent_ppo2.py:189][0m |          -0.0049 |           9.8839 |           4.1150 |
[32m[20230114 18:46:57 @agent_ppo2.py:189][0m |          -0.0056 |           9.7820 |           4.1163 |
[32m[20230114 18:46:57 @agent_ppo2.py:189][0m |           0.0031 |           9.8061 |           4.1108 |
[32m[20230114 18:46:57 @agent_ppo2.py:189][0m |          -0.0063 |           9.7297 |           4.1086 |
[32m[20230114 18:46:57 @agent_ppo2.py:189][0m |          -0.0142 |           9.5982 |           4.1092 |
[32m[20230114 18:46:57 @agent_ppo2.py:189][0m |          -0.0151 |           9.5709 |           4.1113 |
[32m[20230114 18:46:57 @agent_ppo2.py:189][0m |          -0.0129 |           9.5258 |           4.1088 |
[32m[20230114 18:46:57 @agent_ppo2.py:134][0m Policy update time: 0.89 s
[32m[20230114 18:46:57 @agent_ppo2.py:142][0m Average TRAINING episode reward: 255.24
[32m[20230114 18:46:57 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 257.77
[32m[20230114 18:46:57 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 271.92
[32m[20230114 18:46:57 @agent_ppo2.py:147][0m Total time:       4.89 min
[32m[20230114 18:46:57 @agent_ppo2.py:149][0m 372736 total steps have happened
[32m[20230114 18:46:57 @agent_ppo2.py:125][0m #------------------------ Iteration 182 --------------------------#
[32m[20230114 18:46:58 @agent_ppo2.py:131][0m Sampling time: 0.26 s by 4 slaves
[32m[20230114 18:46:58 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:46:58 @agent_ppo2.py:189][0m |           0.0011 |          10.7966 |           4.0527 |
[32m[20230114 18:46:58 @agent_ppo2.py:189][0m |          -0.0032 |          10.0196 |           4.0486 |
[32m[20230114 18:46:58 @agent_ppo2.py:189][0m |          -0.0053 |           9.7286 |           4.0463 |
[32m[20230114 18:46:58 @agent_ppo2.py:189][0m |          -0.0034 |           9.5501 |           4.0432 |
[32m[20230114 18:46:58 @agent_ppo2.py:189][0m |          -0.0053 |           9.3089 |           4.0402 |
[32m[20230114 18:46:58 @agent_ppo2.py:189][0m |          -0.0056 |           9.1458 |           4.0439 |
[32m[20230114 18:46:58 @agent_ppo2.py:189][0m |          -0.0087 |           8.9998 |           4.0403 |
[32m[20230114 18:46:58 @agent_ppo2.py:189][0m |          -0.0107 |           8.8753 |           4.0412 |
[32m[20230114 18:46:59 @agent_ppo2.py:189][0m |          -0.0085 |           8.7607 |           4.0404 |
[32m[20230114 18:46:59 @agent_ppo2.py:189][0m |          -0.0068 |           8.6457 |           4.0390 |
[32m[20230114 18:46:59 @agent_ppo2.py:134][0m Policy update time: 1.03 s
[32m[20230114 18:46:59 @agent_ppo2.py:142][0m Average TRAINING episode reward: 251.10
[32m[20230114 18:46:59 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 255.81
[32m[20230114 18:46:59 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 273.37
[32m[20230114 18:46:59 @agent_ppo2.py:147][0m Total time:       4.91 min
[32m[20230114 18:46:59 @agent_ppo2.py:149][0m 374784 total steps have happened
[32m[20230114 18:46:59 @agent_ppo2.py:125][0m #------------------------ Iteration 183 --------------------------#
[32m[20230114 18:46:59 @agent_ppo2.py:131][0m Sampling time: 0.29 s by 4 slaves
[32m[20230114 18:46:59 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:46:59 @agent_ppo2.py:189][0m |          -0.0003 |          10.8206 |           4.1496 |
[32m[20230114 18:46:59 @agent_ppo2.py:189][0m |          -0.0061 |          10.4453 |           4.1329 |
[32m[20230114 18:47:00 @agent_ppo2.py:189][0m |          -0.0069 |          10.2883 |           4.1382 |
[32m[20230114 18:47:00 @agent_ppo2.py:189][0m |          -0.0088 |          10.0861 |           4.1389 |
[32m[20230114 18:47:00 @agent_ppo2.py:189][0m |          -0.0097 |          10.0001 |           4.1383 |
[32m[20230114 18:47:00 @agent_ppo2.py:189][0m |          -0.0099 |           9.8725 |           4.1401 |
[32m[20230114 18:47:00 @agent_ppo2.py:189][0m |          -0.0104 |           9.7823 |           4.1428 |
[32m[20230114 18:47:00 @agent_ppo2.py:189][0m |          -0.0088 |           9.8152 |           4.1433 |
[32m[20230114 18:47:00 @agent_ppo2.py:189][0m |          -0.0102 |           9.6270 |           4.1409 |
[32m[20230114 18:47:00 @agent_ppo2.py:189][0m |          -0.0089 |           9.6113 |           4.1431 |
[32m[20230114 18:47:00 @agent_ppo2.py:134][0m Policy update time: 0.99 s
[32m[20230114 18:47:00 @agent_ppo2.py:142][0m Average TRAINING episode reward: 256.00
[32m[20230114 18:47:00 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 256.97
[32m[20230114 18:47:00 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 275.10
[32m[20230114 18:47:00 @agent_ppo2.py:147][0m Total time:       4.94 min
[32m[20230114 18:47:00 @agent_ppo2.py:149][0m 376832 total steps have happened
[32m[20230114 18:47:00 @agent_ppo2.py:125][0m #------------------------ Iteration 184 --------------------------#
[32m[20230114 18:47:01 @agent_ppo2.py:131][0m Sampling time: 0.25 s by 4 slaves
[32m[20230114 18:47:01 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:47:01 @agent_ppo2.py:189][0m |           0.0012 |          11.4151 |           4.1329 |
[32m[20230114 18:47:01 @agent_ppo2.py:189][0m |          -0.0022 |          10.8259 |           4.1362 |
[32m[20230114 18:47:01 @agent_ppo2.py:189][0m |          -0.0056 |          10.4869 |           4.1370 |
[32m[20230114 18:47:01 @agent_ppo2.py:189][0m |          -0.0057 |          10.3096 |           4.1389 |
[32m[20230114 18:47:01 @agent_ppo2.py:189][0m |          -0.0063 |          10.1484 |           4.1431 |
[32m[20230114 18:47:01 @agent_ppo2.py:189][0m |          -0.0080 |          10.0231 |           4.1463 |
[32m[20230114 18:47:01 @agent_ppo2.py:189][0m |          -0.0083 |           9.9302 |           4.1488 |
[32m[20230114 18:47:01 @agent_ppo2.py:189][0m |          -0.0083 |           9.8474 |           4.1511 |
[32m[20230114 18:47:01 @agent_ppo2.py:189][0m |          -0.0075 |           9.7768 |           4.1508 |
[32m[20230114 18:47:02 @agent_ppo2.py:189][0m |          -0.0089 |           9.5936 |           4.1531 |
[32m[20230114 18:47:02 @agent_ppo2.py:134][0m Policy update time: 0.94 s
[32m[20230114 18:47:02 @agent_ppo2.py:142][0m Average TRAINING episode reward: 254.96
[32m[20230114 18:47:02 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 258.50
[32m[20230114 18:47:02 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 273.46
[32m[20230114 18:47:02 @agent_ppo2.py:147][0m Total time:       4.96 min
[32m[20230114 18:47:02 @agent_ppo2.py:149][0m 378880 total steps have happened
[32m[20230114 18:47:02 @agent_ppo2.py:125][0m #------------------------ Iteration 185 --------------------------#
[32m[20230114 18:47:02 @agent_ppo2.py:131][0m Sampling time: 0.29 s by 4 slaves
[32m[20230114 18:47:02 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:47:02 @agent_ppo2.py:189][0m |          -0.0009 |           9.5471 |           4.2881 |
[32m[20230114 18:47:02 @agent_ppo2.py:189][0m |          -0.0050 |           8.5628 |           4.2847 |
[32m[20230114 18:47:02 @agent_ppo2.py:189][0m |          -0.0061 |           8.2212 |           4.2790 |
[32m[20230114 18:47:03 @agent_ppo2.py:189][0m |          -0.0075 |           8.0024 |           4.2808 |
[32m[20230114 18:47:03 @agent_ppo2.py:189][0m |          -0.0081 |           7.8428 |           4.2759 |
[32m[20230114 18:47:03 @agent_ppo2.py:189][0m |          -0.0087 |           7.6951 |           4.2757 |
[32m[20230114 18:47:03 @agent_ppo2.py:189][0m |          -0.0088 |           7.5793 |           4.2716 |
[32m[20230114 18:47:03 @agent_ppo2.py:189][0m |          -0.0097 |           7.4700 |           4.2686 |
[32m[20230114 18:47:03 @agent_ppo2.py:189][0m |          -0.0101 |           7.3814 |           4.2695 |
[32m[20230114 18:47:03 @agent_ppo2.py:189][0m |          -0.0100 |           7.3013 |           4.2659 |
[32m[20230114 18:47:03 @agent_ppo2.py:134][0m Policy update time: 1.02 s
[32m[20230114 18:47:03 @agent_ppo2.py:142][0m Average TRAINING episode reward: 253.46
[32m[20230114 18:47:03 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 257.49
[32m[20230114 18:47:03 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 266.86
[32m[20230114 18:47:03 @agent_ppo2.py:147][0m Total time:       4.99 min
[32m[20230114 18:47:03 @agent_ppo2.py:149][0m 380928 total steps have happened
[32m[20230114 18:47:03 @agent_ppo2.py:125][0m #------------------------ Iteration 186 --------------------------#
[32m[20230114 18:47:04 @agent_ppo2.py:131][0m Sampling time: 0.25 s by 4 slaves
[32m[20230114 18:47:04 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:47:04 @agent_ppo2.py:189][0m |           0.0007 |          10.9348 |           4.1880 |
[32m[20230114 18:47:04 @agent_ppo2.py:189][0m |          -0.0038 |           9.7668 |           4.1847 |
[32m[20230114 18:47:04 @agent_ppo2.py:189][0m |          -0.0066 |           9.3056 |           4.1797 |
[32m[20230114 18:47:04 @agent_ppo2.py:189][0m |          -0.0007 |           9.2888 |           4.1795 |
[32m[20230114 18:47:04 @agent_ppo2.py:189][0m |          -0.0087 |           8.8494 |           4.1778 |
[32m[20230114 18:47:04 @agent_ppo2.py:189][0m |          -0.0128 |           8.6600 |           4.1768 |
[32m[20230114 18:47:04 @agent_ppo2.py:189][0m |          -0.0084 |           8.6748 |           4.1738 |
[32m[20230114 18:47:04 @agent_ppo2.py:189][0m |          -0.0144 |           8.3745 |           4.1740 |
[32m[20230114 18:47:04 @agent_ppo2.py:189][0m |          -0.0121 |           8.2620 |           4.1709 |
[32m[20230114 18:47:05 @agent_ppo2.py:189][0m |          -0.0115 |           8.1511 |           4.1700 |
[32m[20230114 18:47:05 @agent_ppo2.py:134][0m Policy update time: 0.92 s
[32m[20230114 18:47:05 @agent_ppo2.py:142][0m Average TRAINING episode reward: 257.23
[32m[20230114 18:47:05 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 263.51
[32m[20230114 18:47:05 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 273.06
[32m[20230114 18:47:05 @agent_ppo2.py:147][0m Total time:       5.01 min
[32m[20230114 18:47:05 @agent_ppo2.py:149][0m 382976 total steps have happened
[32m[20230114 18:47:05 @agent_ppo2.py:125][0m #------------------------ Iteration 187 --------------------------#
[32m[20230114 18:47:05 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:47:05 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:47:05 @agent_ppo2.py:189][0m |           0.0000 |          11.0411 |           4.1856 |
[32m[20230114 18:47:05 @agent_ppo2.py:189][0m |          -0.0038 |          10.2214 |           4.1806 |
[32m[20230114 18:47:05 @agent_ppo2.py:189][0m |          -0.0058 |           9.8089 |           4.1822 |
[32m[20230114 18:47:05 @agent_ppo2.py:189][0m |          -0.0071 |           9.6156 |           4.1720 |
[32m[20230114 18:47:05 @agent_ppo2.py:189][0m |          -0.0069 |           9.4126 |           4.1799 |
[32m[20230114 18:47:06 @agent_ppo2.py:189][0m |          -0.0085 |           9.3028 |           4.1757 |
[32m[20230114 18:47:06 @agent_ppo2.py:189][0m |          -0.0089 |           9.1638 |           4.1733 |
[32m[20230114 18:47:06 @agent_ppo2.py:189][0m |          -0.0093 |           9.0432 |           4.1735 |
[32m[20230114 18:47:06 @agent_ppo2.py:189][0m |          -0.0096 |           8.9635 |           4.1712 |
[32m[20230114 18:47:06 @agent_ppo2.py:189][0m |          -0.0107 |           8.7859 |           4.1698 |
[32m[20230114 18:47:06 @agent_ppo2.py:134][0m Policy update time: 0.84 s
[32m[20230114 18:47:06 @agent_ppo2.py:142][0m Average TRAINING episode reward: 256.91
[32m[20230114 18:47:06 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 260.64
[32m[20230114 18:47:06 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 274.35
[32m[20230114 18:47:06 @agent_ppo2.py:147][0m Total time:       5.03 min
[32m[20230114 18:47:06 @agent_ppo2.py:149][0m 385024 total steps have happened
[32m[20230114 18:47:06 @agent_ppo2.py:125][0m #------------------------ Iteration 188 --------------------------#
[32m[20230114 18:47:06 @agent_ppo2.py:131][0m Sampling time: 0.26 s by 4 slaves
[32m[20230114 18:47:06 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:47:06 @agent_ppo2.py:189][0m |          -0.0005 |          23.5921 |           4.1711 |
[32m[20230114 18:47:07 @agent_ppo2.py:189][0m |          -0.0052 |          11.5682 |           4.1635 |
[32m[20230114 18:47:07 @agent_ppo2.py:189][0m |          -0.0027 |          10.7244 |           4.1629 |
[32m[20230114 18:47:07 @agent_ppo2.py:189][0m |          -0.0060 |          10.3023 |           4.1635 |
[32m[20230114 18:47:07 @agent_ppo2.py:189][0m |          -0.0066 |          10.0070 |           4.1664 |
[32m[20230114 18:47:07 @agent_ppo2.py:189][0m |          -0.0069 |           9.7780 |           4.1622 |
[32m[20230114 18:47:07 @agent_ppo2.py:189][0m |          -0.0071 |           9.6228 |           4.1625 |
[32m[20230114 18:47:07 @agent_ppo2.py:189][0m |          -0.0064 |           9.4424 |           4.1650 |
[32m[20230114 18:47:07 @agent_ppo2.py:189][0m |          -0.0081 |           9.2972 |           4.1609 |
[32m[20230114 18:47:07 @agent_ppo2.py:189][0m |          -0.0081 |           9.2270 |           4.1604 |
[32m[20230114 18:47:07 @agent_ppo2.py:134][0m Policy update time: 0.85 s
[32m[20230114 18:47:07 @agent_ppo2.py:142][0m Average TRAINING episode reward: 182.09
[32m[20230114 18:47:07 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 261.24
[32m[20230114 18:47:07 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 275.86
[32m[20230114 18:47:07 @agent_ppo2.py:147][0m Total time:       5.05 min
[32m[20230114 18:47:07 @agent_ppo2.py:149][0m 387072 total steps have happened
[32m[20230114 18:47:07 @agent_ppo2.py:125][0m #------------------------ Iteration 189 --------------------------#
[32m[20230114 18:47:08 @agent_ppo2.py:131][0m Sampling time: 0.25 s by 4 slaves
[32m[20230114 18:47:08 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:47:08 @agent_ppo2.py:189][0m |          -0.0013 |          12.1991 |           4.1551 |
[32m[20230114 18:47:08 @agent_ppo2.py:189][0m |          -0.0062 |          11.4717 |           4.1535 |
[32m[20230114 18:47:08 @agent_ppo2.py:189][0m |          -0.0092 |          11.2060 |           4.1515 |
[32m[20230114 18:47:08 @agent_ppo2.py:189][0m |          -0.0069 |          11.0819 |           4.1504 |
[32m[20230114 18:47:08 @agent_ppo2.py:189][0m |          -0.0096 |          10.9889 |           4.1495 |
[32m[20230114 18:47:08 @agent_ppo2.py:189][0m |          -0.0119 |          10.8237 |           4.1496 |
[32m[20230114 18:47:08 @agent_ppo2.py:189][0m |          -0.0106 |          10.8478 |           4.1499 |
[32m[20230114 18:47:08 @agent_ppo2.py:189][0m |          -0.0111 |          10.6614 |           4.1471 |
[32m[20230114 18:47:09 @agent_ppo2.py:189][0m |          -0.0116 |          10.7425 |           4.1480 |
[32m[20230114 18:47:09 @agent_ppo2.py:189][0m |          -0.0144 |          10.5341 |           4.1466 |
[32m[20230114 18:47:09 @agent_ppo2.py:134][0m Policy update time: 0.97 s
[32m[20230114 18:47:09 @agent_ppo2.py:142][0m Average TRAINING episode reward: 256.30
[32m[20230114 18:47:09 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 259.40
[32m[20230114 18:47:09 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 276.19
[32m[20230114 18:47:09 @agent_ppo2.py:147][0m Total time:       5.08 min
[32m[20230114 18:47:09 @agent_ppo2.py:149][0m 389120 total steps have happened
[32m[20230114 18:47:09 @agent_ppo2.py:125][0m #------------------------ Iteration 190 --------------------------#
[32m[20230114 18:47:09 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:47:09 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:47:09 @agent_ppo2.py:189][0m |          -0.0032 |           8.9996 |           4.0936 |
[32m[20230114 18:47:09 @agent_ppo2.py:189][0m |          -0.0055 |           7.1850 |           4.0886 |
[32m[20230114 18:47:09 @agent_ppo2.py:189][0m |          -0.0084 |           6.0781 |           4.0865 |
[32m[20230114 18:47:09 @agent_ppo2.py:189][0m |          -0.0104 |           5.4398 |           4.0872 |
[32m[20230114 18:47:10 @agent_ppo2.py:189][0m |          -0.0104 |           4.9811 |           4.0868 |
[32m[20230114 18:47:10 @agent_ppo2.py:189][0m |          -0.0032 |           4.6713 |           4.0881 |
[32m[20230114 18:47:10 @agent_ppo2.py:189][0m |          -0.0090 |           4.4253 |           4.0845 |
[32m[20230114 18:47:10 @agent_ppo2.py:189][0m |          -0.0076 |           4.2013 |           4.0808 |
[32m[20230114 18:47:10 @agent_ppo2.py:189][0m |          -0.0120 |           3.9823 |           4.0837 |
[32m[20230114 18:47:10 @agent_ppo2.py:189][0m |          -0.0113 |           3.8202 |           4.0855 |
[32m[20230114 18:47:10 @agent_ppo2.py:134][0m Policy update time: 0.98 s
[32m[20230114 18:47:10 @agent_ppo2.py:142][0m Average TRAINING episode reward: 254.96
[32m[20230114 18:47:10 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 256.86
[32m[20230114 18:47:10 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 278.12
[32m[20230114 18:47:10 @agent_ppo2.py:147][0m Total time:       5.10 min
[32m[20230114 18:47:10 @agent_ppo2.py:149][0m 391168 total steps have happened
[32m[20230114 18:47:10 @agent_ppo2.py:125][0m #------------------------ Iteration 191 --------------------------#
[32m[20230114 18:47:11 @agent_ppo2.py:131][0m Sampling time: 0.27 s by 4 slaves
[32m[20230114 18:47:11 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:47:11 @agent_ppo2.py:189][0m |          -0.0017 |          21.0199 |           4.1747 |
[32m[20230114 18:47:11 @agent_ppo2.py:189][0m |          -0.0066 |          14.5325 |           4.1720 |
[32m[20230114 18:47:11 @agent_ppo2.py:189][0m |          -0.0086 |          13.3180 |           4.1622 |
[32m[20230114 18:47:11 @agent_ppo2.py:189][0m |          -0.0089 |          12.6606 |           4.1638 |
[32m[20230114 18:47:11 @agent_ppo2.py:189][0m |          -0.0098 |          12.2072 |           4.1627 |
[32m[20230114 18:47:11 @agent_ppo2.py:189][0m |          -0.0106 |          11.8841 |           4.1608 |
[32m[20230114 18:47:11 @agent_ppo2.py:189][0m |          -0.0121 |          11.5672 |           4.1636 |
[32m[20230114 18:47:11 @agent_ppo2.py:189][0m |          -0.0122 |          11.2772 |           4.1577 |
[32m[20230114 18:47:11 @agent_ppo2.py:189][0m |          -0.0120 |          11.1013 |           4.1599 |
[32m[20230114 18:47:11 @agent_ppo2.py:189][0m |          -0.0120 |          10.9331 |           4.1595 |
[32m[20230114 18:47:11 @agent_ppo2.py:134][0m Policy update time: 0.86 s
[32m[20230114 18:47:12 @agent_ppo2.py:142][0m Average TRAINING episode reward: 206.40
[32m[20230114 18:47:12 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 255.21
[32m[20230114 18:47:12 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 271.95
[32m[20230114 18:47:12 @agent_ppo2.py:147][0m Total time:       5.12 min
[32m[20230114 18:47:12 @agent_ppo2.py:149][0m 393216 total steps have happened
[32m[20230114 18:47:12 @agent_ppo2.py:125][0m #------------------------ Iteration 192 --------------------------#
[32m[20230114 18:47:12 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:47:12 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:47:12 @agent_ppo2.py:189][0m |          -0.0066 |          11.6044 |           4.0669 |
[32m[20230114 18:47:12 @agent_ppo2.py:189][0m |           0.0205 |          12.3198 |           4.0550 |
[32m[20230114 18:47:12 @agent_ppo2.py:189][0m |          -0.0062 |          10.0007 |           4.0504 |
[32m[20230114 18:47:12 @agent_ppo2.py:189][0m |          -0.0070 |           9.6347 |           4.0494 |
[32m[20230114 18:47:12 @agent_ppo2.py:189][0m |          -0.0028 |           9.6190 |           4.0470 |
[32m[20230114 18:47:12 @agent_ppo2.py:189][0m |          -0.0026 |           9.3646 |           4.0404 |
[32m[20230114 18:47:12 @agent_ppo2.py:189][0m |          -0.0122 |           9.2830 |           4.0481 |
[32m[20230114 18:47:13 @agent_ppo2.py:189][0m |          -0.0109 |           9.1485 |           4.0454 |
[32m[20230114 18:47:13 @agent_ppo2.py:189][0m |          -0.0133 |           9.0491 |           4.0460 |
[32m[20230114 18:47:13 @agent_ppo2.py:189][0m |          -0.0058 |           9.0188 |           4.0439 |
[32m[20230114 18:47:13 @agent_ppo2.py:134][0m Policy update time: 0.86 s
[32m[20230114 18:47:13 @agent_ppo2.py:142][0m Average TRAINING episode reward: 255.22
[32m[20230114 18:47:13 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 258.03
[32m[20230114 18:47:13 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 276.41
[32m[20230114 18:47:13 @agent_ppo2.py:147][0m Total time:       5.15 min
[32m[20230114 18:47:13 @agent_ppo2.py:149][0m 395264 total steps have happened
[32m[20230114 18:47:13 @agent_ppo2.py:125][0m #------------------------ Iteration 193 --------------------------#
[32m[20230114 18:47:13 @agent_ppo2.py:131][0m Sampling time: 0.30 s by 4 slaves
[32m[20230114 18:47:13 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:47:13 @agent_ppo2.py:189][0m |          -0.0030 |          16.2492 |           4.0987 |
[32m[20230114 18:47:13 @agent_ppo2.py:189][0m |          -0.0088 |          12.2904 |           4.0904 |
[32m[20230114 18:47:14 @agent_ppo2.py:189][0m |          -0.0109 |          11.5831 |           4.0862 |
[32m[20230114 18:47:14 @agent_ppo2.py:189][0m |          -0.0007 |          11.3663 |           4.0889 |
[32m[20230114 18:47:14 @agent_ppo2.py:189][0m |          -0.0080 |          11.2290 |           4.0833 |
[32m[20230114 18:47:14 @agent_ppo2.py:189][0m |          -0.0113 |          10.8531 |           4.0801 |
[32m[20230114 18:47:14 @agent_ppo2.py:189][0m |          -0.0093 |          10.7051 |           4.0817 |
[32m[20230114 18:47:14 @agent_ppo2.py:189][0m |          -0.0130 |          10.5392 |           4.0806 |
[32m[20230114 18:47:14 @agent_ppo2.py:189][0m |          -0.0063 |          10.5071 |           4.0785 |
[32m[20230114 18:47:14 @agent_ppo2.py:189][0m |          -0.0141 |          10.3766 |           4.0777 |
[32m[20230114 18:47:14 @agent_ppo2.py:134][0m Policy update time: 0.84 s
[32m[20230114 18:47:14 @agent_ppo2.py:142][0m Average TRAINING episode reward: 206.79
[32m[20230114 18:47:14 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 259.08
[32m[20230114 18:47:14 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 272.75
[32m[20230114 18:47:14 @agent_ppo2.py:147][0m Total time:       5.17 min
[32m[20230114 18:47:14 @agent_ppo2.py:149][0m 397312 total steps have happened
[32m[20230114 18:47:14 @agent_ppo2.py:125][0m #------------------------ Iteration 194 --------------------------#
[32m[20230114 18:47:15 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:47:15 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:47:15 @agent_ppo2.py:189][0m |          -0.0016 |          19.0127 |           4.0942 |
[32m[20230114 18:47:15 @agent_ppo2.py:189][0m |          -0.0070 |          15.8277 |           4.0877 |
[32m[20230114 18:47:15 @agent_ppo2.py:189][0m |          -0.0077 |          14.4792 |           4.0846 |
[32m[20230114 18:47:15 @agent_ppo2.py:189][0m |          -0.0021 |          14.1960 |           4.0799 |
[32m[20230114 18:47:15 @agent_ppo2.py:189][0m |          -0.0112 |          13.5703 |           4.0776 |
[32m[20230114 18:47:15 @agent_ppo2.py:189][0m |           0.0092 |          13.2917 |           4.0768 |
[32m[20230114 18:47:15 @agent_ppo2.py:189][0m |          -0.0133 |          13.1568 |           4.0645 |
[32m[20230114 18:47:15 @agent_ppo2.py:189][0m |          -0.0113 |          12.8874 |           4.0648 |
[32m[20230114 18:47:15 @agent_ppo2.py:189][0m |          -0.0139 |          12.7594 |           4.0647 |
[32m[20230114 18:47:15 @agent_ppo2.py:189][0m |          -0.0043 |          12.6041 |           4.0623 |
[32m[20230114 18:47:15 @agent_ppo2.py:134][0m Policy update time: 0.86 s
[32m[20230114 18:47:16 @agent_ppo2.py:142][0m Average TRAINING episode reward: 215.94
[32m[20230114 18:47:16 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 262.58
[32m[20230114 18:47:16 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 277.76
[32m[20230114 18:47:16 @agent_ppo2.py:147][0m Total time:       5.19 min
[32m[20230114 18:47:16 @agent_ppo2.py:149][0m 399360 total steps have happened
[32m[20230114 18:47:16 @agent_ppo2.py:125][0m #------------------------ Iteration 195 --------------------------#
[32m[20230114 18:47:16 @agent_ppo2.py:131][0m Sampling time: 0.28 s by 4 slaves
[32m[20230114 18:47:16 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:47:16 @agent_ppo2.py:189][0m |           0.0002 |          26.9433 |           4.0560 |
[32m[20230114 18:47:16 @agent_ppo2.py:189][0m |          -0.0050 |          24.8866 |           4.0514 |
[32m[20230114 18:47:16 @agent_ppo2.py:189][0m |          -0.0065 |          24.1494 |           4.0473 |
[32m[20230114 18:47:16 @agent_ppo2.py:189][0m |          -0.0071 |          23.3570 |           4.0473 |
[32m[20230114 18:47:16 @agent_ppo2.py:189][0m |          -0.0086 |          22.9370 |           4.0455 |
[32m[20230114 18:47:17 @agent_ppo2.py:189][0m |          -0.0087 |          21.9783 |           4.0401 |
[32m[20230114 18:47:17 @agent_ppo2.py:189][0m |          -0.0101 |          21.3186 |           4.0406 |
[32m[20230114 18:47:17 @agent_ppo2.py:189][0m |          -0.0096 |          20.7749 |           4.0405 |
[32m[20230114 18:47:17 @agent_ppo2.py:189][0m |          -0.0101 |          20.0233 |           4.0405 |
[32m[20230114 18:47:17 @agent_ppo2.py:189][0m |          -0.0111 |          19.0558 |           4.0405 |
[32m[20230114 18:47:17 @agent_ppo2.py:134][0m Policy update time: 0.92 s
[32m[20230114 18:47:17 @agent_ppo2.py:142][0m Average TRAINING episode reward: 193.97
[32m[20230114 18:47:17 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 258.01
[32m[20230114 18:47:17 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 272.16
[32m[20230114 18:47:17 @agent_ppo2.py:147][0m Total time:       5.22 min
[32m[20230114 18:47:17 @agent_ppo2.py:149][0m 401408 total steps have happened
[32m[20230114 18:47:17 @agent_ppo2.py:125][0m #------------------------ Iteration 196 --------------------------#
[32m[20230114 18:47:17 @agent_ppo2.py:131][0m Sampling time: 0.28 s by 4 slaves
[32m[20230114 18:47:17 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:47:18 @agent_ppo2.py:189][0m |           0.0022 |          14.8500 |           4.0621 |
[32m[20230114 18:47:18 @agent_ppo2.py:189][0m |          -0.0043 |          13.0377 |           4.0584 |
[32m[20230114 18:47:18 @agent_ppo2.py:189][0m |          -0.0066 |          12.7540 |           4.0581 |
[32m[20230114 18:47:18 @agent_ppo2.py:189][0m |          -0.0065 |          12.4984 |           4.0562 |
[32m[20230114 18:47:18 @agent_ppo2.py:189][0m |          -0.0072 |          12.5104 |           4.0546 |
[32m[20230114 18:47:18 @agent_ppo2.py:189][0m |          -0.0089 |          12.2343 |           4.0553 |
[32m[20230114 18:47:18 @agent_ppo2.py:189][0m |          -0.0078 |          12.1737 |           4.0502 |
[32m[20230114 18:47:18 @agent_ppo2.py:189][0m |          -0.0092 |          12.0112 |           4.0492 |
[32m[20230114 18:47:18 @agent_ppo2.py:189][0m |          -0.0103 |          11.9949 |           4.0519 |
[32m[20230114 18:47:18 @agent_ppo2.py:189][0m |          -0.0092 |          11.8715 |           4.0494 |
[32m[20230114 18:47:18 @agent_ppo2.py:134][0m Policy update time: 0.87 s
[32m[20230114 18:47:18 @agent_ppo2.py:142][0m Average TRAINING episode reward: 255.88
[32m[20230114 18:47:18 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 258.98
[32m[20230114 18:47:18 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 276.00
[32m[20230114 18:47:18 @agent_ppo2.py:147][0m Total time:       5.24 min
[32m[20230114 18:47:18 @agent_ppo2.py:149][0m 403456 total steps have happened
[32m[20230114 18:47:18 @agent_ppo2.py:125][0m #------------------------ Iteration 197 --------------------------#
[32m[20230114 18:47:19 @agent_ppo2.py:131][0m Sampling time: 0.25 s by 4 slaves
[32m[20230114 18:47:19 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:47:19 @agent_ppo2.py:189][0m |           0.0003 |           9.6579 |           4.0350 |
[32m[20230114 18:47:19 @agent_ppo2.py:189][0m |          -0.0045 |           9.0569 |           4.0311 |
[32m[20230114 18:47:19 @agent_ppo2.py:189][0m |          -0.0082 |           8.6685 |           4.0260 |
[32m[20230114 18:47:19 @agent_ppo2.py:189][0m |          -0.0083 |           8.4195 |           4.0269 |
[32m[20230114 18:47:19 @agent_ppo2.py:189][0m |          -0.0092 |           8.1494 |           4.0273 |
[32m[20230114 18:47:19 @agent_ppo2.py:189][0m |          -0.0095 |           7.9994 |           4.0255 |
[32m[20230114 18:47:19 @agent_ppo2.py:189][0m |          -0.0076 |           7.9034 |           4.0260 |
[32m[20230114 18:47:19 @agent_ppo2.py:189][0m |          -0.0113 |           7.7166 |           4.0227 |
[32m[20230114 18:47:19 @agent_ppo2.py:189][0m |          -0.0111 |           7.6250 |           4.0257 |
[32m[20230114 18:47:20 @agent_ppo2.py:189][0m |          -0.0116 |           7.5635 |           4.0263 |
[32m[20230114 18:47:20 @agent_ppo2.py:134][0m Policy update time: 0.85 s
[32m[20230114 18:47:20 @agent_ppo2.py:142][0m Average TRAINING episode reward: 258.45
[32m[20230114 18:47:20 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 262.54
[32m[20230114 18:47:20 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 276.80
[32m[20230114 18:47:20 @agent_ppo2.py:147][0m Total time:       5.26 min
[32m[20230114 18:47:20 @agent_ppo2.py:149][0m 405504 total steps have happened
[32m[20230114 18:47:20 @agent_ppo2.py:125][0m #------------------------ Iteration 198 --------------------------#
[32m[20230114 18:47:20 @agent_ppo2.py:131][0m Sampling time: 0.25 s by 4 slaves
[32m[20230114 18:47:20 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:47:20 @agent_ppo2.py:189][0m |          -0.0109 |          12.3687 |           3.9661 |
[32m[20230114 18:47:20 @agent_ppo2.py:189][0m |          -0.0025 |          11.3866 |           3.9575 |
[32m[20230114 18:47:20 @agent_ppo2.py:189][0m |          -0.0100 |          11.0534 |           3.9629 |
[32m[20230114 18:47:20 @agent_ppo2.py:189][0m |          -0.0079 |          10.8597 |           3.9540 |
[32m[20230114 18:47:20 @agent_ppo2.py:189][0m |          -0.0092 |          10.7153 |           3.9548 |
[32m[20230114 18:47:21 @agent_ppo2.py:189][0m |          -0.0102 |          10.5863 |           3.9532 |
[32m[20230114 18:47:21 @agent_ppo2.py:189][0m |          -0.0115 |          10.5223 |           3.9520 |
[32m[20230114 18:47:21 @agent_ppo2.py:189][0m |          -0.0104 |          10.4040 |           3.9444 |
[32m[20230114 18:47:21 @agent_ppo2.py:189][0m |          -0.0096 |          10.3604 |           3.9500 |
[32m[20230114 18:47:21 @agent_ppo2.py:189][0m |          -0.0104 |          10.2693 |           3.9487 |
[32m[20230114 18:47:21 @agent_ppo2.py:134][0m Policy update time: 0.91 s
[32m[20230114 18:47:21 @agent_ppo2.py:142][0m Average TRAINING episode reward: 254.33
[32m[20230114 18:47:21 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 256.86
[32m[20230114 18:47:21 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 122.89
[32m[20230114 18:47:21 @agent_ppo2.py:147][0m Total time:       5.28 min
[32m[20230114 18:47:21 @agent_ppo2.py:149][0m 407552 total steps have happened
[32m[20230114 18:47:21 @agent_ppo2.py:125][0m #------------------------ Iteration 199 --------------------------#
[32m[20230114 18:47:21 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:47:22 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:47:22 @agent_ppo2.py:189][0m |          -0.0006 |          11.8365 |           4.0674 |
[32m[20230114 18:47:22 @agent_ppo2.py:189][0m |          -0.0074 |          11.4219 |           4.0590 |
[32m[20230114 18:47:22 @agent_ppo2.py:189][0m |          -0.0075 |          11.2573 |           4.0547 |
[32m[20230114 18:47:22 @agent_ppo2.py:189][0m |          -0.0089 |          11.1009 |           4.0544 |
[32m[20230114 18:47:22 @agent_ppo2.py:189][0m |          -0.0039 |          11.3482 |           4.0492 |
[32m[20230114 18:47:22 @agent_ppo2.py:189][0m |          -0.0097 |          10.9251 |           4.0451 |
[32m[20230114 18:47:22 @agent_ppo2.py:189][0m |          -0.0105 |          10.8112 |           4.0451 |
[32m[20230114 18:47:22 @agent_ppo2.py:189][0m |          -0.0081 |          10.7529 |           4.0430 |
[32m[20230114 18:47:22 @agent_ppo2.py:189][0m |          -0.0105 |          10.6631 |           4.0401 |
[32m[20230114 18:47:22 @agent_ppo2.py:189][0m |          -0.0118 |          10.5969 |           4.0383 |
[32m[20230114 18:47:22 @agent_ppo2.py:134][0m Policy update time: 0.84 s
[32m[20230114 18:47:23 @agent_ppo2.py:142][0m Average TRAINING episode reward: 256.83
[32m[20230114 18:47:23 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 259.87
[32m[20230114 18:47:23 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 275.55
[32m[20230114 18:47:23 @agent_ppo2.py:147][0m Total time:       5.31 min
[32m[20230114 18:47:23 @agent_ppo2.py:149][0m 409600 total steps have happened
[32m[20230114 18:47:23 @agent_ppo2.py:125][0m #------------------------ Iteration 200 --------------------------#
[32m[20230114 18:47:23 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:47:23 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:47:23 @agent_ppo2.py:189][0m |          -0.0016 |          10.3690 |           4.0161 |
[32m[20230114 18:47:23 @agent_ppo2.py:189][0m |          -0.0067 |           9.7528 |           4.0126 |
[32m[20230114 18:47:23 @agent_ppo2.py:189][0m |          -0.0091 |           9.4901 |           4.0122 |
[32m[20230114 18:47:23 @agent_ppo2.py:189][0m |          -0.0102 |           9.2696 |           4.0146 |
[32m[20230114 18:47:23 @agent_ppo2.py:189][0m |          -0.0118 |           9.0962 |           4.0139 |
[32m[20230114 18:47:23 @agent_ppo2.py:189][0m |          -0.0101 |           8.9543 |           4.0148 |
[32m[20230114 18:47:23 @agent_ppo2.py:189][0m |          -0.0114 |           8.8318 |           4.0139 |
[32m[20230114 18:47:23 @agent_ppo2.py:189][0m |          -0.0127 |           8.6762 |           4.0157 |
[32m[20230114 18:47:24 @agent_ppo2.py:189][0m |          -0.0064 |           8.6167 |           4.0127 |
[32m[20230114 18:47:24 @agent_ppo2.py:189][0m |          -0.0153 |           8.4017 |           4.0145 |
[32m[20230114 18:47:24 @agent_ppo2.py:134][0m Policy update time: 0.85 s
[32m[20230114 18:47:24 @agent_ppo2.py:142][0m Average TRAINING episode reward: 254.39
[32m[20230114 18:47:24 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 258.01
[32m[20230114 18:47:24 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 276.43
[32m[20230114 18:47:24 @agent_ppo2.py:147][0m Total time:       5.33 min
[32m[20230114 18:47:24 @agent_ppo2.py:149][0m 411648 total steps have happened
[32m[20230114 18:47:24 @agent_ppo2.py:125][0m #------------------------ Iteration 201 --------------------------#
[32m[20230114 18:47:24 @agent_ppo2.py:131][0m Sampling time: 0.26 s by 4 slaves
[32m[20230114 18:47:24 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:47:24 @agent_ppo2.py:189][0m |           0.0024 |           8.1993 |           4.0123 |
[32m[20230114 18:47:24 @agent_ppo2.py:189][0m |          -0.0020 |           5.2838 |           4.0057 |
[32m[20230114 18:47:24 @agent_ppo2.py:189][0m |          -0.0057 |           4.1724 |           3.9984 |
[32m[20230114 18:47:24 @agent_ppo2.py:189][0m |          -0.0049 |           3.6393 |           4.0022 |
[32m[20230114 18:47:25 @agent_ppo2.py:189][0m |          -0.0046 |           3.3610 |           3.9947 |
[32m[20230114 18:47:25 @agent_ppo2.py:189][0m |          -0.0078 |           3.1430 |           3.9980 |
[32m[20230114 18:47:25 @agent_ppo2.py:189][0m |          -0.0090 |           2.9688 |           3.9929 |
[32m[20230114 18:47:25 @agent_ppo2.py:189][0m |          -0.0094 |           2.8707 |           3.9922 |
[32m[20230114 18:47:25 @agent_ppo2.py:189][0m |          -0.0087 |           2.7248 |           3.9911 |
[32m[20230114 18:47:25 @agent_ppo2.py:189][0m |          -0.0094 |           2.5914 |           3.9918 |
[32m[20230114 18:47:25 @agent_ppo2.py:134][0m Policy update time: 0.84 s
[32m[20230114 18:47:25 @agent_ppo2.py:142][0m Average TRAINING episode reward: 256.56
[32m[20230114 18:47:25 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 260.00
[32m[20230114 18:47:25 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 275.03
[32m[20230114 18:47:25 @agent_ppo2.py:147][0m Total time:       5.35 min
[32m[20230114 18:47:25 @agent_ppo2.py:149][0m 413696 total steps have happened
[32m[20230114 18:47:25 @agent_ppo2.py:125][0m #------------------------ Iteration 202 --------------------------#
[32m[20230114 18:47:25 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:47:25 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:47:26 @agent_ppo2.py:189][0m |          -0.0001 |          12.1854 |           4.0121 |
[32m[20230114 18:47:26 @agent_ppo2.py:189][0m |          -0.0045 |          11.8371 |           4.0098 |
[32m[20230114 18:47:26 @agent_ppo2.py:189][0m |          -0.0080 |          11.5931 |           4.0053 |
[32m[20230114 18:47:26 @agent_ppo2.py:189][0m |          -0.0085 |          11.5164 |           4.0079 |
[32m[20230114 18:47:26 @agent_ppo2.py:189][0m |          -0.0083 |          11.6559 |           4.0031 |
[32m[20230114 18:47:26 @agent_ppo2.py:189][0m |          -0.0097 |          11.5360 |           4.0056 |
[32m[20230114 18:47:26 @agent_ppo2.py:189][0m |          -0.0103 |          11.2751 |           4.0061 |
[32m[20230114 18:47:26 @agent_ppo2.py:189][0m |          -0.0097 |          11.4571 |           4.0063 |
[32m[20230114 18:47:26 @agent_ppo2.py:189][0m |          -0.0119 |          11.1780 |           4.0037 |
[32m[20230114 18:47:26 @agent_ppo2.py:189][0m |          -0.0097 |          11.3550 |           4.0044 |
[32m[20230114 18:47:26 @agent_ppo2.py:134][0m Policy update time: 0.88 s
[32m[20230114 18:47:27 @agent_ppo2.py:142][0m Average TRAINING episode reward: 256.01
[32m[20230114 18:47:27 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 258.50
[32m[20230114 18:47:27 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 136.98
[32m[20230114 18:47:27 @agent_ppo2.py:147][0m Total time:       5.37 min
[32m[20230114 18:47:27 @agent_ppo2.py:149][0m 415744 total steps have happened
[32m[20230114 18:47:27 @agent_ppo2.py:125][0m #------------------------ Iteration 203 --------------------------#
[32m[20230114 18:47:27 @agent_ppo2.py:131][0m Sampling time: 0.27 s by 4 slaves
[32m[20230114 18:47:27 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:47:27 @agent_ppo2.py:189][0m |           0.0011 |          12.5653 |           3.9977 |
[32m[20230114 18:47:27 @agent_ppo2.py:189][0m |          -0.0038 |          11.8074 |           3.9877 |
[32m[20230114 18:47:27 @agent_ppo2.py:189][0m |          -0.0025 |          11.7706 |           3.9914 |
[32m[20230114 18:47:27 @agent_ppo2.py:189][0m |          -0.0084 |          11.3899 |           3.9877 |
[32m[20230114 18:47:27 @agent_ppo2.py:189][0m |          -0.0036 |          11.6816 |           3.9917 |
[32m[20230114 18:47:27 @agent_ppo2.py:189][0m |          -0.0064 |          11.2246 |           3.9914 |
[32m[20230114 18:47:27 @agent_ppo2.py:189][0m |          -0.0085 |          10.8664 |           3.9920 |
[32m[20230114 18:47:28 @agent_ppo2.py:189][0m |          -0.0114 |          10.6614 |           3.9948 |
[32m[20230114 18:47:28 @agent_ppo2.py:189][0m |          -0.0105 |          10.5017 |           3.9974 |
[32m[20230114 18:47:28 @agent_ppo2.py:189][0m |          -0.0086 |          10.3843 |           3.9910 |
[32m[20230114 18:47:28 @agent_ppo2.py:134][0m Policy update time: 0.91 s
[32m[20230114 18:47:28 @agent_ppo2.py:142][0m Average TRAINING episode reward: 255.34
[32m[20230114 18:47:28 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 257.20
[32m[20230114 18:47:28 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 274.72
[32m[20230114 18:47:28 @agent_ppo2.py:147][0m Total time:       5.40 min
[32m[20230114 18:47:28 @agent_ppo2.py:149][0m 417792 total steps have happened
[32m[20230114 18:47:28 @agent_ppo2.py:125][0m #------------------------ Iteration 204 --------------------------#
[32m[20230114 18:47:28 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:47:28 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:47:28 @agent_ppo2.py:189][0m |           0.0005 |          10.5567 |           4.0895 |
[32m[20230114 18:47:28 @agent_ppo2.py:189][0m |          -0.0039 |          10.1072 |           4.0805 |
[32m[20230114 18:47:29 @agent_ppo2.py:189][0m |          -0.0065 |           9.6149 |           4.0832 |
[32m[20230114 18:47:29 @agent_ppo2.py:189][0m |          -0.0072 |           9.3666 |           4.0849 |
[32m[20230114 18:47:29 @agent_ppo2.py:189][0m |          -0.0079 |           9.1539 |           4.0851 |
[32m[20230114 18:47:29 @agent_ppo2.py:189][0m |          -0.0094 |           8.8165 |           4.0843 |
[32m[20230114 18:47:29 @agent_ppo2.py:189][0m |          -0.0087 |           8.6854 |           4.0869 |
[32m[20230114 18:47:29 @agent_ppo2.py:189][0m |          -0.0099 |           8.5138 |           4.0863 |
[32m[20230114 18:47:29 @agent_ppo2.py:189][0m |          -0.0092 |           8.4246 |           4.0852 |
[32m[20230114 18:47:29 @agent_ppo2.py:189][0m |          -0.0098 |           8.2553 |           4.0870 |
[32m[20230114 18:47:29 @agent_ppo2.py:134][0m Policy update time: 0.90 s
[32m[20230114 18:47:29 @agent_ppo2.py:142][0m Average TRAINING episode reward: 258.63
[32m[20230114 18:47:29 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 261.54
[32m[20230114 18:47:29 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 275.32
[32m[20230114 18:47:29 @agent_ppo2.py:147][0m Total time:       5.42 min
[32m[20230114 18:47:29 @agent_ppo2.py:149][0m 419840 total steps have happened
[32m[20230114 18:47:29 @agent_ppo2.py:125][0m #------------------------ Iteration 205 --------------------------#
[32m[20230114 18:47:30 @agent_ppo2.py:131][0m Sampling time: 0.25 s by 4 slaves
[32m[20230114 18:47:30 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:47:30 @agent_ppo2.py:189][0m |           0.0006 |          11.9930 |           4.1640 |
[32m[20230114 18:47:30 @agent_ppo2.py:189][0m |          -0.0036 |          10.9613 |           4.1606 |
[32m[20230114 18:47:30 @agent_ppo2.py:189][0m |          -0.0055 |          10.6170 |           4.1615 |
[32m[20230114 18:47:30 @agent_ppo2.py:189][0m |          -0.0062 |          10.3857 |           4.1601 |
[32m[20230114 18:47:30 @agent_ppo2.py:189][0m |          -0.0071 |          10.1823 |           4.1585 |
[32m[20230114 18:47:30 @agent_ppo2.py:189][0m |          -0.0087 |           9.9751 |           4.1593 |
[32m[20230114 18:47:30 @agent_ppo2.py:189][0m |          -0.0093 |           9.7241 |           4.1605 |
[32m[20230114 18:47:30 @agent_ppo2.py:189][0m |          -0.0096 |           9.4413 |           4.1604 |
[32m[20230114 18:47:30 @agent_ppo2.py:189][0m |          -0.0107 |           9.2231 |           4.1552 |
[32m[20230114 18:47:30 @agent_ppo2.py:189][0m |          -0.0101 |           9.0071 |           4.1612 |
[32m[20230114 18:47:30 @agent_ppo2.py:134][0m Policy update time: 0.88 s
[32m[20230114 18:47:31 @agent_ppo2.py:142][0m Average TRAINING episode reward: 253.66
[32m[20230114 18:47:31 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 257.03
[32m[20230114 18:47:31 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 275.41
[32m[20230114 18:47:31 @agent_ppo2.py:147][0m Total time:       5.44 min
[32m[20230114 18:47:31 @agent_ppo2.py:149][0m 421888 total steps have happened
[32m[20230114 18:47:31 @agent_ppo2.py:125][0m #------------------------ Iteration 206 --------------------------#
[32m[20230114 18:47:31 @agent_ppo2.py:131][0m Sampling time: 0.28 s by 4 slaves
[32m[20230114 18:47:31 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:47:31 @agent_ppo2.py:189][0m |          -0.0016 |          11.3380 |           4.1092 |
[32m[20230114 18:47:31 @agent_ppo2.py:189][0m |          -0.0061 |          10.5438 |           4.0977 |
[32m[20230114 18:47:31 @agent_ppo2.py:189][0m |          -0.0072 |          10.0900 |           4.0923 |
[32m[20230114 18:47:31 @agent_ppo2.py:189][0m |          -0.0087 |           9.8193 |           4.0906 |
[32m[20230114 18:47:31 @agent_ppo2.py:189][0m |          -0.0096 |           9.6243 |           4.0868 |
[32m[20230114 18:47:31 @agent_ppo2.py:189][0m |          -0.0103 |           9.4706 |           4.0867 |
[32m[20230114 18:47:32 @agent_ppo2.py:189][0m |          -0.0107 |           9.2114 |           4.0848 |
[32m[20230114 18:47:32 @agent_ppo2.py:189][0m |          -0.0104 |           9.0418 |           4.0830 |
[32m[20230114 18:47:32 @agent_ppo2.py:189][0m |          -0.0119 |           8.7881 |           4.0813 |
[32m[20230114 18:47:32 @agent_ppo2.py:189][0m |          -0.0122 |           8.6115 |           4.0796 |
[32m[20230114 18:47:32 @agent_ppo2.py:134][0m Policy update time: 0.88 s
[32m[20230114 18:47:32 @agent_ppo2.py:142][0m Average TRAINING episode reward: 258.66
[32m[20230114 18:47:32 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 261.26
[32m[20230114 18:47:32 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 275.95
[32m[20230114 18:47:32 @agent_ppo2.py:147][0m Total time:       5.46 min
[32m[20230114 18:47:32 @agent_ppo2.py:149][0m 423936 total steps have happened
[32m[20230114 18:47:32 @agent_ppo2.py:125][0m #------------------------ Iteration 207 --------------------------#
[32m[20230114 18:47:32 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:47:32 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:47:32 @agent_ppo2.py:189][0m |          -0.0004 |          15.5249 |           4.1149 |
[32m[20230114 18:47:32 @agent_ppo2.py:189][0m |          -0.0043 |          12.2438 |           4.1061 |
[32m[20230114 18:47:33 @agent_ppo2.py:189][0m |          -0.0068 |          11.5107 |           4.0966 |
[32m[20230114 18:47:33 @agent_ppo2.py:189][0m |          -0.0076 |          11.0790 |           4.0943 |
[32m[20230114 18:47:33 @agent_ppo2.py:189][0m |          -0.0081 |          10.7555 |           4.0937 |
[32m[20230114 18:47:33 @agent_ppo2.py:189][0m |          -0.0082 |          10.4600 |           4.0874 |
[32m[20230114 18:47:33 @agent_ppo2.py:189][0m |          -0.0091 |          10.2868 |           4.0885 |
[32m[20230114 18:47:33 @agent_ppo2.py:189][0m |          -0.0107 |           9.8775 |           4.0872 |
[32m[20230114 18:47:33 @agent_ppo2.py:189][0m |          -0.0121 |           9.5533 |           4.0817 |
[32m[20230114 18:47:33 @agent_ppo2.py:189][0m |          -0.0117 |           9.3800 |           4.0806 |
[32m[20230114 18:47:33 @agent_ppo2.py:134][0m Policy update time: 0.88 s
[32m[20230114 18:47:33 @agent_ppo2.py:142][0m Average TRAINING episode reward: 231.05
[32m[20230114 18:47:33 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 257.93
[32m[20230114 18:47:33 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 279.41
[32m[20230114 18:47:33 @agent_ppo2.py:147][0m Total time:       5.49 min
[32m[20230114 18:47:33 @agent_ppo2.py:149][0m 425984 total steps have happened
[32m[20230114 18:47:33 @agent_ppo2.py:125][0m #------------------------ Iteration 208 --------------------------#
[32m[20230114 18:47:34 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:47:34 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:47:34 @agent_ppo2.py:189][0m |           0.0011 |          12.5630 |           4.0633 |
[32m[20230114 18:47:34 @agent_ppo2.py:189][0m |           0.0015 |          12.1398 |           4.0589 |
[32m[20230114 18:47:34 @agent_ppo2.py:189][0m |          -0.0019 |          11.8583 |           4.0560 |
[32m[20230114 18:47:34 @agent_ppo2.py:189][0m |          -0.0021 |          11.9456 |           4.0578 |
[32m[20230114 18:47:34 @agent_ppo2.py:189][0m |          -0.0057 |          11.4183 |           4.0577 |
[32m[20230114 18:47:34 @agent_ppo2.py:189][0m |          -0.0039 |          11.4714 |           4.0579 |
[32m[20230114 18:47:34 @agent_ppo2.py:189][0m |          -0.0075 |          11.2639 |           4.0569 |
[32m[20230114 18:47:34 @agent_ppo2.py:189][0m |          -0.0094 |          11.2468 |           4.0560 |
[32m[20230114 18:47:34 @agent_ppo2.py:189][0m |          -0.0103 |          11.1558 |           4.0578 |
[32m[20230114 18:47:34 @agent_ppo2.py:189][0m |          -0.0091 |          11.0921 |           4.0573 |
[32m[20230114 18:47:34 @agent_ppo2.py:134][0m Policy update time: 0.86 s
[32m[20230114 18:47:35 @agent_ppo2.py:142][0m Average TRAINING episode reward: 257.54
[32m[20230114 18:47:35 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 259.92
[32m[20230114 18:47:35 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 276.13
[32m[20230114 18:47:35 @agent_ppo2.py:147][0m Total time:       5.51 min
[32m[20230114 18:47:35 @agent_ppo2.py:149][0m 428032 total steps have happened
[32m[20230114 18:47:35 @agent_ppo2.py:125][0m #------------------------ Iteration 209 --------------------------#
[32m[20230114 18:47:35 @agent_ppo2.py:131][0m Sampling time: 0.26 s by 4 slaves
[32m[20230114 18:47:35 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:47:35 @agent_ppo2.py:189][0m |          -0.0005 |          11.0794 |           4.1089 |
[32m[20230114 18:47:35 @agent_ppo2.py:189][0m |          -0.0053 |          10.5888 |           4.1045 |
[32m[20230114 18:47:35 @agent_ppo2.py:189][0m |          -0.0037 |          10.3324 |           4.0963 |
[32m[20230114 18:47:35 @agent_ppo2.py:189][0m |          -0.0008 |          10.4058 |           4.0985 |
[32m[20230114 18:47:35 @agent_ppo2.py:189][0m |          -0.0055 |           9.8210 |           4.0970 |
[32m[20230114 18:47:35 @agent_ppo2.py:189][0m |          -0.0078 |           9.5961 |           4.0950 |
[32m[20230114 18:47:36 @agent_ppo2.py:189][0m |          -0.0039 |           9.5356 |           4.0943 |
[32m[20230114 18:47:36 @agent_ppo2.py:189][0m |          -0.0093 |           9.1084 |           4.0969 |
[32m[20230114 18:47:36 @agent_ppo2.py:189][0m |          -0.0088 |           8.9133 |           4.0974 |
[32m[20230114 18:47:36 @agent_ppo2.py:189][0m |          -0.0079 |           8.7301 |           4.0936 |
[32m[20230114 18:47:36 @agent_ppo2.py:134][0m Policy update time: 0.86 s
[32m[20230114 18:47:36 @agent_ppo2.py:142][0m Average TRAINING episode reward: 255.26
[32m[20230114 18:47:36 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 258.68
[32m[20230114 18:47:36 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 273.55
[32m[20230114 18:47:36 @agent_ppo2.py:147][0m Total time:       5.53 min
[32m[20230114 18:47:36 @agent_ppo2.py:149][0m 430080 total steps have happened
[32m[20230114 18:47:36 @agent_ppo2.py:125][0m #------------------------ Iteration 210 --------------------------#
[32m[20230114 18:47:36 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:47:36 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:47:36 @agent_ppo2.py:189][0m |           0.0008 |          12.7928 |           4.0507 |
[32m[20230114 18:47:36 @agent_ppo2.py:189][0m |          -0.0030 |          12.1905 |           4.0476 |
[32m[20230114 18:47:37 @agent_ppo2.py:189][0m |          -0.0055 |          11.7715 |           4.0456 |
[32m[20230114 18:47:37 @agent_ppo2.py:189][0m |          -0.0073 |          11.4548 |           4.0414 |
[32m[20230114 18:47:37 @agent_ppo2.py:189][0m |          -0.0066 |          11.3406 |           4.0411 |
[32m[20230114 18:47:37 @agent_ppo2.py:189][0m |          -0.0085 |          10.9843 |           4.0373 |
[32m[20230114 18:47:37 @agent_ppo2.py:189][0m |          -0.0090 |          10.7948 |           4.0375 |
[32m[20230114 18:47:37 @agent_ppo2.py:189][0m |          -0.0105 |          10.5956 |           4.0344 |
[32m[20230114 18:47:37 @agent_ppo2.py:189][0m |          -0.0111 |          10.4269 |           4.0357 |
[32m[20230114 18:47:37 @agent_ppo2.py:189][0m |          -0.0100 |          10.3730 |           4.0330 |
[32m[20230114 18:47:37 @agent_ppo2.py:134][0m Policy update time: 0.86 s
[32m[20230114 18:47:37 @agent_ppo2.py:142][0m Average TRAINING episode reward: 254.40
[32m[20230114 18:47:37 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 257.34
[32m[20230114 18:47:37 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 274.15
[32m[20230114 18:47:37 @agent_ppo2.py:147][0m Total time:       5.55 min
[32m[20230114 18:47:37 @agent_ppo2.py:149][0m 432128 total steps have happened
[32m[20230114 18:47:37 @agent_ppo2.py:125][0m #------------------------ Iteration 211 --------------------------#
[32m[20230114 18:47:38 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:47:38 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:47:38 @agent_ppo2.py:189][0m |           0.0002 |          12.0498 |           4.0800 |
[32m[20230114 18:47:38 @agent_ppo2.py:189][0m |          -0.0037 |          11.5443 |           4.0741 |
[32m[20230114 18:47:38 @agent_ppo2.py:189][0m |          -0.0073 |          11.1276 |           4.0725 |
[32m[20230114 18:47:38 @agent_ppo2.py:189][0m |          -0.0072 |          10.9833 |           4.0758 |
[32m[20230114 18:47:38 @agent_ppo2.py:189][0m |          -0.0089 |          10.8653 |           4.0721 |
[32m[20230114 18:47:38 @agent_ppo2.py:189][0m |          -0.0093 |          10.7756 |           4.0748 |
[32m[20230114 18:47:38 @agent_ppo2.py:189][0m |          -0.0097 |          10.7248 |           4.0782 |
[32m[20230114 18:47:38 @agent_ppo2.py:189][0m |          -0.0083 |          10.7884 |           4.0749 |
[32m[20230114 18:47:38 @agent_ppo2.py:189][0m |          -0.0115 |          10.5623 |           4.0761 |
[32m[20230114 18:47:38 @agent_ppo2.py:189][0m |          -0.0102 |          10.5448 |           4.0792 |
[32m[20230114 18:47:38 @agent_ppo2.py:134][0m Policy update time: 0.85 s
[32m[20230114 18:47:39 @agent_ppo2.py:142][0m Average TRAINING episode reward: 257.03
[32m[20230114 18:47:39 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 258.64
[32m[20230114 18:47:39 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 276.40
[32m[20230114 18:47:39 @agent_ppo2.py:147][0m Total time:       5.57 min
[32m[20230114 18:47:39 @agent_ppo2.py:149][0m 434176 total steps have happened
[32m[20230114 18:47:39 @agent_ppo2.py:125][0m #------------------------ Iteration 212 --------------------------#
[32m[20230114 18:47:39 @agent_ppo2.py:131][0m Sampling time: 0.26 s by 4 slaves
[32m[20230114 18:47:39 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:47:39 @agent_ppo2.py:189][0m |           0.0005 |          10.7891 |           4.0270 |
[32m[20230114 18:47:39 @agent_ppo2.py:189][0m |          -0.0031 |           9.9149 |           4.0224 |
[32m[20230114 18:47:39 @agent_ppo2.py:189][0m |          -0.0055 |           9.2212 |           4.0198 |
[32m[20230114 18:47:39 @agent_ppo2.py:189][0m |          -0.0061 |           8.7429 |           4.0211 |
[32m[20230114 18:47:39 @agent_ppo2.py:189][0m |          -0.0073 |           8.3547 |           4.0236 |
[32m[20230114 18:47:39 @agent_ppo2.py:189][0m |          -0.0088 |           7.9999 |           4.0166 |
[32m[20230114 18:47:39 @agent_ppo2.py:189][0m |          -0.0094 |           7.7554 |           4.0211 |
[32m[20230114 18:47:40 @agent_ppo2.py:189][0m |          -0.0082 |           7.5561 |           4.0198 |
[32m[20230114 18:47:40 @agent_ppo2.py:189][0m |          -0.0098 |           7.3473 |           4.0207 |
[32m[20230114 18:47:40 @agent_ppo2.py:189][0m |          -0.0105 |           7.1781 |           4.0193 |
[32m[20230114 18:47:40 @agent_ppo2.py:134][0m Policy update time: 0.83 s
[32m[20230114 18:47:40 @agent_ppo2.py:142][0m Average TRAINING episode reward: 259.62
[32m[20230114 18:47:40 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 262.44
[32m[20230114 18:47:40 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 275.37
[32m[20230114 18:47:40 @agent_ppo2.py:147][0m Total time:       5.60 min
[32m[20230114 18:47:40 @agent_ppo2.py:149][0m 436224 total steps have happened
[32m[20230114 18:47:40 @agent_ppo2.py:125][0m #------------------------ Iteration 213 --------------------------#
[32m[20230114 18:47:40 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:47:40 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:47:40 @agent_ppo2.py:189][0m |           0.0019 |          14.2106 |           4.0865 |
[32m[20230114 18:47:40 @agent_ppo2.py:189][0m |          -0.0051 |          11.2602 |           4.0812 |
[32m[20230114 18:47:40 @agent_ppo2.py:189][0m |          -0.0046 |          10.7744 |           4.0805 |
[32m[20230114 18:47:41 @agent_ppo2.py:189][0m |          -0.0073 |          10.1467 |           4.0823 |
[32m[20230114 18:47:41 @agent_ppo2.py:189][0m |          -0.0092 |           9.7558 |           4.0781 |
[32m[20230114 18:47:41 @agent_ppo2.py:189][0m |          -0.0102 |           9.3864 |           4.0801 |
[32m[20230114 18:47:41 @agent_ppo2.py:189][0m |          -0.0112 |           9.1515 |           4.0794 |
[32m[20230114 18:47:41 @agent_ppo2.py:189][0m |          -0.0127 |           9.0155 |           4.0784 |
[32m[20230114 18:47:41 @agent_ppo2.py:189][0m |          -0.0093 |           8.9492 |           4.0804 |
[32m[20230114 18:47:41 @agent_ppo2.py:189][0m |          -0.0100 |           8.9981 |           4.0766 |
[32m[20230114 18:47:41 @agent_ppo2.py:134][0m Policy update time: 0.85 s
[32m[20230114 18:47:41 @agent_ppo2.py:142][0m Average TRAINING episode reward: 213.36
[32m[20230114 18:47:41 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 256.58
[32m[20230114 18:47:41 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 278.26
[32m[20230114 18:47:41 @agent_ppo2.py:147][0m Total time:       5.62 min
[32m[20230114 18:47:41 @agent_ppo2.py:149][0m 438272 total steps have happened
[32m[20230114 18:47:41 @agent_ppo2.py:125][0m #------------------------ Iteration 214 --------------------------#
[32m[20230114 18:47:41 @agent_ppo2.py:131][0m Sampling time: 0.25 s by 4 slaves
[32m[20230114 18:47:42 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:47:42 @agent_ppo2.py:189][0m |          -0.0010 |          12.1908 |           4.0761 |
[32m[20230114 18:47:42 @agent_ppo2.py:189][0m |          -0.0067 |          11.1557 |           4.0673 |
[32m[20230114 18:47:42 @agent_ppo2.py:189][0m |          -0.0082 |          10.8854 |           4.0646 |
[32m[20230114 18:47:42 @agent_ppo2.py:189][0m |          -0.0097 |          10.7131 |           4.0657 |
[32m[20230114 18:47:42 @agent_ppo2.py:189][0m |          -0.0114 |          10.4443 |           4.0683 |
[32m[20230114 18:47:42 @agent_ppo2.py:189][0m |          -0.0111 |          10.3438 |           4.0650 |
[32m[20230114 18:47:42 @agent_ppo2.py:189][0m |          -0.0125 |          10.2371 |           4.0648 |
[32m[20230114 18:47:42 @agent_ppo2.py:189][0m |          -0.0125 |          10.0974 |           4.0648 |
[32m[20230114 18:47:42 @agent_ppo2.py:189][0m |          -0.0130 |           9.9988 |           4.0651 |
[32m[20230114 18:47:42 @agent_ppo2.py:189][0m |          -0.0138 |           9.9130 |           4.0706 |
[32m[20230114 18:47:42 @agent_ppo2.py:134][0m Policy update time: 0.90 s
[32m[20230114 18:47:43 @agent_ppo2.py:142][0m Average TRAINING episode reward: 254.89
[32m[20230114 18:47:43 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 257.52
[32m[20230114 18:47:43 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 278.25
[32m[20230114 18:47:43 @agent_ppo2.py:147][0m Total time:       5.64 min
[32m[20230114 18:47:43 @agent_ppo2.py:149][0m 440320 total steps have happened
[32m[20230114 18:47:43 @agent_ppo2.py:125][0m #------------------------ Iteration 215 --------------------------#
[32m[20230114 18:47:43 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:47:43 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:47:43 @agent_ppo2.py:189][0m |           0.0003 |          14.6292 |           4.0692 |
[32m[20230114 18:47:43 @agent_ppo2.py:189][0m |          -0.0040 |          11.7326 |           4.0670 |
[32m[20230114 18:47:43 @agent_ppo2.py:189][0m |          -0.0071 |          11.0080 |           4.0671 |
[32m[20230114 18:47:43 @agent_ppo2.py:189][0m |          -0.0086 |          10.5041 |           4.0666 |
[32m[20230114 18:47:43 @agent_ppo2.py:189][0m |          -0.0099 |          10.1362 |           4.0654 |
[32m[20230114 18:47:43 @agent_ppo2.py:189][0m |          -0.0106 |           9.9373 |           4.0622 |
[32m[20230114 18:47:43 @agent_ppo2.py:189][0m |          -0.0102 |           9.6761 |           4.0642 |
[32m[20230114 18:47:44 @agent_ppo2.py:189][0m |          -0.0121 |           9.4323 |           4.0632 |
[32m[20230114 18:47:44 @agent_ppo2.py:189][0m |          -0.0127 |           9.2511 |           4.0606 |
[32m[20230114 18:47:44 @agent_ppo2.py:189][0m |          -0.0124 |           9.1408 |           4.0588 |
[32m[20230114 18:47:44 @agent_ppo2.py:134][0m Policy update time: 0.84 s
[32m[20230114 18:47:44 @agent_ppo2.py:142][0m Average TRAINING episode reward: 219.67
[32m[20230114 18:47:44 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 261.37
[32m[20230114 18:47:44 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 266.62
[32m[20230114 18:47:44 @agent_ppo2.py:147][0m Total time:       5.66 min
[32m[20230114 18:47:44 @agent_ppo2.py:149][0m 442368 total steps have happened
[32m[20230114 18:47:44 @agent_ppo2.py:125][0m #------------------------ Iteration 216 --------------------------#
[32m[20230114 18:47:44 @agent_ppo2.py:131][0m Sampling time: 0.26 s by 4 slaves
[32m[20230114 18:47:44 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:47:44 @agent_ppo2.py:189][0m |          -0.0051 |          12.9524 |           4.0886 |
[32m[20230114 18:47:44 @agent_ppo2.py:189][0m |          -0.0049 |          12.3776 |           4.0836 |
[32m[20230114 18:47:45 @agent_ppo2.py:189][0m |          -0.0074 |          12.1377 |           4.0787 |
[32m[20230114 18:47:45 @agent_ppo2.py:189][0m |          -0.0084 |          12.0086 |           4.0817 |
[32m[20230114 18:47:45 @agent_ppo2.py:189][0m |          -0.0066 |          11.8974 |           4.0783 |
[32m[20230114 18:47:45 @agent_ppo2.py:189][0m |          -0.0051 |          12.0852 |           4.0774 |
[32m[20230114 18:47:45 @agent_ppo2.py:189][0m |          -0.0107 |          11.6756 |           4.0751 |
[32m[20230114 18:47:45 @agent_ppo2.py:189][0m |          -0.0111 |          11.5754 |           4.0762 |
[32m[20230114 18:47:45 @agent_ppo2.py:189][0m |          -0.0108 |          11.5391 |           4.0756 |
[32m[20230114 18:47:45 @agent_ppo2.py:189][0m |          -0.0078 |          11.4741 |           4.0757 |
[32m[20230114 18:47:45 @agent_ppo2.py:134][0m Policy update time: 0.89 s
[32m[20230114 18:47:45 @agent_ppo2.py:142][0m Average TRAINING episode reward: 252.75
[32m[20230114 18:47:45 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 258.47
[32m[20230114 18:47:45 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 276.47
[32m[20230114 18:47:45 @agent_ppo2.py:147][0m Total time:       5.69 min
[32m[20230114 18:47:45 @agent_ppo2.py:149][0m 444416 total steps have happened
[32m[20230114 18:47:45 @agent_ppo2.py:125][0m #------------------------ Iteration 217 --------------------------#
[32m[20230114 18:47:46 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:47:46 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:47:46 @agent_ppo2.py:189][0m |          -0.0049 |          11.5864 |           4.0506 |
[32m[20230114 18:47:46 @agent_ppo2.py:189][0m |          -0.0095 |          10.8690 |           4.0426 |
[32m[20230114 18:47:46 @agent_ppo2.py:189][0m |          -0.0134 |          10.4122 |           4.0402 |
[32m[20230114 18:47:46 @agent_ppo2.py:189][0m |          -0.0126 |          10.0191 |           4.0395 |
[32m[20230114 18:47:46 @agent_ppo2.py:189][0m |          -0.0099 |           9.5815 |           4.0373 |
[32m[20230114 18:47:46 @agent_ppo2.py:189][0m |          -0.0090 |           9.1544 |           4.0374 |
[32m[20230114 18:47:46 @agent_ppo2.py:189][0m |          -0.0144 |           8.8372 |           4.0367 |
[32m[20230114 18:47:46 @agent_ppo2.py:189][0m |          -0.0146 |           8.4780 |           4.0359 |
[32m[20230114 18:47:46 @agent_ppo2.py:189][0m |          -0.0130 |           8.2601 |           4.0350 |
[32m[20230114 18:47:46 @agent_ppo2.py:189][0m |          -0.0131 |           7.9600 |           4.0359 |
[32m[20230114 18:47:46 @agent_ppo2.py:134][0m Policy update time: 0.84 s
[32m[20230114 18:47:47 @agent_ppo2.py:142][0m Average TRAINING episode reward: 256.16
[32m[20230114 18:47:47 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 257.87
[32m[20230114 18:47:47 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 273.62
[32m[20230114 18:47:47 @agent_ppo2.py:147][0m Total time:       5.71 min
[32m[20230114 18:47:47 @agent_ppo2.py:149][0m 446464 total steps have happened
[32m[20230114 18:47:47 @agent_ppo2.py:125][0m #------------------------ Iteration 218 --------------------------#
[32m[20230114 18:47:47 @agent_ppo2.py:131][0m Sampling time: 0.29 s by 4 slaves
[32m[20230114 18:47:47 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:47:47 @agent_ppo2.py:189][0m |           0.0032 |          13.5105 |           4.0830 |
[32m[20230114 18:47:47 @agent_ppo2.py:189][0m |          -0.0030 |          12.5884 |           4.0822 |
[32m[20230114 18:47:47 @agent_ppo2.py:189][0m |          -0.0055 |          12.2568 |           4.0808 |
[32m[20230114 18:47:47 @agent_ppo2.py:189][0m |          -0.0066 |          11.9760 |           4.0862 |
[32m[20230114 18:47:47 @agent_ppo2.py:189][0m |          -0.0081 |          11.7900 |           4.0834 |
[32m[20230114 18:47:47 @agent_ppo2.py:189][0m |          -0.0082 |          11.6895 |           4.0842 |
[32m[20230114 18:47:48 @agent_ppo2.py:189][0m |          -0.0096 |          11.4333 |           4.0866 |
[32m[20230114 18:47:48 @agent_ppo2.py:189][0m |          -0.0102 |          11.3272 |           4.0873 |
[32m[20230114 18:47:48 @agent_ppo2.py:189][0m |          -0.0104 |          11.2751 |           4.0843 |
[32m[20230114 18:47:48 @agent_ppo2.py:189][0m |          -0.0113 |          11.0883 |           4.0908 |
[32m[20230114 18:47:48 @agent_ppo2.py:134][0m Policy update time: 0.84 s
[32m[20230114 18:47:48 @agent_ppo2.py:142][0m Average TRAINING episode reward: 259.31
[32m[20230114 18:47:48 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 260.81
[32m[20230114 18:47:48 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 275.73
[32m[20230114 18:47:48 @agent_ppo2.py:147][0m Total time:       5.73 min
[32m[20230114 18:47:48 @agent_ppo2.py:149][0m 448512 total steps have happened
[32m[20230114 18:47:48 @agent_ppo2.py:125][0m #------------------------ Iteration 219 --------------------------#
[32m[20230114 18:47:48 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:47:48 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:47:48 @agent_ppo2.py:189][0m |          -0.0014 |          11.7501 |           4.1425 |
[32m[20230114 18:47:48 @agent_ppo2.py:189][0m |          -0.0086 |          11.1240 |           4.1376 |
[32m[20230114 18:47:49 @agent_ppo2.py:189][0m |          -0.0059 |          11.3512 |           4.1303 |
[32m[20230114 18:47:49 @agent_ppo2.py:189][0m |          -0.0056 |          11.1823 |           4.1387 |
[32m[20230114 18:47:49 @agent_ppo2.py:189][0m |          -0.0108 |          10.6788 |           4.1324 |
[32m[20230114 18:47:49 @agent_ppo2.py:189][0m |          -0.0113 |          10.5634 |           4.1335 |
[32m[20230114 18:47:49 @agent_ppo2.py:189][0m |          -0.0092 |          10.7571 |           4.1316 |
[32m[20230114 18:47:49 @agent_ppo2.py:189][0m |          -0.0110 |          10.4747 |           4.1327 |
[32m[20230114 18:47:49 @agent_ppo2.py:189][0m |          -0.0136 |          10.3320 |           4.1305 |
[32m[20230114 18:47:49 @agent_ppo2.py:189][0m |          -0.0108 |          10.2995 |           4.1301 |
[32m[20230114 18:47:49 @agent_ppo2.py:134][0m Policy update time: 0.84 s
[32m[20230114 18:47:49 @agent_ppo2.py:142][0m Average TRAINING episode reward: 259.34
[32m[20230114 18:47:49 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 262.17
[32m[20230114 18:47:49 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 268.91
[32m[20230114 18:47:49 @agent_ppo2.py:147][0m Total time:       5.75 min
[32m[20230114 18:47:49 @agent_ppo2.py:149][0m 450560 total steps have happened
[32m[20230114 18:47:49 @agent_ppo2.py:125][0m #------------------------ Iteration 220 --------------------------#
[32m[20230114 18:47:50 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:47:50 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:47:50 @agent_ppo2.py:189][0m |          -0.0000 |          12.6194 |           4.0653 |
[32m[20230114 18:47:50 @agent_ppo2.py:189][0m |          -0.0029 |          12.0969 |           4.0536 |
[32m[20230114 18:47:50 @agent_ppo2.py:189][0m |          -0.0073 |          11.4622 |           4.0556 |
[32m[20230114 18:47:50 @agent_ppo2.py:189][0m |          -0.0087 |          11.2036 |           4.0556 |
[32m[20230114 18:47:50 @agent_ppo2.py:189][0m |          -0.0088 |          11.0490 |           4.0528 |
[32m[20230114 18:47:50 @agent_ppo2.py:189][0m |          -0.0101 |          10.9204 |           4.0521 |
[32m[20230114 18:47:50 @agent_ppo2.py:189][0m |          -0.0090 |          10.7952 |           4.0511 |
[32m[20230114 18:47:50 @agent_ppo2.py:189][0m |          -0.0095 |          10.8457 |           4.0538 |
[32m[20230114 18:47:50 @agent_ppo2.py:189][0m |          -0.0099 |          10.6386 |           4.0521 |
[32m[20230114 18:47:50 @agent_ppo2.py:189][0m |          -0.0119 |          10.6031 |           4.0542 |
[32m[20230114 18:47:50 @agent_ppo2.py:134][0m Policy update time: 0.86 s
[32m[20230114 18:47:51 @agent_ppo2.py:142][0m Average TRAINING episode reward: 257.02
[32m[20230114 18:47:51 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 259.52
[32m[20230114 18:47:51 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 261.47
[32m[20230114 18:47:51 @agent_ppo2.py:147][0m Total time:       5.77 min
[32m[20230114 18:47:51 @agent_ppo2.py:149][0m 452608 total steps have happened
[32m[20230114 18:47:51 @agent_ppo2.py:125][0m #------------------------ Iteration 221 --------------------------#
[32m[20230114 18:47:51 @agent_ppo2.py:131][0m Sampling time: 0.29 s by 4 slaves
[32m[20230114 18:47:51 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:47:51 @agent_ppo2.py:189][0m |          -0.0011 |          11.7413 |           4.1225 |
[32m[20230114 18:47:51 @agent_ppo2.py:189][0m |          -0.0060 |          11.3476 |           4.1142 |
[32m[20230114 18:47:51 @agent_ppo2.py:189][0m |          -0.0076 |          10.8961 |           4.1125 |
[32m[20230114 18:47:51 @agent_ppo2.py:189][0m |          -0.0080 |          10.7408 |           4.1126 |
[32m[20230114 18:47:51 @agent_ppo2.py:189][0m |          -0.0075 |          10.5728 |           4.1113 |
[32m[20230114 18:47:51 @agent_ppo2.py:189][0m |          -0.0067 |          10.6526 |           4.1124 |
[32m[20230114 18:47:52 @agent_ppo2.py:189][0m |          -0.0084 |          10.6209 |           4.1088 |
[32m[20230114 18:47:52 @agent_ppo2.py:189][0m |          -0.0117 |          10.2190 |           4.1093 |
[32m[20230114 18:47:52 @agent_ppo2.py:189][0m |          -0.0116 |          10.1210 |           4.1096 |
[32m[20230114 18:47:52 @agent_ppo2.py:189][0m |          -0.0127 |          10.0283 |           4.1101 |
[32m[20230114 18:47:52 @agent_ppo2.py:134][0m Policy update time: 0.83 s
[32m[20230114 18:47:52 @agent_ppo2.py:142][0m Average TRAINING episode reward: 259.89
[32m[20230114 18:47:52 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 262.00
[32m[20230114 18:47:52 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 275.32
[32m[20230114 18:47:52 @agent_ppo2.py:147][0m Total time:       5.80 min
[32m[20230114 18:47:52 @agent_ppo2.py:149][0m 454656 total steps have happened
[32m[20230114 18:47:52 @agent_ppo2.py:125][0m #------------------------ Iteration 222 --------------------------#
[32m[20230114 18:47:52 @agent_ppo2.py:131][0m Sampling time: 0.25 s by 4 slaves
[32m[20230114 18:47:52 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:47:52 @agent_ppo2.py:189][0m |           0.0021 |          12.4491 |           4.1200 |
[32m[20230114 18:47:52 @agent_ppo2.py:189][0m |          -0.0015 |          11.7375 |           4.1179 |
[32m[20230114 18:47:53 @agent_ppo2.py:189][0m |          -0.0025 |          11.4559 |           4.1150 |
[32m[20230114 18:47:53 @agent_ppo2.py:189][0m |          -0.0045 |          11.2612 |           4.1178 |
[32m[20230114 18:47:53 @agent_ppo2.py:189][0m |          -0.0053 |          11.1161 |           4.1181 |
[32m[20230114 18:47:53 @agent_ppo2.py:189][0m |          -0.0072 |          10.9921 |           4.1148 |
[32m[20230114 18:47:53 @agent_ppo2.py:189][0m |          -0.0078 |          10.8291 |           4.1143 |
[32m[20230114 18:47:53 @agent_ppo2.py:189][0m |          -0.0094 |          10.7747 |           4.1151 |
[32m[20230114 18:47:53 @agent_ppo2.py:189][0m |          -0.0078 |          10.7530 |           4.1166 |
[32m[20230114 18:47:53 @agent_ppo2.py:189][0m |          -0.0069 |          10.8413 |           4.1134 |
[32m[20230114 18:47:53 @agent_ppo2.py:134][0m Policy update time: 0.88 s
[32m[20230114 18:47:53 @agent_ppo2.py:142][0m Average TRAINING episode reward: 254.60
[32m[20230114 18:47:53 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 257.16
[32m[20230114 18:47:53 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 272.57
[32m[20230114 18:47:53 @agent_ppo2.py:147][0m Total time:       5.82 min
[32m[20230114 18:47:53 @agent_ppo2.py:149][0m 456704 total steps have happened
[32m[20230114 18:47:53 @agent_ppo2.py:125][0m #------------------------ Iteration 223 --------------------------#
[32m[20230114 18:47:54 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:47:54 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:47:54 @agent_ppo2.py:189][0m |          -0.0010 |          11.5684 |           4.0400 |
[32m[20230114 18:47:54 @agent_ppo2.py:189][0m |          -0.0044 |          10.6270 |           4.0359 |
[32m[20230114 18:47:54 @agent_ppo2.py:189][0m |          -0.0059 |          10.0837 |           4.0326 |
[32m[20230114 18:47:54 @agent_ppo2.py:189][0m |          -0.0032 |           9.8610 |           4.0315 |
[32m[20230114 18:47:54 @agent_ppo2.py:189][0m |          -0.0086 |           9.3655 |           4.0274 |
[32m[20230114 18:47:54 @agent_ppo2.py:189][0m |          -0.0100 |           8.8426 |           4.0301 |
[32m[20230114 18:47:54 @agent_ppo2.py:189][0m |          -0.0097 |           8.4892 |           4.0300 |
[32m[20230114 18:47:54 @agent_ppo2.py:189][0m |          -0.0112 |           8.1781 |           4.0229 |
[32m[20230114 18:47:54 @agent_ppo2.py:189][0m |          -0.0104 |           7.8875 |           4.0258 |
[32m[20230114 18:47:54 @agent_ppo2.py:189][0m |          -0.0128 |           7.4245 |           4.0234 |
[32m[20230114 18:47:54 @agent_ppo2.py:134][0m Policy update time: 0.82 s
[32m[20230114 18:47:55 @agent_ppo2.py:142][0m Average TRAINING episode reward: 259.24
[32m[20230114 18:47:55 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 261.32
[32m[20230114 18:47:55 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 276.55
[32m[20230114 18:47:55 @agent_ppo2.py:147][0m Total time:       5.84 min
[32m[20230114 18:47:55 @agent_ppo2.py:149][0m 458752 total steps have happened
[32m[20230114 18:47:55 @agent_ppo2.py:125][0m #------------------------ Iteration 224 --------------------------#
[32m[20230114 18:47:55 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:47:55 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:47:55 @agent_ppo2.py:189][0m |           0.0021 |          13.4213 |           4.1442 |
[32m[20230114 18:47:55 @agent_ppo2.py:189][0m |          -0.0015 |          12.6355 |           4.1423 |
[32m[20230114 18:47:55 @agent_ppo2.py:189][0m |          -0.0038 |          12.1593 |           4.1413 |
[32m[20230114 18:47:55 @agent_ppo2.py:189][0m |          -0.0053 |          11.9253 |           4.1407 |
[32m[20230114 18:47:55 @agent_ppo2.py:189][0m |          -0.0077 |          11.7445 |           4.1439 |
[32m[20230114 18:47:55 @agent_ppo2.py:189][0m |          -0.0090 |          11.6161 |           4.1443 |
[32m[20230114 18:47:55 @agent_ppo2.py:189][0m |          -0.0082 |          11.5148 |           4.1409 |
[32m[20230114 18:47:56 @agent_ppo2.py:189][0m |          -0.0053 |          11.6755 |           4.1435 |
[32m[20230114 18:47:56 @agent_ppo2.py:189][0m |          -0.0113 |          11.3529 |           4.1416 |
[32m[20230114 18:47:56 @agent_ppo2.py:189][0m |          -0.0103 |          11.2235 |           4.1437 |
[32m[20230114 18:47:56 @agent_ppo2.py:134][0m Policy update time: 0.84 s
[32m[20230114 18:47:56 @agent_ppo2.py:142][0m Average TRAINING episode reward: 259.70
[32m[20230114 18:47:56 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 260.64
[32m[20230114 18:47:56 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 274.67
[32m[20230114 18:47:56 @agent_ppo2.py:147][0m Total time:       5.86 min
[32m[20230114 18:47:56 @agent_ppo2.py:149][0m 460800 total steps have happened
[32m[20230114 18:47:56 @agent_ppo2.py:125][0m #------------------------ Iteration 225 --------------------------#
[32m[20230114 18:47:56 @agent_ppo2.py:131][0m Sampling time: 0.26 s by 4 slaves
[32m[20230114 18:47:56 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:47:56 @agent_ppo2.py:189][0m |          -0.0004 |          11.7306 |           4.1140 |
[32m[20230114 18:47:56 @agent_ppo2.py:189][0m |          -0.0061 |          10.2381 |           4.1101 |
[32m[20230114 18:47:56 @agent_ppo2.py:189][0m |          -0.0074 |           9.7525 |           4.1083 |
[32m[20230114 18:47:57 @agent_ppo2.py:189][0m |          -0.0106 |           9.2062 |           4.1073 |
[32m[20230114 18:47:57 @agent_ppo2.py:189][0m |          -0.0109 |           8.8845 |           4.1034 |
[32m[20230114 18:47:57 @agent_ppo2.py:189][0m |          -0.0112 |           8.5638 |           4.0990 |
[32m[20230114 18:47:57 @agent_ppo2.py:189][0m |          -0.0135 |           8.1396 |           4.1043 |
[32m[20230114 18:47:57 @agent_ppo2.py:189][0m |          -0.0143 |           7.8050 |           4.1001 |
[32m[20230114 18:47:57 @agent_ppo2.py:189][0m |          -0.0137 |           7.4800 |           4.1006 |
[32m[20230114 18:47:57 @agent_ppo2.py:189][0m |          -0.0141 |           7.2241 |           4.0990 |
[32m[20230114 18:47:57 @agent_ppo2.py:134][0m Policy update time: 0.89 s
[32m[20230114 18:47:57 @agent_ppo2.py:142][0m Average TRAINING episode reward: 257.17
[32m[20230114 18:47:57 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 259.34
[32m[20230114 18:47:57 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 274.96
[32m[20230114 18:47:57 @agent_ppo2.py:147][0m Total time:       5.88 min
[32m[20230114 18:47:57 @agent_ppo2.py:149][0m 462848 total steps have happened
[32m[20230114 18:47:57 @agent_ppo2.py:125][0m #------------------------ Iteration 226 --------------------------#
[32m[20230114 18:47:58 @agent_ppo2.py:131][0m Sampling time: 0.27 s by 4 slaves
[32m[20230114 18:47:58 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:47:58 @agent_ppo2.py:189][0m |          -0.0015 |          12.5658 |           4.1289 |
[32m[20230114 18:47:58 @agent_ppo2.py:189][0m |          -0.0027 |          11.8632 |           4.1296 |
[32m[20230114 18:47:58 @agent_ppo2.py:189][0m |          -0.0074 |          11.1169 |           4.1312 |
[32m[20230114 18:47:58 @agent_ppo2.py:189][0m |          -0.0077 |          10.5089 |           4.1319 |
[32m[20230114 18:47:58 @agent_ppo2.py:189][0m |          -0.0095 |           9.9016 |           4.1379 |
[32m[20230114 18:47:58 @agent_ppo2.py:189][0m |          -0.0109 |           9.2589 |           4.1378 |
[32m[20230114 18:47:58 @agent_ppo2.py:189][0m |          -0.0109 |           8.3684 |           4.1427 |
[32m[20230114 18:47:58 @agent_ppo2.py:189][0m |          -0.0118 |           7.7664 |           4.1411 |
[32m[20230114 18:47:58 @agent_ppo2.py:189][0m |          -0.0126 |           7.2925 |           4.1479 |
[32m[20230114 18:47:58 @agent_ppo2.py:189][0m |          -0.0115 |           6.8673 |           4.1489 |
[32m[20230114 18:47:58 @agent_ppo2.py:134][0m Policy update time: 0.88 s
[32m[20230114 18:47:59 @agent_ppo2.py:142][0m Average TRAINING episode reward: 257.69
[32m[20230114 18:47:59 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 260.02
[32m[20230114 18:47:59 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 56.64
[32m[20230114 18:47:59 @agent_ppo2.py:147][0m Total time:       5.91 min
[32m[20230114 18:47:59 @agent_ppo2.py:149][0m 464896 total steps have happened
[32m[20230114 18:47:59 @agent_ppo2.py:125][0m #------------------------ Iteration 227 --------------------------#
[32m[20230114 18:47:59 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:47:59 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:47:59 @agent_ppo2.py:189][0m |           0.0008 |          13.3585 |           4.2060 |
[32m[20230114 18:47:59 @agent_ppo2.py:189][0m |          -0.0005 |          12.4487 |           4.2082 |
[32m[20230114 18:47:59 @agent_ppo2.py:189][0m |          -0.0038 |          12.0412 |           4.2042 |
[32m[20230114 18:47:59 @agent_ppo2.py:189][0m |          -0.0047 |          11.8811 |           4.2094 |
[32m[20230114 18:47:59 @agent_ppo2.py:189][0m |          -0.0066 |          11.6094 |           4.2105 |
[32m[20230114 18:47:59 @agent_ppo2.py:189][0m |          -0.0073 |          11.4247 |           4.2094 |
[32m[20230114 18:47:59 @agent_ppo2.py:189][0m |          -0.0083 |          11.3076 |           4.2097 |
[32m[20230114 18:48:00 @agent_ppo2.py:189][0m |          -0.0078 |          11.2024 |           4.2131 |
[32m[20230114 18:48:00 @agent_ppo2.py:189][0m |          -0.0083 |          11.0143 |           4.2116 |
[32m[20230114 18:48:00 @agent_ppo2.py:189][0m |          -0.0089 |          10.8787 |           4.2120 |
[32m[20230114 18:48:00 @agent_ppo2.py:134][0m Policy update time: 0.86 s
[32m[20230114 18:48:00 @agent_ppo2.py:142][0m Average TRAINING episode reward: 260.76
[32m[20230114 18:48:00 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 263.29
[32m[20230114 18:48:00 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 273.85
[32m[20230114 18:48:00 @agent_ppo2.py:147][0m Total time:       5.93 min
[32m[20230114 18:48:00 @agent_ppo2.py:149][0m 466944 total steps have happened
[32m[20230114 18:48:00 @agent_ppo2.py:125][0m #------------------------ Iteration 228 --------------------------#
[32m[20230114 18:48:00 @agent_ppo2.py:131][0m Sampling time: 0.26 s by 4 slaves
[32m[20230114 18:48:00 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:48:00 @agent_ppo2.py:189][0m |           0.0032 |          11.6305 |           4.2092 |
[32m[20230114 18:48:01 @agent_ppo2.py:189][0m |          -0.0021 |          10.5473 |           4.2077 |
[32m[20230114 18:48:01 @agent_ppo2.py:189][0m |          -0.0043 |          10.1121 |           4.2074 |
[32m[20230114 18:48:01 @agent_ppo2.py:189][0m |          -0.0046 |           9.9494 |           4.2058 |
[32m[20230114 18:48:01 @agent_ppo2.py:189][0m |          -0.0072 |           9.6405 |           4.2059 |
[32m[20230114 18:48:01 @agent_ppo2.py:189][0m |          -0.0073 |           9.2958 |           4.2019 |
[32m[20230114 18:48:01 @agent_ppo2.py:189][0m |          -0.0078 |           9.0349 |           4.2037 |
[32m[20230114 18:48:01 @agent_ppo2.py:189][0m |          -0.0088 |           8.7408 |           4.2057 |
[32m[20230114 18:48:01 @agent_ppo2.py:189][0m |          -0.0093 |           8.5690 |           4.2012 |
[32m[20230114 18:48:01 @agent_ppo2.py:189][0m |          -0.0102 |           8.3043 |           4.2018 |
[32m[20230114 18:48:01 @agent_ppo2.py:134][0m Policy update time: 0.92 s
[32m[20230114 18:48:01 @agent_ppo2.py:142][0m Average TRAINING episode reward: 257.05
[32m[20230114 18:48:01 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 260.02
[32m[20230114 18:48:01 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 274.57
[32m[20230114 18:48:01 @agent_ppo2.py:147][0m Total time:       5.95 min
[32m[20230114 18:48:01 @agent_ppo2.py:149][0m 468992 total steps have happened
[32m[20230114 18:48:01 @agent_ppo2.py:125][0m #------------------------ Iteration 229 --------------------------#
[32m[20230114 18:48:02 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:48:02 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:48:02 @agent_ppo2.py:189][0m |          -0.0010 |          12.7644 |           4.1562 |
[32m[20230114 18:48:02 @agent_ppo2.py:189][0m |          -0.0058 |          11.2083 |           4.1538 |
[32m[20230114 18:48:02 @agent_ppo2.py:189][0m |          -0.0129 |          10.8291 |           4.1516 |
[32m[20230114 18:48:02 @agent_ppo2.py:189][0m |          -0.0110 |          10.6035 |           4.1483 |
[32m[20230114 18:48:02 @agent_ppo2.py:189][0m |          -0.0082 |          10.3373 |           4.1497 |
[32m[20230114 18:48:02 @agent_ppo2.py:189][0m |          -0.0032 |          10.1832 |           4.1500 |
[32m[20230114 18:48:02 @agent_ppo2.py:189][0m |          -0.0069 |          10.0752 |           4.1504 |
[32m[20230114 18:48:02 @agent_ppo2.py:189][0m |          -0.0078 |          10.0125 |           4.1459 |
[32m[20230114 18:48:03 @agent_ppo2.py:189][0m |          -0.0028 |           9.9040 |           4.1496 |
[32m[20230114 18:48:03 @agent_ppo2.py:189][0m |          -0.0151 |           9.8786 |           4.1509 |
[32m[20230114 18:48:03 @agent_ppo2.py:134][0m Policy update time: 0.94 s
[32m[20230114 18:48:03 @agent_ppo2.py:142][0m Average TRAINING episode reward: 258.13
[32m[20230114 18:48:03 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 260.40
[32m[20230114 18:48:03 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 274.47
[32m[20230114 18:48:03 @agent_ppo2.py:147][0m Total time:       5.98 min
[32m[20230114 18:48:03 @agent_ppo2.py:149][0m 471040 total steps have happened
[32m[20230114 18:48:03 @agent_ppo2.py:125][0m #------------------------ Iteration 230 --------------------------#
[32m[20230114 18:48:03 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:48:03 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:48:03 @agent_ppo2.py:189][0m |           0.0019 |          13.5341 |           4.2048 |
[32m[20230114 18:48:03 @agent_ppo2.py:189][0m |          -0.0062 |          12.2167 |           4.1959 |
[32m[20230114 18:48:03 @agent_ppo2.py:189][0m |          -0.0046 |          11.8693 |           4.1990 |
[32m[20230114 18:48:03 @agent_ppo2.py:189][0m |          -0.0091 |          11.3514 |           4.1946 |
[32m[20230114 18:48:04 @agent_ppo2.py:189][0m |          -0.0108 |          11.0793 |           4.1926 |
[32m[20230114 18:48:04 @agent_ppo2.py:189][0m |          -0.0103 |          10.8305 |           4.1918 |
[32m[20230114 18:48:04 @agent_ppo2.py:189][0m |          -0.0104 |          10.6642 |           4.1916 |
[32m[20230114 18:48:04 @agent_ppo2.py:189][0m |          -0.0079 |          10.7475 |           4.1923 |
[32m[20230114 18:48:04 @agent_ppo2.py:189][0m |          -0.0119 |          10.3677 |           4.1892 |
[32m[20230114 18:48:04 @agent_ppo2.py:189][0m |          -0.0122 |          10.1464 |           4.1909 |
[32m[20230114 18:48:04 @agent_ppo2.py:134][0m Policy update time: 0.90 s
[32m[20230114 18:48:04 @agent_ppo2.py:142][0m Average TRAINING episode reward: 260.21
[32m[20230114 18:48:04 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 261.14
[32m[20230114 18:48:04 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 274.06
[32m[20230114 18:48:04 @agent_ppo2.py:147][0m Total time:       6.00 min
[32m[20230114 18:48:04 @agent_ppo2.py:149][0m 473088 total steps have happened
[32m[20230114 18:48:04 @agent_ppo2.py:125][0m #------------------------ Iteration 231 --------------------------#
[32m[20230114 18:48:04 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:48:04 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:48:05 @agent_ppo2.py:189][0m |          -0.0011 |          11.6762 |           4.2250 |
[32m[20230114 18:48:05 @agent_ppo2.py:189][0m |          -0.0058 |          10.3033 |           4.2229 |
[32m[20230114 18:48:05 @agent_ppo2.py:189][0m |          -0.0075 |           9.6620 |           4.2238 |
[32m[20230114 18:48:05 @agent_ppo2.py:189][0m |          -0.0095 |           9.3152 |           4.2246 |
[32m[20230114 18:48:05 @agent_ppo2.py:189][0m |          -0.0106 |           9.0267 |           4.2240 |
[32m[20230114 18:48:05 @agent_ppo2.py:189][0m |          -0.0099 |           8.8629 |           4.2265 |
[32m[20230114 18:48:05 @agent_ppo2.py:189][0m |          -0.0124 |           8.6037 |           4.2263 |
[32m[20230114 18:48:05 @agent_ppo2.py:189][0m |          -0.0129 |           8.4152 |           4.2323 |
[32m[20230114 18:48:05 @agent_ppo2.py:189][0m |          -0.0121 |           8.2287 |           4.2285 |
[32m[20230114 18:48:05 @agent_ppo2.py:189][0m |          -0.0127 |           8.0146 |           4.2299 |
[32m[20230114 18:48:05 @agent_ppo2.py:134][0m Policy update time: 0.91 s
[32m[20230114 18:48:06 @agent_ppo2.py:142][0m Average TRAINING episode reward: 258.74
[32m[20230114 18:48:06 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 260.17
[32m[20230114 18:48:06 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 279.45
[32m[20230114 18:48:06 @agent_ppo2.py:147][0m Total time:       6.02 min
[32m[20230114 18:48:06 @agent_ppo2.py:149][0m 475136 total steps have happened
[32m[20230114 18:48:06 @agent_ppo2.py:125][0m #------------------------ Iteration 232 --------------------------#
[32m[20230114 18:48:06 @agent_ppo2.py:131][0m Sampling time: 0.30 s by 4 slaves
[32m[20230114 18:48:06 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:48:06 @agent_ppo2.py:189][0m |          -0.0003 |          13.2614 |           4.1976 |
[32m[20230114 18:48:06 @agent_ppo2.py:189][0m |           0.0030 |          12.6903 |           4.1905 |
[32m[20230114 18:48:06 @agent_ppo2.py:189][0m |          -0.0064 |          12.4562 |           4.1850 |
[32m[20230114 18:48:06 @agent_ppo2.py:189][0m |           0.0003 |          12.2726 |           4.1799 |
[32m[20230114 18:48:06 @agent_ppo2.py:189][0m |          -0.0200 |          12.2061 |           4.1744 |
[32m[20230114 18:48:06 @agent_ppo2.py:189][0m |          -0.0047 |          12.1374 |           4.1788 |
[32m[20230114 18:48:06 @agent_ppo2.py:189][0m |          -0.0082 |          12.0028 |           4.1785 |
[32m[20230114 18:48:07 @agent_ppo2.py:189][0m |          -0.0022 |          11.9522 |           4.1782 |
[32m[20230114 18:48:07 @agent_ppo2.py:189][0m |          -0.0104 |          11.9583 |           4.1748 |
[32m[20230114 18:48:07 @agent_ppo2.py:189][0m |          -0.0109 |          11.9210 |           4.1766 |
[32m[20230114 18:48:07 @agent_ppo2.py:134][0m Policy update time: 0.90 s
[32m[20230114 18:48:07 @agent_ppo2.py:142][0m Average TRAINING episode reward: 258.25
[32m[20230114 18:48:07 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 261.61
[32m[20230114 18:48:07 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 48.31
[32m[20230114 18:48:07 @agent_ppo2.py:147][0m Total time:       6.05 min
[32m[20230114 18:48:07 @agent_ppo2.py:149][0m 477184 total steps have happened
[32m[20230114 18:48:07 @agent_ppo2.py:125][0m #------------------------ Iteration 233 --------------------------#
[32m[20230114 18:48:07 @agent_ppo2.py:131][0m Sampling time: 0.25 s by 4 slaves
[32m[20230114 18:48:07 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:48:07 @agent_ppo2.py:189][0m |          -0.0017 |          11.8891 |           4.1160 |
[32m[20230114 18:48:07 @agent_ppo2.py:189][0m |          -0.0065 |          11.3055 |           4.1169 |
[32m[20230114 18:48:07 @agent_ppo2.py:189][0m |          -0.0095 |          10.9934 |           4.1141 |
[32m[20230114 18:48:08 @agent_ppo2.py:189][0m |          -0.0076 |          10.8024 |           4.1156 |
[32m[20230114 18:48:08 @agent_ppo2.py:189][0m |          -0.0074 |          10.6036 |           4.1158 |
[32m[20230114 18:48:08 @agent_ppo2.py:189][0m |          -0.0102 |          10.4433 |           4.1129 |
[32m[20230114 18:48:08 @agent_ppo2.py:189][0m |          -0.0089 |          10.2926 |           4.1129 |
[32m[20230114 18:48:08 @agent_ppo2.py:189][0m |          -0.0110 |          10.1736 |           4.1176 |
[32m[20230114 18:48:08 @agent_ppo2.py:189][0m |          -0.0133 |          10.0445 |           4.1167 |
[32m[20230114 18:48:08 @agent_ppo2.py:189][0m |          -0.0037 |          10.5772 |           4.1131 |
[32m[20230114 18:48:08 @agent_ppo2.py:134][0m Policy update time: 0.93 s
[32m[20230114 18:48:08 @agent_ppo2.py:142][0m Average TRAINING episode reward: 254.38
[32m[20230114 18:48:08 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 260.62
[32m[20230114 18:48:08 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 274.62
[32m[20230114 18:48:08 @agent_ppo2.py:147][0m Total time:       6.07 min
[32m[20230114 18:48:08 @agent_ppo2.py:149][0m 479232 total steps have happened
[32m[20230114 18:48:08 @agent_ppo2.py:125][0m #------------------------ Iteration 234 --------------------------#
[32m[20230114 18:48:08 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:48:09 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:48:09 @agent_ppo2.py:189][0m |           0.0014 |          11.7138 |           4.2642 |
[32m[20230114 18:48:09 @agent_ppo2.py:189][0m |          -0.0032 |          10.9578 |           4.2656 |
[32m[20230114 18:48:09 @agent_ppo2.py:189][0m |          -0.0045 |          10.6285 |           4.2653 |
[32m[20230114 18:48:09 @agent_ppo2.py:189][0m |          -0.0057 |          10.3728 |           4.2627 |
[32m[20230114 18:48:09 @agent_ppo2.py:189][0m |          -0.0071 |          10.2584 |           4.2663 |
[32m[20230114 18:48:09 @agent_ppo2.py:189][0m |          -0.0075 |          10.0506 |           4.2622 |
[32m[20230114 18:48:09 @agent_ppo2.py:189][0m |          -0.0089 |           9.9544 |           4.2613 |
[32m[20230114 18:48:09 @agent_ppo2.py:189][0m |          -0.0097 |           9.8083 |           4.2605 |
[32m[20230114 18:48:09 @agent_ppo2.py:189][0m |          -0.0099 |           9.6707 |           4.2627 |
[32m[20230114 18:48:09 @agent_ppo2.py:189][0m |          -0.0101 |           9.5687 |           4.2638 |
[32m[20230114 18:48:09 @agent_ppo2.py:134][0m Policy update time: 0.87 s
[32m[20230114 18:48:10 @agent_ppo2.py:142][0m Average TRAINING episode reward: 257.13
[32m[20230114 18:48:10 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 259.67
[32m[20230114 18:48:10 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 273.55
[32m[20230114 18:48:10 @agent_ppo2.py:147][0m Total time:       6.09 min
[32m[20230114 18:48:10 @agent_ppo2.py:149][0m 481280 total steps have happened
[32m[20230114 18:48:10 @agent_ppo2.py:125][0m #------------------------ Iteration 235 --------------------------#
[32m[20230114 18:48:10 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:48:10 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:48:10 @agent_ppo2.py:189][0m |          -0.0005 |          12.6017 |           4.1678 |
[32m[20230114 18:48:10 @agent_ppo2.py:189][0m |          -0.0024 |          11.9137 |           4.1630 |
[32m[20230114 18:48:10 @agent_ppo2.py:189][0m |          -0.0062 |          11.4818 |           4.1559 |
[32m[20230114 18:48:10 @agent_ppo2.py:189][0m |          -0.0066 |          11.1541 |           4.1508 |
[32m[20230114 18:48:10 @agent_ppo2.py:189][0m |          -0.0075 |          10.9174 |           4.1504 |
[32m[20230114 18:48:10 @agent_ppo2.py:189][0m |          -0.0072 |          10.6893 |           4.1480 |
[32m[20230114 18:48:10 @agent_ppo2.py:189][0m |          -0.0087 |          10.5640 |           4.1432 |
[32m[20230114 18:48:11 @agent_ppo2.py:189][0m |          -0.0090 |          10.4019 |           4.1415 |
[32m[20230114 18:48:11 @agent_ppo2.py:189][0m |          -0.0099 |          10.2422 |           4.1350 |
[32m[20230114 18:48:11 @agent_ppo2.py:189][0m |          -0.0098 |          10.0809 |           4.1369 |
[32m[20230114 18:48:11 @agent_ppo2.py:134][0m Policy update time: 0.93 s
[32m[20230114 18:48:11 @agent_ppo2.py:142][0m Average TRAINING episode reward: 257.98
[32m[20230114 18:48:11 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 260.64
[32m[20230114 18:48:11 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 276.22
[32m[20230114 18:48:11 @agent_ppo2.py:147][0m Total time:       6.11 min
[32m[20230114 18:48:11 @agent_ppo2.py:149][0m 483328 total steps have happened
[32m[20230114 18:48:11 @agent_ppo2.py:125][0m #------------------------ Iteration 236 --------------------------#
[32m[20230114 18:48:11 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:48:11 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:48:11 @agent_ppo2.py:189][0m |          -0.0005 |          10.9843 |           4.2427 |
[32m[20230114 18:48:11 @agent_ppo2.py:189][0m |          -0.0053 |           8.5952 |           4.2409 |
[32m[20230114 18:48:12 @agent_ppo2.py:189][0m |          -0.0072 |           7.4075 |           4.2409 |
[32m[20230114 18:48:12 @agent_ppo2.py:189][0m |          -0.0081 |           6.5185 |           4.2413 |
[32m[20230114 18:48:12 @agent_ppo2.py:189][0m |          -0.0104 |           5.7802 |           4.2393 |
[32m[20230114 18:48:12 @agent_ppo2.py:189][0m |          -0.0107 |           5.3340 |           4.2388 |
[32m[20230114 18:48:12 @agent_ppo2.py:189][0m |          -0.0118 |           4.8214 |           4.2371 |
[32m[20230114 18:48:12 @agent_ppo2.py:189][0m |          -0.0123 |           4.4933 |           4.2405 |
[32m[20230114 18:48:12 @agent_ppo2.py:189][0m |          -0.0121 |           4.2255 |           4.2417 |
[32m[20230114 18:48:12 @agent_ppo2.py:189][0m |          -0.0128 |           3.9454 |           4.2397 |
[32m[20230114 18:48:12 @agent_ppo2.py:134][0m Policy update time: 0.91 s
[32m[20230114 18:48:12 @agent_ppo2.py:142][0m Average TRAINING episode reward: 258.30
[32m[20230114 18:48:12 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 263.83
[32m[20230114 18:48:12 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 275.88
[32m[20230114 18:48:12 @agent_ppo2.py:147][0m Total time:       6.14 min
[32m[20230114 18:48:12 @agent_ppo2.py:149][0m 485376 total steps have happened
[32m[20230114 18:48:12 @agent_ppo2.py:125][0m #------------------------ Iteration 237 --------------------------#
[32m[20230114 18:48:13 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:48:13 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:48:13 @agent_ppo2.py:189][0m |           0.0012 |          12.2289 |           4.2410 |
[32m[20230114 18:48:13 @agent_ppo2.py:189][0m |          -0.0053 |           9.9582 |           4.2275 |
[32m[20230114 18:48:13 @agent_ppo2.py:189][0m |          -0.0014 |           9.8085 |           4.2308 |
[32m[20230114 18:48:13 @agent_ppo2.py:189][0m |          -0.0053 |           8.7713 |           4.2201 |
[32m[20230114 18:48:13 @agent_ppo2.py:189][0m |          -0.0098 |           8.5233 |           4.2293 |
[32m[20230114 18:48:13 @agent_ppo2.py:189][0m |          -0.0102 |           8.1670 |           4.2275 |
[32m[20230114 18:48:13 @agent_ppo2.py:189][0m |          -0.0123 |           7.9252 |           4.2243 |
[32m[20230114 18:48:13 @agent_ppo2.py:189][0m |          -0.0119 |           7.7219 |           4.2271 |
[32m[20230114 18:48:13 @agent_ppo2.py:189][0m |          -0.0109 |           7.4820 |           4.2238 |
[32m[20230114 18:48:13 @agent_ppo2.py:189][0m |          -0.0120 |           7.4141 |           4.2254 |
[32m[20230114 18:48:13 @agent_ppo2.py:134][0m Policy update time: 0.92 s
[32m[20230114 18:48:14 @agent_ppo2.py:142][0m Average TRAINING episode reward: 255.34
[32m[20230114 18:48:14 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 260.32
[32m[20230114 18:48:14 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 275.64
[32m[20230114 18:48:14 @agent_ppo2.py:147][0m Total time:       6.16 min
[32m[20230114 18:48:14 @agent_ppo2.py:149][0m 487424 total steps have happened
[32m[20230114 18:48:14 @agent_ppo2.py:125][0m #------------------------ Iteration 238 --------------------------#
[32m[20230114 18:48:14 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:48:14 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:48:14 @agent_ppo2.py:189][0m |           0.0005 |          15.0257 |           4.1835 |
[32m[20230114 18:48:14 @agent_ppo2.py:189][0m |          -0.0019 |          12.3075 |           4.1804 |
[32m[20230114 18:48:14 @agent_ppo2.py:189][0m |          -0.0069 |          11.2568 |           4.1815 |
[32m[20230114 18:48:14 @agent_ppo2.py:189][0m |          -0.0081 |          10.7600 |           4.1783 |
[32m[20230114 18:48:14 @agent_ppo2.py:189][0m |          -0.0091 |          10.3602 |           4.1809 |
[32m[20230114 18:48:15 @agent_ppo2.py:189][0m |          -0.0000 |          10.2896 |           4.1810 |
[32m[20230114 18:48:15 @agent_ppo2.py:189][0m |          -0.0099 |           9.8315 |           4.1776 |
[32m[20230114 18:48:15 @agent_ppo2.py:189][0m |          -0.0104 |           9.6518 |           4.1795 |
[32m[20230114 18:48:15 @agent_ppo2.py:189][0m |          -0.0060 |           9.6446 |           4.1785 |
[32m[20230114 18:48:15 @agent_ppo2.py:189][0m |          -0.0110 |           9.3105 |           4.1805 |
[32m[20230114 18:48:15 @agent_ppo2.py:134][0m Policy update time: 0.87 s
[32m[20230114 18:48:15 @agent_ppo2.py:142][0m Average TRAINING episode reward: 258.14
[32m[20230114 18:48:15 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 260.08
[32m[20230114 18:48:15 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 274.35
[32m[20230114 18:48:15 @agent_ppo2.py:147][0m Total time:       6.18 min
[32m[20230114 18:48:15 @agent_ppo2.py:149][0m 489472 total steps have happened
[32m[20230114 18:48:15 @agent_ppo2.py:125][0m #------------------------ Iteration 239 --------------------------#
[32m[20230114 18:48:15 @agent_ppo2.py:131][0m Sampling time: 0.25 s by 4 slaves
[32m[20230114 18:48:15 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:48:15 @agent_ppo2.py:189][0m |           0.0010 |          12.8187 |           4.1966 |
[32m[20230114 18:48:16 @agent_ppo2.py:189][0m |          -0.0041 |          11.3486 |           4.1857 |
[32m[20230114 18:48:16 @agent_ppo2.py:189][0m |          -0.0059 |          10.7837 |           4.1906 |
[32m[20230114 18:48:16 @agent_ppo2.py:189][0m |          -0.0076 |          10.4324 |           4.1794 |
[32m[20230114 18:48:16 @agent_ppo2.py:189][0m |          -0.0085 |          10.2148 |           4.1807 |
[32m[20230114 18:48:16 @agent_ppo2.py:189][0m |          -0.0097 |           9.9639 |           4.1794 |
[32m[20230114 18:48:16 @agent_ppo2.py:189][0m |          -0.0099 |           9.7716 |           4.1764 |
[32m[20230114 18:48:16 @agent_ppo2.py:189][0m |          -0.0107 |           9.6099 |           4.1762 |
[32m[20230114 18:48:16 @agent_ppo2.py:189][0m |          -0.0107 |           9.5035 |           4.1733 |
[32m[20230114 18:48:16 @agent_ppo2.py:189][0m |          -0.0108 |           9.3351 |           4.1766 |
[32m[20230114 18:48:16 @agent_ppo2.py:134][0m Policy update time: 0.87 s
[32m[20230114 18:48:16 @agent_ppo2.py:142][0m Average TRAINING episode reward: 257.24
[32m[20230114 18:48:16 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 261.41
[32m[20230114 18:48:16 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 98.76
[32m[20230114 18:48:16 @agent_ppo2.py:147][0m Total time:       6.20 min
[32m[20230114 18:48:16 @agent_ppo2.py:149][0m 491520 total steps have happened
[32m[20230114 18:48:16 @agent_ppo2.py:125][0m #------------------------ Iteration 240 --------------------------#
[32m[20230114 18:48:17 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:48:17 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:48:17 @agent_ppo2.py:189][0m |           0.0007 |          13.9314 |           4.2029 |
[32m[20230114 18:48:17 @agent_ppo2.py:189][0m |          -0.0028 |          12.5892 |           4.2022 |
[32m[20230114 18:48:17 @agent_ppo2.py:189][0m |          -0.0045 |          12.2216 |           4.2040 |
[32m[20230114 18:48:17 @agent_ppo2.py:189][0m |          -0.0058 |          11.8217 |           4.1999 |
[32m[20230114 18:48:17 @agent_ppo2.py:189][0m |          -0.0064 |          11.3784 |           4.2006 |
[32m[20230114 18:48:17 @agent_ppo2.py:189][0m |          -0.0077 |          10.9677 |           4.1970 |
[32m[20230114 18:48:17 @agent_ppo2.py:189][0m |          -0.0089 |          10.5714 |           4.1969 |
[32m[20230114 18:48:17 @agent_ppo2.py:189][0m |          -0.0088 |          10.1993 |           4.1980 |
[32m[20230114 18:48:17 @agent_ppo2.py:189][0m |          -0.0092 |           9.9407 |           4.1998 |
[32m[20230114 18:48:17 @agent_ppo2.py:189][0m |          -0.0104 |           9.7277 |           4.1977 |
[32m[20230114 18:48:17 @agent_ppo2.py:134][0m Policy update time: 0.88 s
[32m[20230114 18:48:18 @agent_ppo2.py:142][0m Average TRAINING episode reward: 257.29
[32m[20230114 18:48:18 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 260.64
[32m[20230114 18:48:18 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 273.54
[32m[20230114 18:48:18 @agent_ppo2.py:147][0m Total time:       6.23 min
[32m[20230114 18:48:18 @agent_ppo2.py:149][0m 493568 total steps have happened
[32m[20230114 18:48:18 @agent_ppo2.py:125][0m #------------------------ Iteration 241 --------------------------#
[32m[20230114 18:48:18 @agent_ppo2.py:131][0m Sampling time: 0.27 s by 4 slaves
[32m[20230114 18:48:18 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:48:18 @agent_ppo2.py:189][0m |          -0.0030 |          12.3314 |           4.1201 |
[32m[20230114 18:48:18 @agent_ppo2.py:189][0m |          -0.0080 |          10.4454 |           4.1100 |
[32m[20230114 18:48:18 @agent_ppo2.py:189][0m |          -0.0083 |           9.8688 |           4.1093 |
[32m[20230114 18:48:18 @agent_ppo2.py:189][0m |          -0.0132 |           9.2036 |           4.1045 |
[32m[20230114 18:48:18 @agent_ppo2.py:189][0m |          -0.0097 |           9.1035 |           4.1025 |
[32m[20230114 18:48:19 @agent_ppo2.py:189][0m |          -0.0140 |           8.5132 |           4.0973 |
[32m[20230114 18:48:19 @agent_ppo2.py:189][0m |          -0.0125 |           8.4022 |           4.1007 |
[32m[20230114 18:48:19 @agent_ppo2.py:189][0m |          -0.0121 |           8.1564 |           4.1000 |
[32m[20230114 18:48:19 @agent_ppo2.py:189][0m |          -0.0131 |           8.0555 |           4.0988 |
[32m[20230114 18:48:19 @agent_ppo2.py:189][0m |          -0.0132 |           7.8338 |           4.0976 |
[32m[20230114 18:48:19 @agent_ppo2.py:134][0m Policy update time: 0.93 s
[32m[20230114 18:48:19 @agent_ppo2.py:142][0m Average TRAINING episode reward: 256.57
[32m[20230114 18:48:19 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 258.65
[32m[20230114 18:48:19 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 269.15
[32m[20230114 18:48:19 @agent_ppo2.py:147][0m Total time:       6.25 min
[32m[20230114 18:48:19 @agent_ppo2.py:149][0m 495616 total steps have happened
[32m[20230114 18:48:19 @agent_ppo2.py:125][0m #------------------------ Iteration 242 --------------------------#
[32m[20230114 18:48:19 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:48:19 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:48:19 @agent_ppo2.py:189][0m |           0.0057 |          16.3724 |           4.1945 |
[32m[20230114 18:48:20 @agent_ppo2.py:189][0m |          -0.0075 |          10.3304 |           4.1811 |
[32m[20230114 18:48:20 @agent_ppo2.py:189][0m |          -0.0070 |           9.4735 |           4.1820 |
[32m[20230114 18:48:20 @agent_ppo2.py:189][0m |          -0.0109 |           8.6829 |           4.1791 |
[32m[20230114 18:48:20 @agent_ppo2.py:189][0m |          -0.0062 |           8.3061 |           4.1783 |
[32m[20230114 18:48:20 @agent_ppo2.py:189][0m |          -0.0119 |           7.9753 |           4.1754 |
[32m[20230114 18:48:20 @agent_ppo2.py:189][0m |          -0.0139 |           7.6622 |           4.1755 |
[32m[20230114 18:48:20 @agent_ppo2.py:189][0m |          -0.0128 |           7.4196 |           4.1742 |
[32m[20230114 18:48:20 @agent_ppo2.py:189][0m |          -0.0105 |           7.4272 |           4.1765 |
[32m[20230114 18:48:20 @agent_ppo2.py:189][0m |          -0.0126 |           7.0766 |           4.1732 |
[32m[20230114 18:48:20 @agent_ppo2.py:134][0m Policy update time: 0.83 s
[32m[20230114 18:48:20 @agent_ppo2.py:142][0m Average TRAINING episode reward: 215.18
[32m[20230114 18:48:20 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 262.32
[32m[20230114 18:48:20 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 274.64
[32m[20230114 18:48:20 @agent_ppo2.py:147][0m Total time:       6.27 min
[32m[20230114 18:48:20 @agent_ppo2.py:149][0m 497664 total steps have happened
[32m[20230114 18:48:20 @agent_ppo2.py:125][0m #------------------------ Iteration 243 --------------------------#
[32m[20230114 18:48:21 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:48:21 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:48:21 @agent_ppo2.py:189][0m |          -0.0008 |          15.1910 |           4.1870 |
[32m[20230114 18:48:21 @agent_ppo2.py:189][0m |          -0.0047 |          13.8413 |           4.1786 |
[32m[20230114 18:48:21 @agent_ppo2.py:189][0m |          -0.0061 |          13.2165 |           4.1825 |
[32m[20230114 18:48:21 @agent_ppo2.py:189][0m |          -0.0086 |          12.7660 |           4.1764 |
[32m[20230114 18:48:21 @agent_ppo2.py:189][0m |          -0.0084 |          12.6329 |           4.1798 |
[32m[20230114 18:48:21 @agent_ppo2.py:189][0m |          -0.0094 |          12.2278 |           4.1798 |
[32m[20230114 18:48:21 @agent_ppo2.py:189][0m |          -0.0107 |          11.9755 |           4.1816 |
[32m[20230114 18:48:21 @agent_ppo2.py:189][0m |          -0.0119 |          11.7680 |           4.1809 |
[32m[20230114 18:48:21 @agent_ppo2.py:189][0m |          -0.0113 |          11.7068 |           4.1808 |
[32m[20230114 18:48:21 @agent_ppo2.py:189][0m |          -0.0106 |          11.4783 |           4.1812 |
[32m[20230114 18:48:21 @agent_ppo2.py:134][0m Policy update time: 0.89 s
[32m[20230114 18:48:22 @agent_ppo2.py:142][0m Average TRAINING episode reward: 254.39
[32m[20230114 18:48:22 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 258.07
[32m[20230114 18:48:22 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 269.75
[32m[20230114 18:48:22 @agent_ppo2.py:147][0m Total time:       6.29 min
[32m[20230114 18:48:22 @agent_ppo2.py:149][0m 499712 total steps have happened
[32m[20230114 18:48:22 @agent_ppo2.py:125][0m #------------------------ Iteration 244 --------------------------#
[32m[20230114 18:48:22 @agent_ppo2.py:131][0m Sampling time: 0.26 s by 4 slaves
[32m[20230114 18:48:22 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:48:22 @agent_ppo2.py:189][0m |           0.0213 |          13.8024 |           4.1597 |
[32m[20230114 18:48:22 @agent_ppo2.py:189][0m |          -0.0032 |          11.5813 |           4.1515 |
[32m[20230114 18:48:22 @agent_ppo2.py:189][0m |           0.0147 |          12.8503 |           4.1494 |
[32m[20230114 18:48:22 @agent_ppo2.py:189][0m |          -0.0088 |          10.8226 |           4.1447 |
[32m[20230114 18:48:22 @agent_ppo2.py:189][0m |          -0.0115 |          10.4134 |           4.1462 |
[32m[20230114 18:48:23 @agent_ppo2.py:189][0m |          -0.0121 |          10.2354 |           4.1434 |
[32m[20230114 18:48:23 @agent_ppo2.py:189][0m |          -0.0081 |          10.1956 |           4.1456 |
[32m[20230114 18:48:23 @agent_ppo2.py:189][0m |          -0.0139 |          10.0579 |           4.1422 |
[32m[20230114 18:48:23 @agent_ppo2.py:189][0m |           0.0205 |          12.5695 |           4.1404 |
[32m[20230114 18:48:23 @agent_ppo2.py:189][0m |          -0.0101 |           9.9124 |           4.1356 |
[32m[20230114 18:48:23 @agent_ppo2.py:134][0m Policy update time: 0.95 s
[32m[20230114 18:48:23 @agent_ppo2.py:142][0m Average TRAINING episode reward: 258.30
[32m[20230114 18:48:23 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 260.01
[32m[20230114 18:48:23 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 272.25
[32m[20230114 18:48:23 @agent_ppo2.py:147][0m Total time:       6.32 min
[32m[20230114 18:48:23 @agent_ppo2.py:149][0m 501760 total steps have happened
[32m[20230114 18:48:23 @agent_ppo2.py:125][0m #------------------------ Iteration 245 --------------------------#
[32m[20230114 18:48:23 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:48:23 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:48:24 @agent_ppo2.py:189][0m |          -0.0052 |          11.9228 |           4.1500 |
[32m[20230114 18:48:24 @agent_ppo2.py:189][0m |          -0.0060 |          10.7264 |           4.1493 |
[32m[20230114 18:48:24 @agent_ppo2.py:189][0m |          -0.0061 |          10.1330 |           4.1457 |
[32m[20230114 18:48:24 @agent_ppo2.py:189][0m |          -0.0104 |           9.5879 |           4.1454 |
[32m[20230114 18:48:24 @agent_ppo2.py:189][0m |          -0.0091 |           9.0725 |           4.1436 |
[32m[20230114 18:48:24 @agent_ppo2.py:189][0m |          -0.0113 |           8.8207 |           4.1433 |
[32m[20230114 18:48:24 @agent_ppo2.py:189][0m |          -0.0050 |           8.6308 |           4.1423 |
[32m[20230114 18:48:24 @agent_ppo2.py:189][0m |          -0.0134 |           8.4623 |           4.1419 |
[32m[20230114 18:48:24 @agent_ppo2.py:189][0m |          -0.0093 |           8.2877 |           4.1435 |
[32m[20230114 18:48:24 @agent_ppo2.py:189][0m |          -0.0080 |           8.2659 |           4.1412 |
[32m[20230114 18:48:24 @agent_ppo2.py:134][0m Policy update time: 0.89 s
[32m[20230114 18:48:24 @agent_ppo2.py:142][0m Average TRAINING episode reward: 256.06
[32m[20230114 18:48:24 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 258.09
[32m[20230114 18:48:24 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 269.08
[32m[20230114 18:48:24 @agent_ppo2.py:147][0m Total time:       6.34 min
[32m[20230114 18:48:24 @agent_ppo2.py:149][0m 503808 total steps have happened
[32m[20230114 18:48:24 @agent_ppo2.py:125][0m #------------------------ Iteration 246 --------------------------#
[32m[20230114 18:48:25 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:48:25 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:48:25 @agent_ppo2.py:189][0m |           0.0012 |          13.6040 |           4.2215 |
[32m[20230114 18:48:25 @agent_ppo2.py:189][0m |          -0.0054 |          12.3782 |           4.2120 |
[32m[20230114 18:48:25 @agent_ppo2.py:189][0m |          -0.0075 |          11.9438 |           4.2160 |
[32m[20230114 18:48:25 @agent_ppo2.py:189][0m |          -0.0070 |          11.7395 |           4.2161 |
[32m[20230114 18:48:25 @agent_ppo2.py:189][0m |          -0.0091 |          11.6330 |           4.2177 |
[32m[20230114 18:48:25 @agent_ppo2.py:189][0m |          -0.0084 |          11.4698 |           4.2209 |
[32m[20230114 18:48:25 @agent_ppo2.py:189][0m |          -0.0096 |          11.2663 |           4.2180 |
[32m[20230114 18:48:25 @agent_ppo2.py:189][0m |          -0.0102 |          11.1253 |           4.2201 |
[32m[20230114 18:48:25 @agent_ppo2.py:189][0m |          -0.0102 |          11.1126 |           4.2201 |
[32m[20230114 18:48:26 @agent_ppo2.py:189][0m |          -0.0118 |          10.8374 |           4.2195 |
[32m[20230114 18:48:26 @agent_ppo2.py:134][0m Policy update time: 0.85 s
[32m[20230114 18:48:26 @agent_ppo2.py:142][0m Average TRAINING episode reward: 257.00
[32m[20230114 18:48:26 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 257.70
[32m[20230114 18:48:26 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 267.71
[32m[20230114 18:48:26 @agent_ppo2.py:147][0m Total time:       6.36 min
[32m[20230114 18:48:26 @agent_ppo2.py:149][0m 505856 total steps have happened
[32m[20230114 18:48:26 @agent_ppo2.py:125][0m #------------------------ Iteration 247 --------------------------#
[32m[20230114 18:48:26 @agent_ppo2.py:131][0m Sampling time: 0.32 s by 4 slaves
[32m[20230114 18:48:26 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:48:26 @agent_ppo2.py:189][0m |           0.0001 |          12.5149 |           4.1799 |
[32m[20230114 18:48:26 @agent_ppo2.py:189][0m |          -0.0032 |          11.4218 |           4.1705 |
[32m[20230114 18:48:26 @agent_ppo2.py:189][0m |          -0.0047 |          10.9500 |           4.1687 |
[32m[20230114 18:48:26 @agent_ppo2.py:189][0m |          -0.0074 |          10.6054 |           4.1602 |
[32m[20230114 18:48:27 @agent_ppo2.py:189][0m |          -0.0068 |          10.3624 |           4.1674 |
[32m[20230114 18:48:27 @agent_ppo2.py:189][0m |          -0.0080 |          10.1480 |           4.1591 |
[32m[20230114 18:48:27 @agent_ppo2.py:189][0m |          -0.0060 |           9.9695 |           4.1596 |
[32m[20230114 18:48:27 @agent_ppo2.py:189][0m |          -0.0083 |           9.7172 |           4.1599 |
[32m[20230114 18:48:27 @agent_ppo2.py:189][0m |          -0.0097 |           9.4793 |           4.1576 |
[32m[20230114 18:48:27 @agent_ppo2.py:189][0m |          -0.0076 |           9.3302 |           4.1576 |
[32m[20230114 18:48:27 @agent_ppo2.py:134][0m Policy update time: 0.89 s
[32m[20230114 18:48:27 @agent_ppo2.py:142][0m Average TRAINING episode reward: 253.12
[32m[20230114 18:48:27 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 255.15
[32m[20230114 18:48:27 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 269.77
[32m[20230114 18:48:27 @agent_ppo2.py:147][0m Total time:       6.38 min
[32m[20230114 18:48:27 @agent_ppo2.py:149][0m 507904 total steps have happened
[32m[20230114 18:48:27 @agent_ppo2.py:125][0m #------------------------ Iteration 248 --------------------------#
[32m[20230114 18:48:27 @agent_ppo2.py:131][0m Sampling time: 0.25 s by 4 slaves
[32m[20230114 18:48:28 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:48:28 @agent_ppo2.py:189][0m |          -0.0036 |          13.4967 |           4.1584 |
[32m[20230114 18:48:28 @agent_ppo2.py:189][0m |          -0.0083 |          11.7891 |           4.1567 |
[32m[20230114 18:48:28 @agent_ppo2.py:189][0m |          -0.0070 |          11.3088 |           4.1540 |
[32m[20230114 18:48:28 @agent_ppo2.py:189][0m |          -0.0101 |          10.9458 |           4.1561 |
[32m[20230114 18:48:28 @agent_ppo2.py:189][0m |          -0.0120 |          10.7096 |           4.1548 |
[32m[20230114 18:48:28 @agent_ppo2.py:189][0m |          -0.0037 |          11.0540 |           4.1569 |
[32m[20230114 18:48:28 @agent_ppo2.py:189][0m |          -0.0070 |          10.3228 |           4.1525 |
[32m[20230114 18:48:28 @agent_ppo2.py:189][0m |          -0.0183 |          10.0412 |           4.1541 |
[32m[20230114 18:48:28 @agent_ppo2.py:189][0m |          -0.0089 |           9.8171 |           4.1571 |
[32m[20230114 18:48:28 @agent_ppo2.py:189][0m |          -0.0103 |           9.5856 |           4.1573 |
[32m[20230114 18:48:28 @agent_ppo2.py:134][0m Policy update time: 0.85 s
[32m[20230114 18:48:29 @agent_ppo2.py:142][0m Average TRAINING episode reward: 255.06
[32m[20230114 18:48:29 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 258.28
[32m[20230114 18:48:29 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 269.97
[32m[20230114 18:48:29 @agent_ppo2.py:147][0m Total time:       6.41 min
[32m[20230114 18:48:29 @agent_ppo2.py:149][0m 509952 total steps have happened
[32m[20230114 18:48:29 @agent_ppo2.py:125][0m #------------------------ Iteration 249 --------------------------#
[32m[20230114 18:48:29 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:48:29 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:48:29 @agent_ppo2.py:189][0m |           0.0003 |          12.1615 |           4.1908 |
[32m[20230114 18:48:29 @agent_ppo2.py:189][0m |          -0.0056 |          10.3717 |           4.1877 |
[32m[20230114 18:48:29 @agent_ppo2.py:189][0m |          -0.0076 |           9.5920 |           4.1838 |
[32m[20230114 18:48:29 @agent_ppo2.py:189][0m |          -0.0090 |           8.9771 |           4.1853 |
[32m[20230114 18:48:29 @agent_ppo2.py:189][0m |          -0.0095 |           8.7144 |           4.1827 |
[32m[20230114 18:48:29 @agent_ppo2.py:189][0m |          -0.0104 |           8.2915 |           4.1829 |
[32m[20230114 18:48:29 @agent_ppo2.py:189][0m |          -0.0111 |           7.9735 |           4.1853 |
[32m[20230114 18:48:29 @agent_ppo2.py:189][0m |          -0.0119 |           7.7000 |           4.1816 |
[32m[20230114 18:48:30 @agent_ppo2.py:189][0m |          -0.0107 |           7.4759 |           4.1852 |
[32m[20230114 18:48:30 @agent_ppo2.py:189][0m |          -0.0141 |           7.2510 |           4.1821 |
[32m[20230114 18:48:30 @agent_ppo2.py:134][0m Policy update time: 0.86 s
[32m[20230114 18:48:30 @agent_ppo2.py:142][0m Average TRAINING episode reward: 252.26
[32m[20230114 18:48:30 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 254.75
[32m[20230114 18:48:30 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 272.85
[32m[20230114 18:48:30 @agent_ppo2.py:147][0m Total time:       6.43 min
[32m[20230114 18:48:30 @agent_ppo2.py:149][0m 512000 total steps have happened
[32m[20230114 18:48:30 @agent_ppo2.py:125][0m #------------------------ Iteration 250 --------------------------#
[32m[20230114 18:48:30 @agent_ppo2.py:131][0m Sampling time: 0.31 s by 4 slaves
[32m[20230114 18:48:30 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:48:30 @agent_ppo2.py:189][0m |           0.0030 |          16.5090 |           4.2539 |
[32m[20230114 18:48:30 @agent_ppo2.py:189][0m |          -0.0066 |          11.9319 |           4.2517 |
[32m[20230114 18:48:30 @agent_ppo2.py:189][0m |          -0.0080 |          11.0978 |           4.2484 |
[32m[20230114 18:48:31 @agent_ppo2.py:189][0m |          -0.0097 |          10.5945 |           4.2452 |
[32m[20230114 18:48:31 @agent_ppo2.py:189][0m |          -0.0115 |          10.2303 |           4.2407 |
[32m[20230114 18:48:31 @agent_ppo2.py:189][0m |          -0.0107 |           9.8642 |           4.2371 |
[32m[20230114 18:48:31 @agent_ppo2.py:189][0m |          -0.0087 |           9.5914 |           4.2356 |
[32m[20230114 18:48:31 @agent_ppo2.py:189][0m |          -0.0108 |           9.3070 |           4.2359 |
[32m[20230114 18:48:31 @agent_ppo2.py:189][0m |          -0.0127 |           9.1802 |           4.2322 |
[32m[20230114 18:48:31 @agent_ppo2.py:189][0m |          -0.0141 |           8.9635 |           4.2301 |
[32m[20230114 18:48:31 @agent_ppo2.py:134][0m Policy update time: 0.92 s
[32m[20230114 18:48:31 @agent_ppo2.py:142][0m Average TRAINING episode reward: 196.60
[32m[20230114 18:48:31 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 257.84
[32m[20230114 18:48:31 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 273.88
[32m[20230114 18:48:31 @agent_ppo2.py:147][0m Total time:       6.45 min
[32m[20230114 18:48:31 @agent_ppo2.py:149][0m 514048 total steps have happened
[32m[20230114 18:48:31 @agent_ppo2.py:125][0m #------------------------ Iteration 251 --------------------------#
[32m[20230114 18:48:31 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:48:32 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:48:32 @agent_ppo2.py:189][0m |           0.0026 |          13.5437 |           4.2031 |
[32m[20230114 18:48:32 @agent_ppo2.py:189][0m |          -0.0008 |          12.4517 |           4.2006 |
[32m[20230114 18:48:32 @agent_ppo2.py:189][0m |          -0.0028 |          12.1235 |           4.1985 |
[32m[20230114 18:48:32 @agent_ppo2.py:189][0m |          -0.0039 |          11.9098 |           4.1986 |
[32m[20230114 18:48:32 @agent_ppo2.py:189][0m |          -0.0039 |          11.7116 |           4.2042 |
[32m[20230114 18:48:32 @agent_ppo2.py:189][0m |          -0.0056 |          11.5957 |           4.1982 |
[32m[20230114 18:48:32 @agent_ppo2.py:189][0m |          -0.0056 |          11.5236 |           4.2014 |
[32m[20230114 18:48:32 @agent_ppo2.py:189][0m |          -0.0072 |          11.4276 |           4.1997 |
[32m[20230114 18:48:32 @agent_ppo2.py:189][0m |          -0.0066 |          11.3524 |           4.1991 |
[32m[20230114 18:48:32 @agent_ppo2.py:189][0m |          -0.0076 |          11.2807 |           4.2083 |
[32m[20230114 18:48:32 @agent_ppo2.py:134][0m Policy update time: 0.83 s
[32m[20230114 18:48:33 @agent_ppo2.py:142][0m Average TRAINING episode reward: 256.49
[32m[20230114 18:48:33 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 258.57
[32m[20230114 18:48:33 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 272.76
[32m[20230114 18:48:33 @agent_ppo2.py:147][0m Total time:       6.47 min
[32m[20230114 18:48:33 @agent_ppo2.py:149][0m 516096 total steps have happened
[32m[20230114 18:48:33 @agent_ppo2.py:125][0m #------------------------ Iteration 252 --------------------------#
[32m[20230114 18:48:33 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:48:33 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:48:33 @agent_ppo2.py:189][0m |          -0.0001 |           9.8040 |           4.1680 |
[32m[20230114 18:48:33 @agent_ppo2.py:189][0m |          -0.0054 |           8.0385 |           4.1614 |
[32m[20230114 18:48:33 @agent_ppo2.py:189][0m |          -0.0050 |           7.3632 |           4.1609 |
[32m[20230114 18:48:33 @agent_ppo2.py:189][0m |          -0.0079 |           6.7512 |           4.1566 |
[32m[20230114 18:48:33 @agent_ppo2.py:189][0m |          -0.0093 |           6.4641 |           4.1566 |
[32m[20230114 18:48:33 @agent_ppo2.py:189][0m |          -0.0103 |           6.1805 |           4.1574 |
[32m[20230114 18:48:33 @agent_ppo2.py:189][0m |          -0.0109 |           5.9510 |           4.1542 |
[32m[20230114 18:48:33 @agent_ppo2.py:189][0m |          -0.0114 |           5.6975 |           4.1537 |
[32m[20230114 18:48:34 @agent_ppo2.py:189][0m |          -0.0122 |           5.5204 |           4.1544 |
[32m[20230114 18:48:34 @agent_ppo2.py:189][0m |          -0.0113 |           5.3596 |           4.1560 |
[32m[20230114 18:48:34 @agent_ppo2.py:134][0m Policy update time: 0.86 s
[32m[20230114 18:48:34 @agent_ppo2.py:142][0m Average TRAINING episode reward: 254.79
[32m[20230114 18:48:34 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 258.94
[32m[20230114 18:48:34 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 269.91
[32m[20230114 18:48:34 @agent_ppo2.py:147][0m Total time:       6.49 min
[32m[20230114 18:48:34 @agent_ppo2.py:149][0m 518144 total steps have happened
[32m[20230114 18:48:34 @agent_ppo2.py:125][0m #------------------------ Iteration 253 --------------------------#
[32m[20230114 18:48:34 @agent_ppo2.py:131][0m Sampling time: 0.28 s by 4 slaves
[32m[20230114 18:48:34 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:48:34 @agent_ppo2.py:189][0m |          -0.0017 |          13.2323 |           4.2668 |
[32m[20230114 18:48:34 @agent_ppo2.py:189][0m |          -0.0068 |          12.2932 |           4.2646 |
[32m[20230114 18:48:34 @agent_ppo2.py:189][0m |          -0.0075 |          11.9661 |           4.2627 |
[32m[20230114 18:48:35 @agent_ppo2.py:189][0m |          -0.0093 |          11.6749 |           4.2674 |
[32m[20230114 18:48:35 @agent_ppo2.py:189][0m |          -0.0110 |          11.4558 |           4.2663 |
[32m[20230114 18:48:35 @agent_ppo2.py:189][0m |          -0.0116 |          11.2421 |           4.2676 |
[32m[20230114 18:48:35 @agent_ppo2.py:189][0m |          -0.0113 |          11.0050 |           4.2684 |
[32m[20230114 18:48:35 @agent_ppo2.py:189][0m |          -0.0130 |          10.8145 |           4.2682 |
[32m[20230114 18:48:35 @agent_ppo2.py:189][0m |          -0.0126 |          10.6236 |           4.2692 |
[32m[20230114 18:48:35 @agent_ppo2.py:189][0m |          -0.0131 |          10.3545 |           4.2690 |
[32m[20230114 18:48:35 @agent_ppo2.py:134][0m Policy update time: 0.88 s
[32m[20230114 18:48:35 @agent_ppo2.py:142][0m Average TRAINING episode reward: 253.70
[32m[20230114 18:48:35 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 259.38
[32m[20230114 18:48:35 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 272.55
[32m[20230114 18:48:35 @agent_ppo2.py:147][0m Total time:       6.52 min
[32m[20230114 18:48:35 @agent_ppo2.py:149][0m 520192 total steps have happened
[32m[20230114 18:48:35 @agent_ppo2.py:125][0m #------------------------ Iteration 254 --------------------------#
[32m[20230114 18:48:35 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:48:36 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:48:36 @agent_ppo2.py:189][0m |           0.0007 |          13.1270 |           4.2386 |
[32m[20230114 18:48:36 @agent_ppo2.py:189][0m |          -0.0012 |          12.2820 |           4.2274 |
[32m[20230114 18:48:36 @agent_ppo2.py:189][0m |          -0.0065 |          11.6488 |           4.2290 |
[32m[20230114 18:48:36 @agent_ppo2.py:189][0m |          -0.0095 |          11.3322 |           4.2211 |
[32m[20230114 18:48:36 @agent_ppo2.py:189][0m |          -0.0107 |          11.0937 |           4.2227 |
[32m[20230114 18:48:36 @agent_ppo2.py:189][0m |          -0.0126 |          10.8265 |           4.2228 |
[32m[20230114 18:48:36 @agent_ppo2.py:189][0m |          -0.0127 |          10.6148 |           4.2208 |
[32m[20230114 18:48:36 @agent_ppo2.py:189][0m |          -0.0121 |          10.4258 |           4.2199 |
[32m[20230114 18:48:36 @agent_ppo2.py:189][0m |          -0.0141 |          10.1786 |           4.2156 |
[32m[20230114 18:48:36 @agent_ppo2.py:189][0m |          -0.0149 |          10.0699 |           4.2185 |
[32m[20230114 18:48:36 @agent_ppo2.py:134][0m Policy update time: 0.86 s
[32m[20230114 18:48:37 @agent_ppo2.py:142][0m Average TRAINING episode reward: 259.33
[32m[20230114 18:48:37 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 260.47
[32m[20230114 18:48:37 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 264.34
[32m[20230114 18:48:37 @agent_ppo2.py:147][0m Total time:       6.54 min
[32m[20230114 18:48:37 @agent_ppo2.py:149][0m 522240 total steps have happened
[32m[20230114 18:48:37 @agent_ppo2.py:125][0m #------------------------ Iteration 255 --------------------------#
[32m[20230114 18:48:37 @agent_ppo2.py:131][0m Sampling time: 0.25 s by 4 slaves
[32m[20230114 18:48:37 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:48:37 @agent_ppo2.py:189][0m |           0.0012 |          12.9112 |           4.2425 |
[32m[20230114 18:48:37 @agent_ppo2.py:189][0m |          -0.0044 |          11.6611 |           4.2390 |
[32m[20230114 18:48:37 @agent_ppo2.py:189][0m |          -0.0071 |          11.1869 |           4.2387 |
[32m[20230114 18:48:37 @agent_ppo2.py:189][0m |          -0.0081 |          10.9097 |           4.2373 |
[32m[20230114 18:48:37 @agent_ppo2.py:189][0m |          -0.0102 |          10.6938 |           4.2356 |
[32m[20230114 18:48:37 @agent_ppo2.py:189][0m |          -0.0086 |          10.5851 |           4.2387 |
[32m[20230114 18:48:37 @agent_ppo2.py:189][0m |          -0.0099 |          10.3928 |           4.2387 |
[32m[20230114 18:48:38 @agent_ppo2.py:189][0m |          -0.0108 |          10.2544 |           4.2383 |
[32m[20230114 18:48:38 @agent_ppo2.py:189][0m |          -0.0112 |          10.1225 |           4.2386 |
[32m[20230114 18:48:38 @agent_ppo2.py:189][0m |          -0.0125 |          10.0076 |           4.2373 |
[32m[20230114 18:48:38 @agent_ppo2.py:134][0m Policy update time: 0.92 s
[32m[20230114 18:48:38 @agent_ppo2.py:142][0m Average TRAINING episode reward: 250.71
[32m[20230114 18:48:38 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 254.30
[32m[20230114 18:48:38 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 271.04
[32m[20230114 18:48:38 @agent_ppo2.py:147][0m Total time:       6.56 min
[32m[20230114 18:48:38 @agent_ppo2.py:149][0m 524288 total steps have happened
[32m[20230114 18:48:38 @agent_ppo2.py:125][0m #------------------------ Iteration 256 --------------------------#
[32m[20230114 18:48:38 @agent_ppo2.py:131][0m Sampling time: 0.28 s by 4 slaves
[32m[20230114 18:48:38 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:48:38 @agent_ppo2.py:189][0m |          -0.0062 |          16.4780 |           4.2283 |
[32m[20230114 18:48:38 @agent_ppo2.py:189][0m |          -0.0110 |          12.9141 |           4.2193 |
[32m[20230114 18:48:39 @agent_ppo2.py:189][0m |          -0.0121 |          11.6719 |           4.2199 |
[32m[20230114 18:48:39 @agent_ppo2.py:189][0m |          -0.0128 |          10.9720 |           4.2204 |
[32m[20230114 18:48:39 @agent_ppo2.py:189][0m |          -0.0073 |          10.6303 |           4.2172 |
[32m[20230114 18:48:39 @agent_ppo2.py:189][0m |          -0.0121 |          10.1116 |           4.2198 |
[32m[20230114 18:48:39 @agent_ppo2.py:189][0m |          -0.0175 |           9.8457 |           4.2208 |
[32m[20230114 18:48:39 @agent_ppo2.py:189][0m |          -0.0159 |           9.7487 |           4.2195 |
[32m[20230114 18:48:39 @agent_ppo2.py:189][0m |          -0.0163 |           9.4560 |           4.2174 |
[32m[20230114 18:48:39 @agent_ppo2.py:189][0m |          -0.0182 |           9.1927 |           4.2181 |
[32m[20230114 18:48:39 @agent_ppo2.py:134][0m Policy update time: 0.92 s
[32m[20230114 18:48:39 @agent_ppo2.py:142][0m Average TRAINING episode reward: 190.28
[32m[20230114 18:48:39 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 259.02
[32m[20230114 18:48:39 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 267.80
[32m[20230114 18:48:39 @agent_ppo2.py:147][0m Total time:       6.59 min
[32m[20230114 18:48:39 @agent_ppo2.py:149][0m 526336 total steps have happened
[32m[20230114 18:48:39 @agent_ppo2.py:125][0m #------------------------ Iteration 257 --------------------------#
[32m[20230114 18:48:40 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:48:40 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:48:40 @agent_ppo2.py:189][0m |          -0.0014 |          12.1299 |           4.2884 |
[32m[20230114 18:48:40 @agent_ppo2.py:189][0m |          -0.0063 |          10.3494 |           4.2884 |
[32m[20230114 18:48:40 @agent_ppo2.py:189][0m |          -0.0058 |           9.3642 |           4.2892 |
[32m[20230114 18:48:40 @agent_ppo2.py:189][0m |          -0.0089 |           8.6201 |           4.2827 |
[32m[20230114 18:48:40 @agent_ppo2.py:189][0m |          -0.0083 |           8.1732 |           4.2856 |
[32m[20230114 18:48:40 @agent_ppo2.py:189][0m |          -0.0053 |           7.9491 |           4.2843 |
[32m[20230114 18:48:40 @agent_ppo2.py:189][0m |          -0.0099 |           7.6062 |           4.2836 |
[32m[20230114 18:48:40 @agent_ppo2.py:189][0m |          -0.0071 |           7.2714 |           4.2821 |
[32m[20230114 18:48:40 @agent_ppo2.py:189][0m |          -0.0103 |           7.0371 |           4.2805 |
[32m[20230114 18:48:40 @agent_ppo2.py:189][0m |          -0.0119 |           6.8675 |           4.2822 |
[32m[20230114 18:48:40 @agent_ppo2.py:134][0m Policy update time: 0.87 s
[32m[20230114 18:48:41 @agent_ppo2.py:142][0m Average TRAINING episode reward: 256.06
[32m[20230114 18:48:41 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 258.08
[32m[20230114 18:48:41 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 268.30
[32m[20230114 18:48:41 @agent_ppo2.py:147][0m Total time:       6.61 min
[32m[20230114 18:48:41 @agent_ppo2.py:149][0m 528384 total steps have happened
[32m[20230114 18:48:41 @agent_ppo2.py:125][0m #------------------------ Iteration 258 --------------------------#
[32m[20230114 18:48:41 @agent_ppo2.py:131][0m Sampling time: 0.26 s by 4 slaves
[32m[20230114 18:48:41 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:48:41 @agent_ppo2.py:189][0m |          -0.0043 |          13.2238 |           4.2989 |
[32m[20230114 18:48:41 @agent_ppo2.py:189][0m |          -0.0051 |          11.6620 |           4.2936 |
[32m[20230114 18:48:41 @agent_ppo2.py:189][0m |          -0.0057 |          10.9881 |           4.2926 |
[32m[20230114 18:48:41 @agent_ppo2.py:189][0m |          -0.0059 |          10.8006 |           4.2915 |
[32m[20230114 18:48:41 @agent_ppo2.py:189][0m |          -0.0102 |          10.1514 |           4.2936 |
[32m[20230114 18:48:42 @agent_ppo2.py:189][0m |          -0.0121 |           9.9224 |           4.2906 |
[32m[20230114 18:48:42 @agent_ppo2.py:189][0m |          -0.0126 |           9.7142 |           4.2915 |
[32m[20230114 18:48:42 @agent_ppo2.py:189][0m |          -0.0100 |           9.5693 |           4.2860 |
[32m[20230114 18:48:42 @agent_ppo2.py:189][0m |          -0.0137 |           9.4635 |           4.2927 |
[32m[20230114 18:48:42 @agent_ppo2.py:189][0m |          -0.0140 |           9.3006 |           4.2890 |
[32m[20230114 18:48:42 @agent_ppo2.py:134][0m Policy update time: 0.92 s
[32m[20230114 18:48:42 @agent_ppo2.py:142][0m Average TRAINING episode reward: 250.55
[32m[20230114 18:48:42 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 254.01
[32m[20230114 18:48:42 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 257.23
[32m[20230114 18:48:42 @agent_ppo2.py:147][0m Total time:       6.63 min
[32m[20230114 18:48:42 @agent_ppo2.py:149][0m 530432 total steps have happened
[32m[20230114 18:48:42 @agent_ppo2.py:125][0m #------------------------ Iteration 259 --------------------------#
[32m[20230114 18:48:42 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:48:42 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:48:43 @agent_ppo2.py:189][0m |          -0.0005 |          11.6200 |           4.3897 |
[32m[20230114 18:48:43 @agent_ppo2.py:189][0m |          -0.0048 |          10.2781 |           4.3845 |
[32m[20230114 18:48:43 @agent_ppo2.py:189][0m |          -0.0070 |           9.5486 |           4.3798 |
[32m[20230114 18:48:43 @agent_ppo2.py:189][0m |          -0.0083 |           8.7882 |           4.3801 |
[32m[20230114 18:48:43 @agent_ppo2.py:189][0m |          -0.0102 |           8.2795 |           4.3783 |
[32m[20230114 18:48:43 @agent_ppo2.py:189][0m |          -0.0107 |           7.8131 |           4.3772 |
[32m[20230114 18:48:43 @agent_ppo2.py:189][0m |          -0.0113 |           7.5019 |           4.3745 |
[32m[20230114 18:48:43 @agent_ppo2.py:189][0m |          -0.0122 |           7.1675 |           4.3737 |
[32m[20230114 18:48:43 @agent_ppo2.py:189][0m |          -0.0124 |           6.9776 |           4.3737 |
[32m[20230114 18:48:43 @agent_ppo2.py:189][0m |          -0.0129 |           6.6989 |           4.3745 |
[32m[20230114 18:48:43 @agent_ppo2.py:134][0m Policy update time: 0.85 s
[32m[20230114 18:48:43 @agent_ppo2.py:142][0m Average TRAINING episode reward: 258.07
[32m[20230114 18:48:43 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 263.26
[32m[20230114 18:48:43 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 269.38
[32m[20230114 18:48:43 @agent_ppo2.py:147][0m Total time:       6.66 min
[32m[20230114 18:48:43 @agent_ppo2.py:149][0m 532480 total steps have happened
[32m[20230114 18:48:43 @agent_ppo2.py:125][0m #------------------------ Iteration 260 --------------------------#
[32m[20230114 18:48:44 @agent_ppo2.py:131][0m Sampling time: 0.27 s by 4 slaves
[32m[20230114 18:48:44 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:48:44 @agent_ppo2.py:189][0m |           0.0018 |          13.2781 |           4.2517 |
[32m[20230114 18:48:44 @agent_ppo2.py:189][0m |          -0.0045 |          11.4508 |           4.2579 |
[32m[20230114 18:48:44 @agent_ppo2.py:189][0m |          -0.0070 |          10.9958 |           4.2563 |
[32m[20230114 18:48:44 @agent_ppo2.py:189][0m |          -0.0050 |          10.7494 |           4.2490 |
[32m[20230114 18:48:44 @agent_ppo2.py:189][0m |          -0.0063 |          10.4253 |           4.2554 |
[32m[20230114 18:48:44 @agent_ppo2.py:189][0m |          -0.0078 |          10.2581 |           4.2571 |
[32m[20230114 18:48:44 @agent_ppo2.py:189][0m |          -0.0077 |          10.0540 |           4.2532 |
[32m[20230114 18:48:45 @agent_ppo2.py:189][0m |          -0.0022 |          10.1892 |           4.2536 |
[32m[20230114 18:48:45 @agent_ppo2.py:189][0m |          -0.0070 |           9.7207 |           4.2489 |
[32m[20230114 18:48:45 @agent_ppo2.py:189][0m |          -0.0092 |           9.5375 |           4.2558 |
[32m[20230114 18:48:45 @agent_ppo2.py:134][0m Policy update time: 1.02 s
[32m[20230114 18:48:45 @agent_ppo2.py:142][0m Average TRAINING episode reward: 239.75
[32m[20230114 18:48:45 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 243.28
[32m[20230114 18:48:45 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 267.22
[32m[20230114 18:48:45 @agent_ppo2.py:147][0m Total time:       6.68 min
[32m[20230114 18:48:45 @agent_ppo2.py:149][0m 534528 total steps have happened
[32m[20230114 18:48:45 @agent_ppo2.py:125][0m #------------------------ Iteration 261 --------------------------#
[32m[20230114 18:48:45 @agent_ppo2.py:131][0m Sampling time: 0.28 s by 4 slaves
[32m[20230114 18:48:45 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:48:45 @agent_ppo2.py:189][0m |           0.0010 |          11.5166 |           4.3283 |
[32m[20230114 18:48:46 @agent_ppo2.py:189][0m |          -0.0019 |          10.3050 |           4.3217 |
[32m[20230114 18:48:46 @agent_ppo2.py:189][0m |          -0.0033 |           9.8806 |           4.3158 |
[32m[20230114 18:48:46 @agent_ppo2.py:189][0m |          -0.0049 |           9.6175 |           4.3138 |
[32m[20230114 18:48:46 @agent_ppo2.py:189][0m |          -0.0053 |           9.4320 |           4.3138 |
[32m[20230114 18:48:46 @agent_ppo2.py:189][0m |          -0.0058 |           9.2741 |           4.3111 |
[32m[20230114 18:48:46 @agent_ppo2.py:189][0m |          -0.0064 |           9.1394 |           4.3142 |
[32m[20230114 18:48:46 @agent_ppo2.py:189][0m |          -0.0069 |           9.0100 |           4.3139 |
[32m[20230114 18:48:46 @agent_ppo2.py:189][0m |          -0.0074 |           8.8553 |           4.3120 |
[32m[20230114 18:48:46 @agent_ppo2.py:189][0m |          -0.0076 |           8.7620 |           4.3117 |
[32m[20230114 18:48:46 @agent_ppo2.py:134][0m Policy update time: 0.84 s
[32m[20230114 18:48:46 @agent_ppo2.py:142][0m Average TRAINING episode reward: 256.09
[32m[20230114 18:48:46 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 260.78
[32m[20230114 18:48:46 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 266.53
[32m[20230114 18:48:46 @agent_ppo2.py:147][0m Total time:       6.70 min
[32m[20230114 18:48:46 @agent_ppo2.py:149][0m 536576 total steps have happened
[32m[20230114 18:48:46 @agent_ppo2.py:125][0m #------------------------ Iteration 262 --------------------------#
[32m[20230114 18:48:47 @agent_ppo2.py:131][0m Sampling time: 0.25 s by 4 slaves
[32m[20230114 18:48:47 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:48:47 @agent_ppo2.py:189][0m |          -0.0031 |          11.9787 |           4.2479 |
[32m[20230114 18:48:47 @agent_ppo2.py:189][0m |          -0.0052 |          10.7610 |           4.2432 |
[32m[20230114 18:48:47 @agent_ppo2.py:189][0m |          -0.0080 |          10.0324 |           4.2397 |
[32m[20230114 18:48:47 @agent_ppo2.py:189][0m |          -0.0076 |           9.7252 |           4.2437 |
[32m[20230114 18:48:47 @agent_ppo2.py:189][0m |          -0.0066 |           9.2329 |           4.2378 |
[32m[20230114 18:48:47 @agent_ppo2.py:189][0m |          -0.0083 |           9.0206 |           4.2425 |
[32m[20230114 18:48:47 @agent_ppo2.py:189][0m |          -0.0142 |           8.5809 |           4.2408 |
[32m[20230114 18:48:47 @agent_ppo2.py:189][0m |          -0.0149 |           8.2426 |           4.2446 |
[32m[20230114 18:48:47 @agent_ppo2.py:189][0m |          -0.0129 |           8.0261 |           4.2417 |
[32m[20230114 18:48:47 @agent_ppo2.py:189][0m |          -0.0153 |           7.8427 |           4.2456 |
[32m[20230114 18:48:47 @agent_ppo2.py:134][0m Policy update time: 0.91 s
[32m[20230114 18:48:48 @agent_ppo2.py:142][0m Average TRAINING episode reward: 254.77
[32m[20230114 18:48:48 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 259.18
[32m[20230114 18:48:48 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 270.96
[32m[20230114 18:48:48 @agent_ppo2.py:147][0m Total time:       6.73 min
[32m[20230114 18:48:48 @agent_ppo2.py:149][0m 538624 total steps have happened
[32m[20230114 18:48:48 @agent_ppo2.py:125][0m #------------------------ Iteration 263 --------------------------#
[32m[20230114 18:48:48 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:48:48 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:48:48 @agent_ppo2.py:189][0m |          -0.0013 |          13.0814 |           4.2541 |
[32m[20230114 18:48:48 @agent_ppo2.py:189][0m |          -0.0073 |          12.1599 |           4.2421 |
[32m[20230114 18:48:48 @agent_ppo2.py:189][0m |          -0.0059 |          12.1756 |           4.2379 |
[32m[20230114 18:48:48 @agent_ppo2.py:189][0m |          -0.0067 |          12.0085 |           4.2374 |
[32m[20230114 18:48:48 @agent_ppo2.py:189][0m |          -0.0102 |          11.5328 |           4.2315 |
[32m[20230114 18:48:48 @agent_ppo2.py:189][0m |          -0.0068 |          11.8577 |           4.2363 |
[32m[20230114 18:48:49 @agent_ppo2.py:189][0m |          -0.0076 |          11.4483 |           4.2318 |
[32m[20230114 18:48:49 @agent_ppo2.py:189][0m |          -0.0078 |          11.3871 |           4.2330 |
[32m[20230114 18:48:49 @agent_ppo2.py:189][0m |          -0.0050 |          11.7051 |           4.2334 |
[32m[20230114 18:48:49 @agent_ppo2.py:189][0m |          -0.0109 |          11.0220 |           4.2293 |
[32m[20230114 18:48:49 @agent_ppo2.py:134][0m Policy update time: 0.88 s
[32m[20230114 18:48:49 @agent_ppo2.py:142][0m Average TRAINING episode reward: 256.19
[32m[20230114 18:48:49 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 259.37
[32m[20230114 18:48:49 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 269.62
[32m[20230114 18:48:49 @agent_ppo2.py:147][0m Total time:       6.75 min
[32m[20230114 18:48:49 @agent_ppo2.py:149][0m 540672 total steps have happened
[32m[20230114 18:48:49 @agent_ppo2.py:125][0m #------------------------ Iteration 264 --------------------------#
[32m[20230114 18:48:49 @agent_ppo2.py:131][0m Sampling time: 0.26 s by 4 slaves
[32m[20230114 18:48:49 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:48:49 @agent_ppo2.py:189][0m |          -0.0009 |          11.4588 |           4.2859 |
[32m[20230114 18:48:50 @agent_ppo2.py:189][0m |          -0.0020 |          11.1303 |           4.2859 |
[32m[20230114 18:48:50 @agent_ppo2.py:189][0m |          -0.0087 |          10.4172 |           4.2797 |
[32m[20230114 18:48:50 @agent_ppo2.py:189][0m |          -0.0111 |          10.1572 |           4.2779 |
[32m[20230114 18:48:50 @agent_ppo2.py:189][0m |          -0.0123 |           9.9667 |           4.2767 |
[32m[20230114 18:48:50 @agent_ppo2.py:189][0m |          -0.0109 |           9.7854 |           4.2800 |
[32m[20230114 18:48:50 @agent_ppo2.py:189][0m |          -0.0119 |           9.6378 |           4.2760 |
[32m[20230114 18:48:50 @agent_ppo2.py:189][0m |          -0.0132 |           9.5029 |           4.2771 |
[32m[20230114 18:48:50 @agent_ppo2.py:189][0m |          -0.0148 |           9.4199 |           4.2755 |
[32m[20230114 18:48:50 @agent_ppo2.py:189][0m |          -0.0150 |           9.2653 |           4.2742 |
[32m[20230114 18:48:50 @agent_ppo2.py:134][0m Policy update time: 0.88 s
[32m[20230114 18:48:50 @agent_ppo2.py:142][0m Average TRAINING episode reward: 255.68
[32m[20230114 18:48:50 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 259.60
[32m[20230114 18:48:50 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 272.76
[32m[20230114 18:48:50 @agent_ppo2.py:147][0m Total time:       6.77 min
[32m[20230114 18:48:50 @agent_ppo2.py:149][0m 542720 total steps have happened
[32m[20230114 18:48:50 @agent_ppo2.py:125][0m #------------------------ Iteration 265 --------------------------#
[32m[20230114 18:48:51 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:48:51 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:48:51 @agent_ppo2.py:189][0m |          -0.0003 |          11.6414 |           4.2520 |
[32m[20230114 18:48:51 @agent_ppo2.py:189][0m |          -0.0051 |          10.4590 |           4.2439 |
[32m[20230114 18:48:51 @agent_ppo2.py:189][0m |          -0.0073 |           9.9189 |           4.2441 |
[32m[20230114 18:48:51 @agent_ppo2.py:189][0m |          -0.0082 |           9.5221 |           4.2435 |
[32m[20230114 18:48:51 @agent_ppo2.py:189][0m |          -0.0092 |           9.1691 |           4.2449 |
[32m[20230114 18:48:51 @agent_ppo2.py:189][0m |          -0.0105 |           8.8952 |           4.2422 |
[32m[20230114 18:48:51 @agent_ppo2.py:189][0m |          -0.0110 |           8.7195 |           4.2410 |
[32m[20230114 18:48:51 @agent_ppo2.py:189][0m |          -0.0119 |           8.5683 |           4.2415 |
[32m[20230114 18:48:51 @agent_ppo2.py:189][0m |          -0.0124 |           8.5151 |           4.2384 |
[32m[20230114 18:48:52 @agent_ppo2.py:189][0m |          -0.0124 |           8.3041 |           4.2402 |
[32m[20230114 18:48:52 @agent_ppo2.py:134][0m Policy update time: 0.94 s
[32m[20230114 18:48:52 @agent_ppo2.py:142][0m Average TRAINING episode reward: 257.41
[32m[20230114 18:48:52 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 259.66
[32m[20230114 18:48:52 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 272.59
[32m[20230114 18:48:52 @agent_ppo2.py:147][0m Total time:       6.79 min
[32m[20230114 18:48:52 @agent_ppo2.py:149][0m 544768 total steps have happened
[32m[20230114 18:48:52 @agent_ppo2.py:125][0m #------------------------ Iteration 266 --------------------------#
[32m[20230114 18:48:52 @agent_ppo2.py:131][0m Sampling time: 0.25 s by 4 slaves
[32m[20230114 18:48:52 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:48:52 @agent_ppo2.py:189][0m |           0.0020 |          11.5861 |           4.2926 |
[32m[20230114 18:48:52 @agent_ppo2.py:189][0m |          -0.0042 |          10.3825 |           4.2892 |
[32m[20230114 18:48:52 @agent_ppo2.py:189][0m |          -0.0053 |          10.1187 |           4.2875 |
[32m[20230114 18:48:52 @agent_ppo2.py:189][0m |          -0.0058 |           9.5236 |           4.2857 |
[32m[20230114 18:48:53 @agent_ppo2.py:189][0m |          -0.0080 |           9.1627 |           4.2850 |
[32m[20230114 18:48:53 @agent_ppo2.py:189][0m |          -0.0081 |           8.8211 |           4.2877 |
[32m[20230114 18:48:53 @agent_ppo2.py:189][0m |          -0.0097 |           8.6672 |           4.2899 |
[32m[20230114 18:48:53 @agent_ppo2.py:189][0m |          -0.0107 |           8.4668 |           4.2871 |
[32m[20230114 18:48:53 @agent_ppo2.py:189][0m |          -0.0081 |           8.3874 |           4.2861 |
[32m[20230114 18:48:53 @agent_ppo2.py:189][0m |          -0.0118 |           8.2271 |           4.2828 |
[32m[20230114 18:48:53 @agent_ppo2.py:134][0m Policy update time: 0.91 s
[32m[20230114 18:48:53 @agent_ppo2.py:142][0m Average TRAINING episode reward: 259.11
[32m[20230114 18:48:53 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 261.42
[32m[20230114 18:48:53 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 273.70
[32m[20230114 18:48:53 @agent_ppo2.py:147][0m Total time:       6.82 min
[32m[20230114 18:48:53 @agent_ppo2.py:149][0m 546816 total steps have happened
[32m[20230114 18:48:53 @agent_ppo2.py:125][0m #------------------------ Iteration 267 --------------------------#
[32m[20230114 18:48:53 @agent_ppo2.py:131][0m Sampling time: 0.27 s by 4 slaves
[32m[20230114 18:48:54 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:48:54 @agent_ppo2.py:189][0m |           0.0020 |          12.3915 |           4.2926 |
[32m[20230114 18:48:54 @agent_ppo2.py:189][0m |          -0.0013 |          10.7048 |           4.2940 |
[32m[20230114 18:48:54 @agent_ppo2.py:189][0m |          -0.0037 |          10.0784 |           4.2911 |
[32m[20230114 18:48:54 @agent_ppo2.py:189][0m |          -0.0051 |           9.6628 |           4.2940 |
[32m[20230114 18:48:54 @agent_ppo2.py:189][0m |          -0.0060 |           9.3110 |           4.2943 |
[32m[20230114 18:48:54 @agent_ppo2.py:189][0m |          -0.0076 |           9.0755 |           4.2925 |
[32m[20230114 18:48:54 @agent_ppo2.py:189][0m |          -0.0094 |           8.7815 |           4.2985 |
[32m[20230114 18:48:54 @agent_ppo2.py:189][0m |          -0.0087 |           8.6371 |           4.2986 |
[32m[20230114 18:48:54 @agent_ppo2.py:189][0m |          -0.0103 |           8.4197 |           4.2946 |
[32m[20230114 18:48:54 @agent_ppo2.py:189][0m |          -0.0098 |           8.2152 |           4.2972 |
[32m[20230114 18:48:54 @agent_ppo2.py:134][0m Policy update time: 0.88 s
[32m[20230114 18:48:55 @agent_ppo2.py:142][0m Average TRAINING episode reward: 258.25
[32m[20230114 18:48:55 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 260.94
[32m[20230114 18:48:55 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 274.16
[32m[20230114 18:48:55 @agent_ppo2.py:147][0m Total time:       6.84 min
[32m[20230114 18:48:55 @agent_ppo2.py:149][0m 548864 total steps have happened
[32m[20230114 18:48:55 @agent_ppo2.py:125][0m #------------------------ Iteration 268 --------------------------#
[32m[20230114 18:48:55 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:48:55 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:48:55 @agent_ppo2.py:189][0m |          -0.0013 |          12.5599 |           4.3624 |
[32m[20230114 18:48:55 @agent_ppo2.py:189][0m |          -0.0059 |          11.8830 |           4.3541 |
[32m[20230114 18:48:55 @agent_ppo2.py:189][0m |          -0.0058 |          11.7573 |           4.3524 |
[32m[20230114 18:48:55 @agent_ppo2.py:189][0m |          -0.0071 |          11.4860 |           4.3577 |
[32m[20230114 18:48:55 @agent_ppo2.py:189][0m |          -0.0069 |          11.4102 |           4.3544 |
[32m[20230114 18:48:55 @agent_ppo2.py:189][0m |          -0.0089 |          11.1990 |           4.3579 |
[32m[20230114 18:48:55 @agent_ppo2.py:189][0m |          -0.0091 |          11.0529 |           4.3550 |
[32m[20230114 18:48:55 @agent_ppo2.py:189][0m |          -0.0099 |          10.9168 |           4.3559 |
[32m[20230114 18:48:56 @agent_ppo2.py:189][0m |          -0.0109 |          10.8185 |           4.3586 |
[32m[20230114 18:48:56 @agent_ppo2.py:189][0m |          -0.0105 |          10.7359 |           4.3560 |
[32m[20230114 18:48:56 @agent_ppo2.py:134][0m Policy update time: 0.86 s
[32m[20230114 18:48:56 @agent_ppo2.py:142][0m Average TRAINING episode reward: 258.21
[32m[20230114 18:48:56 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 262.11
[32m[20230114 18:48:56 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 274.10
[32m[20230114 18:48:56 @agent_ppo2.py:147][0m Total time:       6.86 min
[32m[20230114 18:48:56 @agent_ppo2.py:149][0m 550912 total steps have happened
[32m[20230114 18:48:56 @agent_ppo2.py:125][0m #------------------------ Iteration 269 --------------------------#
[32m[20230114 18:48:56 @agent_ppo2.py:131][0m Sampling time: 0.25 s by 4 slaves
[32m[20230114 18:48:56 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:48:56 @agent_ppo2.py:189][0m |          -0.0001 |          11.9861 |           4.2543 |
[32m[20230114 18:48:56 @agent_ppo2.py:189][0m |          -0.0052 |          11.0415 |           4.2540 |
[32m[20230114 18:48:56 @agent_ppo2.py:189][0m |          -0.0075 |          10.4510 |           4.2512 |
[32m[20230114 18:48:56 @agent_ppo2.py:189][0m |          -0.0096 |           9.9813 |           4.2496 |
[32m[20230114 18:48:57 @agent_ppo2.py:189][0m |          -0.0113 |           9.6248 |           4.2503 |
[32m[20230114 18:48:57 @agent_ppo2.py:189][0m |          -0.0101 |           9.4492 |           4.2526 |
[32m[20230114 18:48:57 @agent_ppo2.py:189][0m |          -0.0124 |           9.1484 |           4.2511 |
[32m[20230114 18:48:57 @agent_ppo2.py:189][0m |          -0.0129 |           8.9090 |           4.2503 |
[32m[20230114 18:48:57 @agent_ppo2.py:189][0m |          -0.0139 |           8.7354 |           4.2497 |
[32m[20230114 18:48:57 @agent_ppo2.py:189][0m |          -0.0138 |           8.5845 |           4.2487 |
[32m[20230114 18:48:57 @agent_ppo2.py:134][0m Policy update time: 0.88 s
[32m[20230114 18:48:57 @agent_ppo2.py:142][0m Average TRAINING episode reward: 257.74
[32m[20230114 18:48:57 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 262.58
[32m[20230114 18:48:57 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 270.79
[32m[20230114 18:48:57 @agent_ppo2.py:147][0m Total time:       6.88 min
[32m[20230114 18:48:57 @agent_ppo2.py:149][0m 552960 total steps have happened
[32m[20230114 18:48:57 @agent_ppo2.py:125][0m #------------------------ Iteration 270 --------------------------#
[32m[20230114 18:48:57 @agent_ppo2.py:131][0m Sampling time: 0.30 s by 4 slaves
[32m[20230114 18:48:58 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:48:58 @agent_ppo2.py:189][0m |          -0.0012 |          13.1043 |           4.4204 |
[32m[20230114 18:48:58 @agent_ppo2.py:189][0m |          -0.0066 |          11.3519 |           4.4106 |
[32m[20230114 18:48:58 @agent_ppo2.py:189][0m |          -0.0007 |          10.9425 |           4.4102 |
[32m[20230114 18:48:58 @agent_ppo2.py:189][0m |          -0.0076 |          10.1551 |           4.4100 |
[32m[20230114 18:48:58 @agent_ppo2.py:189][0m |          -0.0110 |           9.8219 |           4.4110 |
[32m[20230114 18:48:58 @agent_ppo2.py:189][0m |          -0.0120 |           9.4271 |           4.4110 |
[32m[20230114 18:48:58 @agent_ppo2.py:189][0m |          -0.0079 |           9.2557 |           4.4111 |
[32m[20230114 18:48:58 @agent_ppo2.py:189][0m |          -0.0106 |           8.8586 |           4.4097 |
[32m[20230114 18:48:58 @agent_ppo2.py:189][0m |          -0.0093 |           8.7705 |           4.4056 |
[32m[20230114 18:48:58 @agent_ppo2.py:189][0m |          -0.0110 |           8.5592 |           4.4073 |
[32m[20230114 18:48:58 @agent_ppo2.py:134][0m Policy update time: 0.83 s
[32m[20230114 18:48:58 @agent_ppo2.py:142][0m Average TRAINING episode reward: 259.33
[32m[20230114 18:48:58 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 260.92
[32m[20230114 18:48:58 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 272.45
[32m[20230114 18:48:58 @agent_ppo2.py:147][0m Total time:       6.91 min
[32m[20230114 18:48:58 @agent_ppo2.py:149][0m 555008 total steps have happened
[32m[20230114 18:48:58 @agent_ppo2.py:125][0m #------------------------ Iteration 271 --------------------------#
[32m[20230114 18:48:59 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:48:59 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:48:59 @agent_ppo2.py:189][0m |          -0.0016 |          11.5046 |           4.4235 |
[32m[20230114 18:48:59 @agent_ppo2.py:189][0m |          -0.0043 |           9.9832 |           4.4138 |
[32m[20230114 18:48:59 @agent_ppo2.py:189][0m |          -0.0084 |           9.3207 |           4.4111 |
[32m[20230114 18:48:59 @agent_ppo2.py:189][0m |          -0.0099 |           8.8796 |           4.4095 |
[32m[20230114 18:48:59 @agent_ppo2.py:189][0m |          -0.0121 |           8.5497 |           4.4045 |
[32m[20230114 18:48:59 @agent_ppo2.py:189][0m |          -0.0123 |           8.1884 |           4.4037 |
[32m[20230114 18:48:59 @agent_ppo2.py:189][0m |          -0.0127 |           7.9556 |           4.4019 |
[32m[20230114 18:48:59 @agent_ppo2.py:189][0m |          -0.0102 |           7.7966 |           4.4028 |
[32m[20230114 18:49:00 @agent_ppo2.py:189][0m |          -0.0117 |           7.6209 |           4.4055 |
[32m[20230114 18:49:00 @agent_ppo2.py:189][0m |          -0.0105 |           7.4812 |           4.3998 |
[32m[20230114 18:49:00 @agent_ppo2.py:134][0m Policy update time: 0.88 s
[32m[20230114 18:49:00 @agent_ppo2.py:142][0m Average TRAINING episode reward: 254.99
[32m[20230114 18:49:00 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 257.26
[32m[20230114 18:49:00 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 273.66
[32m[20230114 18:49:00 @agent_ppo2.py:147][0m Total time:       6.93 min
[32m[20230114 18:49:00 @agent_ppo2.py:149][0m 557056 total steps have happened
[32m[20230114 18:49:00 @agent_ppo2.py:125][0m #------------------------ Iteration 272 --------------------------#
[32m[20230114 18:49:00 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:49:00 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:49:00 @agent_ppo2.py:189][0m |          -0.0078 |          12.5738 |           4.3053 |
[32m[20230114 18:49:00 @agent_ppo2.py:189][0m |          -0.0085 |          11.4758 |           4.3012 |
[32m[20230114 18:49:00 @agent_ppo2.py:189][0m |           0.0115 |          12.1644 |           4.3014 |
[32m[20230114 18:49:00 @agent_ppo2.py:189][0m |          -0.0109 |          10.7929 |           4.2915 |
[32m[20230114 18:49:01 @agent_ppo2.py:189][0m |          -0.0140 |          10.4478 |           4.2975 |
[32m[20230114 18:49:01 @agent_ppo2.py:189][0m |          -0.0088 |          10.2276 |           4.2966 |
[32m[20230114 18:49:01 @agent_ppo2.py:189][0m |          -0.0129 |          10.0639 |           4.2965 |
[32m[20230114 18:49:01 @agent_ppo2.py:189][0m |          -0.0102 |           9.8730 |           4.2960 |
[32m[20230114 18:49:01 @agent_ppo2.py:189][0m |          -0.0140 |           9.6830 |           4.2934 |
[32m[20230114 18:49:01 @agent_ppo2.py:189][0m |          -0.0143 |           9.5797 |           4.2970 |
[32m[20230114 18:49:01 @agent_ppo2.py:134][0m Policy update time: 0.84 s
[32m[20230114 18:49:01 @agent_ppo2.py:142][0m Average TRAINING episode reward: 258.92
[32m[20230114 18:49:01 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 262.74
[32m[20230114 18:49:01 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 279.11
[32m[20230114 18:49:01 @agent_ppo2.py:147][0m Total time:       6.95 min
[32m[20230114 18:49:01 @agent_ppo2.py:149][0m 559104 total steps have happened
[32m[20230114 18:49:01 @agent_ppo2.py:125][0m #------------------------ Iteration 273 --------------------------#
[32m[20230114 18:49:01 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:49:01 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:49:01 @agent_ppo2.py:189][0m |          -0.0034 |          17.4203 |           4.3785 |
[32m[20230114 18:49:02 @agent_ppo2.py:189][0m |          -0.0069 |          10.2990 |           4.3768 |
[32m[20230114 18:49:02 @agent_ppo2.py:189][0m |          -0.0067 |           9.0396 |           4.3706 |
[32m[20230114 18:49:02 @agent_ppo2.py:189][0m |          -0.0094 |           8.5489 |           4.3700 |
[32m[20230114 18:49:02 @agent_ppo2.py:189][0m |          -0.0125 |           7.9068 |           4.3672 |
[32m[20230114 18:49:02 @agent_ppo2.py:189][0m |          -0.0155 |           7.5383 |           4.3640 |
[32m[20230114 18:49:02 @agent_ppo2.py:189][0m |          -0.0152 |           7.2411 |           4.3653 |
[32m[20230114 18:49:02 @agent_ppo2.py:189][0m |          -0.0120 |           7.0706 |           4.3610 |
[32m[20230114 18:49:02 @agent_ppo2.py:189][0m |          -0.0144 |           6.8116 |           4.3620 |
[32m[20230114 18:49:02 @agent_ppo2.py:189][0m |          -0.0177 |           6.6086 |           4.3617 |
[32m[20230114 18:49:02 @agent_ppo2.py:134][0m Policy update time: 0.74 s
[32m[20230114 18:49:02 @agent_ppo2.py:142][0m Average TRAINING episode reward: 211.25
[32m[20230114 18:49:02 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 261.17
[32m[20230114 18:49:02 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 274.94
[32m[20230114 18:49:02 @agent_ppo2.py:147][0m Total time:       6.97 min
[32m[20230114 18:49:02 @agent_ppo2.py:149][0m 561152 total steps have happened
[32m[20230114 18:49:02 @agent_ppo2.py:125][0m #------------------------ Iteration 274 --------------------------#
[32m[20230114 18:49:03 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:49:03 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:49:03 @agent_ppo2.py:189][0m |           0.0192 |          15.4111 |           4.3565 |
[32m[20230114 18:49:03 @agent_ppo2.py:189][0m |          -0.0053 |          13.7000 |           4.3573 |
[32m[20230114 18:49:03 @agent_ppo2.py:189][0m |          -0.0085 |          13.1303 |           4.3590 |
[32m[20230114 18:49:03 @agent_ppo2.py:189][0m |          -0.0037 |          12.7882 |           4.3603 |
[32m[20230114 18:49:03 @agent_ppo2.py:189][0m |          -0.0106 |          12.4765 |           4.3686 |
[32m[20230114 18:49:03 @agent_ppo2.py:189][0m |          -0.0123 |          12.1850 |           4.3611 |
[32m[20230114 18:49:03 @agent_ppo2.py:189][0m |          -0.0123 |          11.9543 |           4.3651 |
[32m[20230114 18:49:03 @agent_ppo2.py:189][0m |          -0.0137 |          11.7688 |           4.3685 |
[32m[20230114 18:49:03 @agent_ppo2.py:189][0m |          -0.0145 |          11.6470 |           4.3693 |
[32m[20230114 18:49:03 @agent_ppo2.py:189][0m |          -0.0166 |          11.4947 |           4.3690 |
[32m[20230114 18:49:03 @agent_ppo2.py:134][0m Policy update time: 0.84 s
[32m[20230114 18:49:04 @agent_ppo2.py:142][0m Average TRAINING episode reward: 259.65
[32m[20230114 18:49:04 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 260.89
[32m[20230114 18:49:04 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 275.09
[32m[20230114 18:49:04 @agent_ppo2.py:147][0m Total time:       6.99 min
[32m[20230114 18:49:04 @agent_ppo2.py:149][0m 563200 total steps have happened
[32m[20230114 18:49:04 @agent_ppo2.py:125][0m #------------------------ Iteration 275 --------------------------#
[32m[20230114 18:49:04 @agent_ppo2.py:131][0m Sampling time: 0.27 s by 4 slaves
[32m[20230114 18:49:04 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:49:04 @agent_ppo2.py:189][0m |          -0.0040 |          13.0483 |           4.4596 |
[32m[20230114 18:49:04 @agent_ppo2.py:189][0m |          -0.0058 |          11.9151 |           4.4456 |
[32m[20230114 18:49:04 @agent_ppo2.py:189][0m |          -0.0097 |          11.3612 |           4.4525 |
[32m[20230114 18:49:04 @agent_ppo2.py:189][0m |          -0.0104 |          10.9693 |           4.4450 |
[32m[20230114 18:49:04 @agent_ppo2.py:189][0m |          -0.0130 |          10.5510 |           4.4499 |
[32m[20230114 18:49:04 @agent_ppo2.py:189][0m |          -0.0141 |          10.2274 |           4.4519 |
[32m[20230114 18:49:04 @agent_ppo2.py:189][0m |          -0.0096 |          10.2722 |           4.4496 |
[32m[20230114 18:49:05 @agent_ppo2.py:189][0m |          -0.0145 |           9.7137 |           4.4587 |
[32m[20230114 18:49:05 @agent_ppo2.py:189][0m |          -0.0160 |           9.3537 |           4.4567 |
[32m[20230114 18:49:05 @agent_ppo2.py:189][0m |          -0.0162 |           9.1334 |           4.4588 |
[32m[20230114 18:49:05 @agent_ppo2.py:134][0m Policy update time: 0.87 s
[32m[20230114 18:49:05 @agent_ppo2.py:142][0m Average TRAINING episode reward: 255.12
[32m[20230114 18:49:05 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 259.75
[32m[20230114 18:49:05 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 266.54
[32m[20230114 18:49:05 @agent_ppo2.py:147][0m Total time:       7.01 min
[32m[20230114 18:49:05 @agent_ppo2.py:149][0m 565248 total steps have happened
[32m[20230114 18:49:05 @agent_ppo2.py:125][0m #------------------------ Iteration 276 --------------------------#
[32m[20230114 18:49:05 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:49:05 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:49:05 @agent_ppo2.py:189][0m |          -0.0000 |          13.7470 |           4.3968 |
[32m[20230114 18:49:05 @agent_ppo2.py:189][0m |          -0.0059 |          13.0090 |           4.3964 |
[32m[20230114 18:49:05 @agent_ppo2.py:189][0m |          -0.0086 |          12.7036 |           4.3951 |
[32m[20230114 18:49:06 @agent_ppo2.py:189][0m |          -0.0115 |          12.5678 |           4.3958 |
[32m[20230114 18:49:06 @agent_ppo2.py:189][0m |          -0.0115 |          12.3385 |           4.3909 |
[32m[20230114 18:49:06 @agent_ppo2.py:189][0m |          -0.0113 |          12.2374 |           4.3897 |
[32m[20230114 18:49:06 @agent_ppo2.py:189][0m |          -0.0094 |          12.2404 |           4.3962 |
[32m[20230114 18:49:06 @agent_ppo2.py:189][0m |          -0.0083 |          12.4400 |           4.3908 |
[32m[20230114 18:49:06 @agent_ppo2.py:189][0m |          -0.0100 |          12.0065 |           4.3881 |
[32m[20230114 18:49:06 @agent_ppo2.py:189][0m |          -0.0122 |          11.7979 |           4.3866 |
[32m[20230114 18:49:06 @agent_ppo2.py:134][0m Policy update time: 0.83 s
[32m[20230114 18:49:06 @agent_ppo2.py:142][0m Average TRAINING episode reward: 260.10
[32m[20230114 18:49:06 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 261.32
[32m[20230114 18:49:06 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 273.42
[32m[20230114 18:49:06 @agent_ppo2.py:147][0m Total time:       7.03 min
[32m[20230114 18:49:06 @agent_ppo2.py:149][0m 567296 total steps have happened
[32m[20230114 18:49:06 @agent_ppo2.py:125][0m #------------------------ Iteration 277 --------------------------#
[32m[20230114 18:49:06 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:49:06 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:49:07 @agent_ppo2.py:189][0m |          -0.0002 |          13.6945 |           4.4581 |
[32m[20230114 18:49:07 @agent_ppo2.py:189][0m |          -0.0005 |          12.5079 |           4.4478 |
[32m[20230114 18:49:07 @agent_ppo2.py:189][0m |          -0.0047 |          11.6946 |           4.4550 |
[32m[20230114 18:49:07 @agent_ppo2.py:189][0m |          -0.0063 |          11.0041 |           4.4490 |
[32m[20230114 18:49:07 @agent_ppo2.py:189][0m |          -0.0068 |          10.5208 |           4.4539 |
[32m[20230114 18:49:07 @agent_ppo2.py:189][0m |          -0.0057 |          10.3031 |           4.4519 |
[32m[20230114 18:49:07 @agent_ppo2.py:189][0m |          -0.0092 |           9.8437 |           4.4505 |
[32m[20230114 18:49:07 @agent_ppo2.py:189][0m |          -0.0079 |           9.5917 |           4.4498 |
[32m[20230114 18:49:07 @agent_ppo2.py:189][0m |          -0.0096 |           9.3198 |           4.4517 |
[32m[20230114 18:49:07 @agent_ppo2.py:189][0m |          -0.0090 |           9.4878 |           4.4542 |
[32m[20230114 18:49:07 @agent_ppo2.py:134][0m Policy update time: 0.83 s
[32m[20230114 18:49:07 @agent_ppo2.py:142][0m Average TRAINING episode reward: 259.12
[32m[20230114 18:49:07 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 261.80
[32m[20230114 18:49:07 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 274.29
[32m[20230114 18:49:07 @agent_ppo2.py:147][0m Total time:       7.05 min
[32m[20230114 18:49:07 @agent_ppo2.py:149][0m 569344 total steps have happened
[32m[20230114 18:49:07 @agent_ppo2.py:125][0m #------------------------ Iteration 278 --------------------------#
[32m[20230114 18:49:08 @agent_ppo2.py:131][0m Sampling time: 0.26 s by 4 slaves
[32m[20230114 18:49:08 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:49:08 @agent_ppo2.py:189][0m |           0.0030 |          21.1777 |           4.5128 |
[32m[20230114 18:49:08 @agent_ppo2.py:189][0m |          -0.0056 |          15.7942 |           4.5103 |
[32m[20230114 18:49:08 @agent_ppo2.py:189][0m |          -0.0084 |          14.7787 |           4.5075 |
[32m[20230114 18:49:08 @agent_ppo2.py:189][0m |          -0.0095 |          14.1959 |           4.5055 |
[32m[20230114 18:49:08 @agent_ppo2.py:189][0m |          -0.0109 |          13.7180 |           4.5043 |
[32m[20230114 18:49:08 @agent_ppo2.py:189][0m |          -0.0104 |          13.5184 |           4.5026 |
[32m[20230114 18:49:08 @agent_ppo2.py:189][0m |          -0.0112 |          13.3203 |           4.5007 |
[32m[20230114 18:49:08 @agent_ppo2.py:189][0m |          -0.0125 |          13.0482 |           4.4993 |
[32m[20230114 18:49:08 @agent_ppo2.py:189][0m |          -0.0131 |          12.8846 |           4.5011 |
[32m[20230114 18:49:09 @agent_ppo2.py:189][0m |          -0.0139 |          12.7055 |           4.5021 |
[32m[20230114 18:49:09 @agent_ppo2.py:134][0m Policy update time: 0.87 s
[32m[20230114 18:49:09 @agent_ppo2.py:142][0m Average TRAINING episode reward: 193.92
[32m[20230114 18:49:09 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 263.27
[32m[20230114 18:49:09 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 273.68
[32m[20230114 18:49:09 @agent_ppo2.py:147][0m Total time:       7.08 min
[32m[20230114 18:49:09 @agent_ppo2.py:149][0m 571392 total steps have happened
[32m[20230114 18:49:09 @agent_ppo2.py:125][0m #------------------------ Iteration 279 --------------------------#
[32m[20230114 18:49:09 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:49:09 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:49:09 @agent_ppo2.py:189][0m |          -0.0025 |          12.6877 |           4.5183 |
[32m[20230114 18:49:09 @agent_ppo2.py:189][0m |           0.0096 |          12.8353 |           4.5145 |
[32m[20230114 18:49:09 @agent_ppo2.py:189][0m |          -0.0086 |          10.0366 |           4.5161 |
[32m[20230114 18:49:09 @agent_ppo2.py:189][0m |          -0.0077 |           9.4408 |           4.5161 |
[32m[20230114 18:49:09 @agent_ppo2.py:189][0m |          -0.0101 |           8.8892 |           4.5147 |
[32m[20230114 18:49:10 @agent_ppo2.py:189][0m |          -0.0109 |           8.4052 |           4.5103 |
[32m[20230114 18:49:10 @agent_ppo2.py:189][0m |          -0.0127 |           7.9203 |           4.5115 |
[32m[20230114 18:49:10 @agent_ppo2.py:189][0m |          -0.0112 |           7.4764 |           4.5118 |
[32m[20230114 18:49:10 @agent_ppo2.py:189][0m |          -0.0154 |           7.1057 |           4.5087 |
[32m[20230114 18:49:10 @agent_ppo2.py:189][0m |          -0.0147 |           6.6875 |           4.5081 |
[32m[20230114 18:49:10 @agent_ppo2.py:134][0m Policy update time: 0.82 s
[32m[20230114 18:49:10 @agent_ppo2.py:142][0m Average TRAINING episode reward: 262.74
[32m[20230114 18:49:10 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 265.15
[32m[20230114 18:49:10 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 272.67
[32m[20230114 18:49:10 @agent_ppo2.py:147][0m Total time:       7.10 min
[32m[20230114 18:49:10 @agent_ppo2.py:149][0m 573440 total steps have happened
[32m[20230114 18:49:10 @agent_ppo2.py:125][0m #------------------------ Iteration 280 --------------------------#
[32m[20230114 18:49:10 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:49:10 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:49:10 @agent_ppo2.py:189][0m |           0.0058 |          14.3938 |           4.5295 |
[32m[20230114 18:49:11 @agent_ppo2.py:189][0m |          -0.0056 |          11.9205 |           4.5207 |
[32m[20230114 18:49:11 @agent_ppo2.py:189][0m |          -0.0083 |          11.3354 |           4.5233 |
[32m[20230114 18:49:11 @agent_ppo2.py:189][0m |          -0.0067 |          10.9615 |           4.5230 |
[32m[20230114 18:49:11 @agent_ppo2.py:189][0m |          -0.0115 |          10.6710 |           4.5188 |
[32m[20230114 18:49:11 @agent_ppo2.py:189][0m |          -0.0101 |          10.3957 |           4.5176 |
[32m[20230114 18:49:11 @agent_ppo2.py:189][0m |          -0.0116 |          10.1742 |           4.5168 |
[32m[20230114 18:49:11 @agent_ppo2.py:189][0m |          -0.0147 |           9.9472 |           4.5176 |
[32m[20230114 18:49:11 @agent_ppo2.py:189][0m |          -0.0162 |           9.7758 |           4.5159 |
[32m[20230114 18:49:11 @agent_ppo2.py:189][0m |          -0.0119 |           9.5992 |           4.5118 |
[32m[20230114 18:49:11 @agent_ppo2.py:134][0m Policy update time: 0.85 s
[32m[20230114 18:49:11 @agent_ppo2.py:142][0m Average TRAINING episode reward: 256.96
[32m[20230114 18:49:11 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 258.42
[32m[20230114 18:49:11 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 102.64
[32m[20230114 18:49:11 @agent_ppo2.py:147][0m Total time:       7.12 min
[32m[20230114 18:49:11 @agent_ppo2.py:149][0m 575488 total steps have happened
[32m[20230114 18:49:11 @agent_ppo2.py:125][0m #------------------------ Iteration 281 --------------------------#
[32m[20230114 18:49:12 @agent_ppo2.py:131][0m Sampling time: 0.26 s by 4 slaves
[32m[20230114 18:49:12 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:49:12 @agent_ppo2.py:189][0m |          -0.0007 |          16.2728 |           4.5690 |
[32m[20230114 18:49:12 @agent_ppo2.py:189][0m |          -0.0038 |          13.6029 |           4.5609 |
[32m[20230114 18:49:12 @agent_ppo2.py:189][0m |          -0.0065 |          12.9652 |           4.5590 |
[32m[20230114 18:49:12 @agent_ppo2.py:189][0m |          -0.0078 |          12.5843 |           4.5557 |
[32m[20230114 18:49:12 @agent_ppo2.py:189][0m |          -0.0094 |          12.3057 |           4.5574 |
[32m[20230114 18:49:12 @agent_ppo2.py:189][0m |          -0.0089 |          12.1007 |           4.5584 |
[32m[20230114 18:49:12 @agent_ppo2.py:189][0m |          -0.0106 |          11.8288 |           4.5571 |
[32m[20230114 18:49:12 @agent_ppo2.py:189][0m |          -0.0107 |          11.6752 |           4.5555 |
[32m[20230114 18:49:12 @agent_ppo2.py:189][0m |          -0.0100 |          11.6150 |           4.5563 |
[32m[20230114 18:49:12 @agent_ppo2.py:189][0m |          -0.0119 |          11.3142 |           4.5554 |
[32m[20230114 18:49:12 @agent_ppo2.py:134][0m Policy update time: 0.84 s
[32m[20230114 18:49:13 @agent_ppo2.py:142][0m Average TRAINING episode reward: 258.01
[32m[20230114 18:49:13 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 259.54
[32m[20230114 18:49:13 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 274.14
[32m[20230114 18:49:13 @agent_ppo2.py:147][0m Total time:       7.14 min
[32m[20230114 18:49:13 @agent_ppo2.py:149][0m 577536 total steps have happened
[32m[20230114 18:49:13 @agent_ppo2.py:125][0m #------------------------ Iteration 282 --------------------------#
[32m[20230114 18:49:13 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:49:13 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:49:13 @agent_ppo2.py:189][0m |           0.0003 |          12.6987 |           4.5730 |
[32m[20230114 18:49:13 @agent_ppo2.py:189][0m |          -0.0036 |          11.4581 |           4.5672 |
[32m[20230114 18:49:13 @agent_ppo2.py:189][0m |          -0.0048 |          10.6969 |           4.5631 |
[32m[20230114 18:49:13 @agent_ppo2.py:189][0m |          -0.0075 |           9.9117 |           4.5586 |
[32m[20230114 18:49:13 @agent_ppo2.py:189][0m |          -0.0080 |           9.5809 |           4.5500 |
[32m[20230114 18:49:13 @agent_ppo2.py:189][0m |          -0.0095 |           9.2641 |           4.5497 |
[32m[20230114 18:49:13 @agent_ppo2.py:189][0m |          -0.0100 |           8.9588 |           4.5528 |
[32m[20230114 18:49:14 @agent_ppo2.py:189][0m |          -0.0106 |           8.7963 |           4.5461 |
[32m[20230114 18:49:14 @agent_ppo2.py:189][0m |          -0.0121 |           8.5004 |           4.5436 |
[32m[20230114 18:49:14 @agent_ppo2.py:189][0m |          -0.0113 |           8.4760 |           4.5403 |
[32m[20230114 18:49:14 @agent_ppo2.py:134][0m Policy update time: 0.84 s
[32m[20230114 18:49:14 @agent_ppo2.py:142][0m Average TRAINING episode reward: 260.21
[32m[20230114 18:49:14 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 264.14
[32m[20230114 18:49:14 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 276.84
[32m[20230114 18:49:14 @agent_ppo2.py:147][0m Total time:       7.16 min
[32m[20230114 18:49:14 @agent_ppo2.py:149][0m 579584 total steps have happened
[32m[20230114 18:49:14 @agent_ppo2.py:125][0m #------------------------ Iteration 283 --------------------------#
[32m[20230114 18:49:14 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:49:14 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:49:14 @agent_ppo2.py:189][0m |           0.0025 |          12.8971 |           4.4605 |
[32m[20230114 18:49:14 @agent_ppo2.py:189][0m |           0.0017 |          11.1781 |           4.4574 |
[32m[20230114 18:49:14 @agent_ppo2.py:189][0m |          -0.0036 |          10.4267 |           4.4538 |
[32m[20230114 18:49:15 @agent_ppo2.py:189][0m |          -0.0061 |           9.9899 |           4.4611 |
[32m[20230114 18:49:15 @agent_ppo2.py:189][0m |          -0.0046 |           9.6255 |           4.4623 |
[32m[20230114 18:49:15 @agent_ppo2.py:189][0m |          -0.0109 |           9.3166 |           4.4648 |
[32m[20230114 18:49:15 @agent_ppo2.py:189][0m |          -0.0072 |           9.0389 |           4.4602 |
[32m[20230114 18:49:15 @agent_ppo2.py:189][0m |          -0.0111 |           8.6149 |           4.4588 |
[32m[20230114 18:49:15 @agent_ppo2.py:189][0m |          -0.0072 |           8.3389 |           4.4613 |
[32m[20230114 18:49:15 @agent_ppo2.py:189][0m |          -0.0099 |           8.0256 |           4.4658 |
[32m[20230114 18:49:15 @agent_ppo2.py:134][0m Policy update time: 0.84 s
[32m[20230114 18:49:15 @agent_ppo2.py:142][0m Average TRAINING episode reward: 259.75
[32m[20230114 18:49:15 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 264.42
[32m[20230114 18:49:15 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 270.72
[32m[20230114 18:49:15 @agent_ppo2.py:147][0m Total time:       7.18 min
[32m[20230114 18:49:15 @agent_ppo2.py:149][0m 581632 total steps have happened
[32m[20230114 18:49:15 @agent_ppo2.py:125][0m #------------------------ Iteration 284 --------------------------#
[32m[20230114 18:49:15 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:49:16 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:49:16 @agent_ppo2.py:189][0m |          -0.0016 |          13.1191 |           4.4642 |
[32m[20230114 18:49:16 @agent_ppo2.py:189][0m |          -0.0071 |          12.0561 |           4.4585 |
[32m[20230114 18:49:16 @agent_ppo2.py:189][0m |          -0.0068 |          11.8727 |           4.4580 |
[32m[20230114 18:49:16 @agent_ppo2.py:189][0m |          -0.0065 |          11.6015 |           4.4547 |
[32m[20230114 18:49:16 @agent_ppo2.py:189][0m |          -0.0079 |          11.0201 |           4.4583 |
[32m[20230114 18:49:16 @agent_ppo2.py:189][0m |          -0.0085 |          10.8216 |           4.4613 |
[32m[20230114 18:49:16 @agent_ppo2.py:189][0m |          -0.0117 |          10.6374 |           4.4587 |
[32m[20230114 18:49:16 @agent_ppo2.py:189][0m |          -0.0090 |          10.4695 |           4.4582 |
[32m[20230114 18:49:16 @agent_ppo2.py:189][0m |          -0.0115 |          10.3411 |           4.4598 |
[32m[20230114 18:49:16 @agent_ppo2.py:189][0m |          -0.0111 |          10.1642 |           4.4602 |
[32m[20230114 18:49:16 @agent_ppo2.py:134][0m Policy update time: 0.84 s
[32m[20230114 18:49:16 @agent_ppo2.py:142][0m Average TRAINING episode reward: 260.58
[32m[20230114 18:49:16 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 262.64
[32m[20230114 18:49:16 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 268.14
[32m[20230114 18:49:16 @agent_ppo2.py:147][0m Total time:       7.21 min
[32m[20230114 18:49:16 @agent_ppo2.py:149][0m 583680 total steps have happened
[32m[20230114 18:49:16 @agent_ppo2.py:125][0m #------------------------ Iteration 285 --------------------------#
[32m[20230114 18:49:17 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:49:17 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:49:17 @agent_ppo2.py:189][0m |          -0.0000 |          13.0621 |           4.6374 |
[32m[20230114 18:49:17 @agent_ppo2.py:189][0m |          -0.0068 |          10.8669 |           4.6356 |
[32m[20230114 18:49:17 @agent_ppo2.py:189][0m |          -0.0088 |           9.6524 |           4.6271 |
[32m[20230114 18:49:17 @agent_ppo2.py:189][0m |          -0.0095 |           8.7800 |           4.6265 |
[32m[20230114 18:49:17 @agent_ppo2.py:189][0m |          -0.0113 |           8.1289 |           4.6180 |
[32m[20230114 18:49:17 @agent_ppo2.py:189][0m |          -0.0116 |           7.5248 |           4.6186 |
[32m[20230114 18:49:17 @agent_ppo2.py:189][0m |          -0.0125 |           6.9720 |           4.6171 |
[32m[20230114 18:49:17 @agent_ppo2.py:189][0m |          -0.0125 |           6.5314 |           4.6076 |
[32m[20230114 18:49:17 @agent_ppo2.py:189][0m |          -0.0132 |           6.1936 |           4.6081 |
[32m[20230114 18:49:18 @agent_ppo2.py:189][0m |          -0.0134 |           5.9290 |           4.6048 |
[32m[20230114 18:49:18 @agent_ppo2.py:134][0m Policy update time: 0.84 s
[32m[20230114 18:49:18 @agent_ppo2.py:142][0m Average TRAINING episode reward: 258.74
[32m[20230114 18:49:18 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 262.48
[32m[20230114 18:49:18 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 274.02
[32m[20230114 18:49:18 @agent_ppo2.py:147][0m Total time:       7.23 min
[32m[20230114 18:49:18 @agent_ppo2.py:149][0m 585728 total steps have happened
[32m[20230114 18:49:18 @agent_ppo2.py:125][0m #------------------------ Iteration 286 --------------------------#
[32m[20230114 18:49:18 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:49:18 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:49:18 @agent_ppo2.py:189][0m |          -0.0006 |          13.6305 |           4.5337 |
[32m[20230114 18:49:18 @agent_ppo2.py:189][0m |          -0.0041 |          12.2565 |           4.5289 |
[32m[20230114 18:49:18 @agent_ppo2.py:189][0m |          -0.0059 |          11.7669 |           4.5287 |
[32m[20230114 18:49:18 @agent_ppo2.py:189][0m |          -0.0072 |          11.4531 |           4.5276 |
[32m[20230114 18:49:18 @agent_ppo2.py:189][0m |          -0.0079 |          11.2574 |           4.5255 |
[32m[20230114 18:49:19 @agent_ppo2.py:189][0m |          -0.0099 |          10.8981 |           4.5252 |
[32m[20230114 18:49:19 @agent_ppo2.py:189][0m |          -0.0092 |          10.6737 |           4.5229 |
[32m[20230114 18:49:19 @agent_ppo2.py:189][0m |          -0.0123 |          10.4809 |           4.5239 |
[32m[20230114 18:49:19 @agent_ppo2.py:189][0m |          -0.0127 |          10.3049 |           4.5235 |
[32m[20230114 18:49:19 @agent_ppo2.py:189][0m |          -0.0124 |          10.1763 |           4.5295 |
[32m[20230114 18:49:19 @agent_ppo2.py:134][0m Policy update time: 0.87 s
[32m[20230114 18:49:19 @agent_ppo2.py:142][0m Average TRAINING episode reward: 255.55
[32m[20230114 18:49:19 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 261.83
[32m[20230114 18:49:19 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 271.21
[32m[20230114 18:49:19 @agent_ppo2.py:147][0m Total time:       7.25 min
[32m[20230114 18:49:19 @agent_ppo2.py:149][0m 587776 total steps have happened
[32m[20230114 18:49:19 @agent_ppo2.py:125][0m #------------------------ Iteration 287 --------------------------#
[32m[20230114 18:49:19 @agent_ppo2.py:131][0m Sampling time: 0.26 s by 4 slaves
[32m[20230114 18:49:19 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:49:20 @agent_ppo2.py:189][0m |           0.0021 |          13.7244 |           4.4979 |
[32m[20230114 18:49:20 @agent_ppo2.py:189][0m |          -0.0012 |          12.4054 |           4.4915 |
[32m[20230114 18:49:20 @agent_ppo2.py:189][0m |          -0.0057 |          11.7649 |           4.4894 |
[32m[20230114 18:49:20 @agent_ppo2.py:189][0m |          -0.0060 |          11.5536 |           4.4869 |
[32m[20230114 18:49:20 @agent_ppo2.py:189][0m |          -0.0077 |          11.2871 |           4.4918 |
[32m[20230114 18:49:20 @agent_ppo2.py:189][0m |          -0.0077 |          10.9593 |           4.4885 |
[32m[20230114 18:49:20 @agent_ppo2.py:189][0m |          -0.0086 |          10.7687 |           4.4898 |
[32m[20230114 18:49:20 @agent_ppo2.py:189][0m |          -0.0078 |          10.7860 |           4.4884 |
[32m[20230114 18:49:20 @agent_ppo2.py:189][0m |          -0.0085 |          10.5252 |           4.4879 |
[32m[20230114 18:49:20 @agent_ppo2.py:189][0m |          -0.0122 |          10.2431 |           4.4886 |
[32m[20230114 18:49:20 @agent_ppo2.py:134][0m Policy update time: 0.87 s
[32m[20230114 18:49:20 @agent_ppo2.py:142][0m Average TRAINING episode reward: 255.42
[32m[20230114 18:49:20 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 261.37
[32m[20230114 18:49:20 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 270.77
[32m[20230114 18:49:20 @agent_ppo2.py:147][0m Total time:       7.27 min
[32m[20230114 18:49:20 @agent_ppo2.py:149][0m 589824 total steps have happened
[32m[20230114 18:49:20 @agent_ppo2.py:125][0m #------------------------ Iteration 288 --------------------------#
[32m[20230114 18:49:21 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:49:21 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:49:21 @agent_ppo2.py:189][0m |           0.0006 |          13.7301 |           4.5383 |
[32m[20230114 18:49:21 @agent_ppo2.py:189][0m |          -0.0069 |          12.5058 |           4.5268 |
[32m[20230114 18:49:21 @agent_ppo2.py:189][0m |          -0.0045 |          12.1740 |           4.5223 |
[32m[20230114 18:49:21 @agent_ppo2.py:189][0m |          -0.0097 |          11.9879 |           4.5166 |
[32m[20230114 18:49:21 @agent_ppo2.py:189][0m |          -0.0087 |          11.8888 |           4.5158 |
[32m[20230114 18:49:21 @agent_ppo2.py:189][0m |          -0.0106 |          11.7560 |           4.5138 |
[32m[20230114 18:49:21 @agent_ppo2.py:189][0m |          -0.0077 |          12.2544 |           4.5128 |
[32m[20230114 18:49:21 @agent_ppo2.py:189][0m |          -0.0093 |          11.5202 |           4.5036 |
[32m[20230114 18:49:21 @agent_ppo2.py:189][0m |          -0.0114 |          11.3888 |           4.5022 |
[32m[20230114 18:49:22 @agent_ppo2.py:189][0m |          -0.0100 |          11.2279 |           4.5042 |
[32m[20230114 18:49:22 @agent_ppo2.py:134][0m Policy update time: 0.86 s
[32m[20230114 18:49:22 @agent_ppo2.py:142][0m Average TRAINING episode reward: 255.34
[32m[20230114 18:49:22 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 257.08
[32m[20230114 18:49:22 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 272.61
[32m[20230114 18:49:22 @agent_ppo2.py:147][0m Total time:       7.29 min
[32m[20230114 18:49:22 @agent_ppo2.py:149][0m 591872 total steps have happened
[32m[20230114 18:49:22 @agent_ppo2.py:125][0m #------------------------ Iteration 289 --------------------------#
[32m[20230114 18:49:22 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:49:22 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:49:22 @agent_ppo2.py:189][0m |           0.0023 |          17.2260 |           4.4982 |
[32m[20230114 18:49:22 @agent_ppo2.py:189][0m |          -0.0009 |          14.0214 |           4.4956 |
[32m[20230114 18:49:22 @agent_ppo2.py:189][0m |          -0.0078 |          13.0353 |           4.4920 |
[32m[20230114 18:49:22 @agent_ppo2.py:189][0m |          -0.0102 |          12.7269 |           4.4859 |
[32m[20230114 18:49:22 @agent_ppo2.py:189][0m |          -0.0147 |          12.2866 |           4.4872 |
[32m[20230114 18:49:22 @agent_ppo2.py:189][0m |          -0.0147 |          12.0265 |           4.4852 |
[32m[20230114 18:49:23 @agent_ppo2.py:189][0m |          -0.0131 |          11.9503 |           4.4827 |
[32m[20230114 18:49:23 @agent_ppo2.py:189][0m |          -0.0144 |          11.8645 |           4.4838 |
[32m[20230114 18:49:23 @agent_ppo2.py:189][0m |          -0.0154 |          11.6616 |           4.4816 |
[32m[20230114 18:49:23 @agent_ppo2.py:189][0m |          -0.0125 |          11.4598 |           4.4808 |
[32m[20230114 18:49:23 @agent_ppo2.py:134][0m Policy update time: 0.74 s
[32m[20230114 18:49:23 @agent_ppo2.py:142][0m Average TRAINING episode reward: 202.21
[32m[20230114 18:49:23 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 259.26
[32m[20230114 18:49:23 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 269.12
[32m[20230114 18:49:23 @agent_ppo2.py:147][0m Total time:       7.31 min
[32m[20230114 18:49:23 @agent_ppo2.py:149][0m 593920 total steps have happened
[32m[20230114 18:49:23 @agent_ppo2.py:125][0m #------------------------ Iteration 290 --------------------------#
[32m[20230114 18:49:23 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:49:23 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:49:23 @agent_ppo2.py:189][0m |          -0.0032 |          13.8809 |           4.4768 |
[32m[20230114 18:49:23 @agent_ppo2.py:189][0m |          -0.0033 |          13.1383 |           4.4685 |
[32m[20230114 18:49:23 @agent_ppo2.py:189][0m |          -0.0042 |          12.6577 |           4.4662 |
[32m[20230114 18:49:24 @agent_ppo2.py:189][0m |          -0.0057 |          12.2041 |           4.4618 |
[32m[20230114 18:49:24 @agent_ppo2.py:189][0m |          -0.0089 |          11.8254 |           4.4599 |
[32m[20230114 18:49:24 @agent_ppo2.py:189][0m |          -0.0083 |          11.4407 |           4.4586 |
[32m[20230114 18:49:24 @agent_ppo2.py:189][0m |          -0.0118 |          11.1543 |           4.4525 |
[32m[20230114 18:49:24 @agent_ppo2.py:189][0m |          -0.0116 |          10.9106 |           4.4557 |
[32m[20230114 18:49:24 @agent_ppo2.py:189][0m |          -0.0106 |          10.6893 |           4.4532 |
[32m[20230114 18:49:24 @agent_ppo2.py:189][0m |          -0.0097 |          10.5692 |           4.4518 |
[32m[20230114 18:49:24 @agent_ppo2.py:134][0m Policy update time: 0.83 s
[32m[20230114 18:49:24 @agent_ppo2.py:142][0m Average TRAINING episode reward: 257.47
[32m[20230114 18:49:24 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 261.58
[32m[20230114 18:49:24 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 271.03
[32m[20230114 18:49:24 @agent_ppo2.py:147][0m Total time:       7.33 min
[32m[20230114 18:49:24 @agent_ppo2.py:149][0m 595968 total steps have happened
[32m[20230114 18:49:24 @agent_ppo2.py:125][0m #------------------------ Iteration 291 --------------------------#
[32m[20230114 18:49:24 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:49:25 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:49:25 @agent_ppo2.py:189][0m |          -0.0001 |          14.1151 |           4.4920 |
[32m[20230114 18:49:25 @agent_ppo2.py:189][0m |          -0.0067 |          12.5729 |           4.4866 |
[32m[20230114 18:49:25 @agent_ppo2.py:189][0m |          -0.0085 |          11.9820 |           4.4836 |
[32m[20230114 18:49:25 @agent_ppo2.py:189][0m |          -0.0095 |          11.5844 |           4.4842 |
[32m[20230114 18:49:25 @agent_ppo2.py:189][0m |          -0.0096 |          11.3463 |           4.4827 |
[32m[20230114 18:49:25 @agent_ppo2.py:189][0m |          -0.0109 |          10.9215 |           4.4792 |
[32m[20230114 18:49:25 @agent_ppo2.py:189][0m |          -0.0121 |          10.6097 |           4.4787 |
[32m[20230114 18:49:25 @agent_ppo2.py:189][0m |          -0.0121 |          10.3401 |           4.4754 |
[32m[20230114 18:49:25 @agent_ppo2.py:189][0m |          -0.0123 |          10.0995 |           4.4706 |
[32m[20230114 18:49:25 @agent_ppo2.py:189][0m |          -0.0137 |           9.9007 |           4.4724 |
[32m[20230114 18:49:25 @agent_ppo2.py:134][0m Policy update time: 0.83 s
[32m[20230114 18:49:25 @agent_ppo2.py:142][0m Average TRAINING episode reward: 259.97
[32m[20230114 18:49:25 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 263.17
[32m[20230114 18:49:25 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 272.86
[32m[20230114 18:49:25 @agent_ppo2.py:147][0m Total time:       7.36 min
[32m[20230114 18:49:25 @agent_ppo2.py:149][0m 598016 total steps have happened
[32m[20230114 18:49:26 @agent_ppo2.py:125][0m #------------------------ Iteration 292 --------------------------#
[32m[20230114 18:49:26 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:49:26 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:49:26 @agent_ppo2.py:189][0m |          -0.0006 |          13.5230 |           4.4551 |
[32m[20230114 18:49:26 @agent_ppo2.py:189][0m |           0.0017 |          13.3176 |           4.4550 |
[32m[20230114 18:49:26 @agent_ppo2.py:189][0m |          -0.0056 |          12.2697 |           4.4550 |
[32m[20230114 18:49:26 @agent_ppo2.py:189][0m |          -0.0056 |          12.0737 |           4.4570 |
[32m[20230114 18:49:26 @agent_ppo2.py:189][0m |          -0.0058 |          12.0772 |           4.4551 |
[32m[20230114 18:49:26 @agent_ppo2.py:189][0m |          -0.0098 |          11.7609 |           4.4516 |
[32m[20230114 18:49:26 @agent_ppo2.py:189][0m |          -0.0107 |          11.6297 |           4.4561 |
[32m[20230114 18:49:26 @agent_ppo2.py:189][0m |          -0.0126 |          11.5255 |           4.4530 |
[32m[20230114 18:49:27 @agent_ppo2.py:189][0m |          -0.0120 |          11.4831 |           4.4551 |
[32m[20230114 18:49:27 @agent_ppo2.py:189][0m |          -0.0131 |          11.3863 |           4.4551 |
[32m[20230114 18:49:27 @agent_ppo2.py:134][0m Policy update time: 0.87 s
[32m[20230114 18:49:27 @agent_ppo2.py:142][0m Average TRAINING episode reward: 255.60
[32m[20230114 18:49:27 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 258.47
[32m[20230114 18:49:27 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 273.04
[32m[20230114 18:49:27 @agent_ppo2.py:147][0m Total time:       7.38 min
[32m[20230114 18:49:27 @agent_ppo2.py:149][0m 600064 total steps have happened
[32m[20230114 18:49:27 @agent_ppo2.py:125][0m #------------------------ Iteration 293 --------------------------#
[32m[20230114 18:49:27 @agent_ppo2.py:131][0m Sampling time: 0.26 s by 4 slaves
[32m[20230114 18:49:27 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:49:27 @agent_ppo2.py:189][0m |          -0.0006 |          22.9544 |           4.5486 |
[32m[20230114 18:49:27 @agent_ppo2.py:189][0m |          -0.0058 |          15.5934 |           4.5375 |
[32m[20230114 18:49:27 @agent_ppo2.py:189][0m |          -0.0087 |          14.1719 |           4.5328 |
[32m[20230114 18:49:27 @agent_ppo2.py:189][0m |          -0.0089 |          13.3824 |           4.5320 |
[32m[20230114 18:49:27 @agent_ppo2.py:189][0m |          -0.0109 |          12.8911 |           4.5272 |
[32m[20230114 18:49:28 @agent_ppo2.py:189][0m |          -0.0124 |          12.2403 |           4.5302 |
[32m[20230114 18:49:28 @agent_ppo2.py:189][0m |          -0.0128 |          11.9496 |           4.5253 |
[32m[20230114 18:49:28 @agent_ppo2.py:189][0m |          -0.0125 |          11.6290 |           4.5248 |
[32m[20230114 18:49:28 @agent_ppo2.py:189][0m |          -0.0131 |          11.4126 |           4.5234 |
[32m[20230114 18:49:28 @agent_ppo2.py:189][0m |          -0.0135 |          11.0302 |           4.5226 |
[32m[20230114 18:49:28 @agent_ppo2.py:134][0m Policy update time: 0.75 s
[32m[20230114 18:49:28 @agent_ppo2.py:142][0m Average TRAINING episode reward: 206.65
[32m[20230114 18:49:28 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 264.45
[32m[20230114 18:49:28 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 273.48
[32m[20230114 18:49:28 @agent_ppo2.py:147][0m Total time:       7.40 min
[32m[20230114 18:49:28 @agent_ppo2.py:149][0m 602112 total steps have happened
[32m[20230114 18:49:28 @agent_ppo2.py:125][0m #------------------------ Iteration 294 --------------------------#
[32m[20230114 18:49:28 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:49:28 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:49:28 @agent_ppo2.py:189][0m |           0.0011 |          15.4979 |           4.5065 |
[32m[20230114 18:49:29 @agent_ppo2.py:189][0m |          -0.0027 |          14.0124 |           4.5035 |
[32m[20230114 18:49:29 @agent_ppo2.py:189][0m |          -0.0054 |          13.5580 |           4.5039 |
[32m[20230114 18:49:29 @agent_ppo2.py:189][0m |          -0.0067 |          13.2855 |           4.5022 |
[32m[20230114 18:49:29 @agent_ppo2.py:189][0m |          -0.0076 |          13.0736 |           4.5016 |
[32m[20230114 18:49:29 @agent_ppo2.py:189][0m |          -0.0083 |          12.8956 |           4.4992 |
[32m[20230114 18:49:29 @agent_ppo2.py:189][0m |          -0.0092 |          12.7572 |           4.5005 |
[32m[20230114 18:49:29 @agent_ppo2.py:189][0m |          -0.0095 |          12.6571 |           4.4973 |
[32m[20230114 18:49:29 @agent_ppo2.py:189][0m |          -0.0099 |          12.5310 |           4.5007 |
[32m[20230114 18:49:29 @agent_ppo2.py:189][0m |          -0.0108 |          12.4210 |           4.4967 |
[32m[20230114 18:49:29 @agent_ppo2.py:134][0m Policy update time: 0.85 s
[32m[20230114 18:49:29 @agent_ppo2.py:142][0m Average TRAINING episode reward: 258.47
[32m[20230114 18:49:29 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 264.13
[32m[20230114 18:49:29 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 273.66
[32m[20230114 18:49:29 @agent_ppo2.py:147][0m Total time:       7.42 min
[32m[20230114 18:49:29 @agent_ppo2.py:149][0m 604160 total steps have happened
[32m[20230114 18:49:29 @agent_ppo2.py:125][0m #------------------------ Iteration 295 --------------------------#
[32m[20230114 18:49:30 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:49:30 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:49:30 @agent_ppo2.py:189][0m |          -0.0037 |          15.2119 |           4.3644 |
[32m[20230114 18:49:30 @agent_ppo2.py:189][0m |          -0.0062 |          12.7052 |           4.3594 |
[32m[20230114 18:49:30 @agent_ppo2.py:189][0m |          -0.0115 |          11.8482 |           4.3529 |
[32m[20230114 18:49:30 @agent_ppo2.py:189][0m |          -0.0068 |          11.2385 |           4.3525 |
[32m[20230114 18:49:30 @agent_ppo2.py:189][0m |           0.0024 |          10.9602 |           4.3560 |
[32m[20230114 18:49:30 @agent_ppo2.py:189][0m |          -0.0092 |          10.7015 |           4.3521 |
[32m[20230114 18:49:30 @agent_ppo2.py:189][0m |          -0.0079 |          10.3857 |           4.3581 |
[32m[20230114 18:49:30 @agent_ppo2.py:189][0m |          -0.0044 |          10.1214 |           4.3527 |
[32m[20230114 18:49:30 @agent_ppo2.py:189][0m |          -0.0074 |           9.9599 |           4.3534 |
[32m[20230114 18:49:30 @agent_ppo2.py:189][0m |          -0.0318 |           9.7398 |           4.3532 |
[32m[20230114 18:49:30 @agent_ppo2.py:134][0m Policy update time: 0.87 s
[32m[20230114 18:49:31 @agent_ppo2.py:142][0m Average TRAINING episode reward: 256.14
[32m[20230114 18:49:31 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 258.89
[32m[20230114 18:49:31 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 274.64
[32m[20230114 18:49:31 @agent_ppo2.py:147][0m Total time:       7.44 min
[32m[20230114 18:49:31 @agent_ppo2.py:149][0m 606208 total steps have happened
[32m[20230114 18:49:31 @agent_ppo2.py:125][0m #------------------------ Iteration 296 --------------------------#
[32m[20230114 18:49:31 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:49:31 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:49:31 @agent_ppo2.py:189][0m |           0.0055 |          13.8657 |           4.4838 |
[32m[20230114 18:49:31 @agent_ppo2.py:189][0m |          -0.0054 |          11.3432 |           4.4830 |
[32m[20230114 18:49:31 @agent_ppo2.py:189][0m |          -0.0079 |          10.6188 |           4.4828 |
[32m[20230114 18:49:31 @agent_ppo2.py:189][0m |          -0.0084 |          10.1673 |           4.4796 |
[32m[20230114 18:49:31 @agent_ppo2.py:189][0m |          -0.0090 |           9.5807 |           4.4760 |
[32m[20230114 18:49:31 @agent_ppo2.py:189][0m |          -0.0099 |           9.2270 |           4.4772 |
[32m[20230114 18:49:32 @agent_ppo2.py:189][0m |          -0.0050 |           9.1288 |           4.4764 |
[32m[20230114 18:49:32 @agent_ppo2.py:189][0m |          -0.0113 |           8.5485 |           4.4732 |
[32m[20230114 18:49:32 @agent_ppo2.py:189][0m |          -0.0039 |           9.0870 |           4.4708 |
[32m[20230114 18:49:32 @agent_ppo2.py:189][0m |          -0.0055 |           8.7250 |           4.4748 |
[32m[20230114 18:49:32 @agent_ppo2.py:134][0m Policy update time: 0.83 s
[32m[20230114 18:49:32 @agent_ppo2.py:142][0m Average TRAINING episode reward: 259.14
[32m[20230114 18:49:32 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 261.90
[32m[20230114 18:49:32 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 273.71
[32m[20230114 18:49:32 @agent_ppo2.py:147][0m Total time:       7.46 min
[32m[20230114 18:49:32 @agent_ppo2.py:149][0m 608256 total steps have happened
[32m[20230114 18:49:32 @agent_ppo2.py:125][0m #------------------------ Iteration 297 --------------------------#
[32m[20230114 18:49:32 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:49:32 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:49:32 @agent_ppo2.py:189][0m |           0.0005 |          11.9872 |           4.3776 |
[32m[20230114 18:49:32 @agent_ppo2.py:189][0m |          -0.0056 |           6.1687 |           4.3739 |
[32m[20230114 18:49:32 @agent_ppo2.py:189][0m |          -0.0083 |           4.4317 |           4.3713 |
[32m[20230114 18:49:33 @agent_ppo2.py:189][0m |          -0.0087 |           3.7001 |           4.3682 |
[32m[20230114 18:49:33 @agent_ppo2.py:189][0m |          -0.0110 |           3.2390 |           4.3688 |
[32m[20230114 18:49:33 @agent_ppo2.py:189][0m |          -0.0108 |           2.9503 |           4.3679 |
[32m[20230114 18:49:33 @agent_ppo2.py:189][0m |          -0.0121 |           2.7348 |           4.3694 |
[32m[20230114 18:49:33 @agent_ppo2.py:189][0m |          -0.0121 |           2.5346 |           4.3679 |
[32m[20230114 18:49:33 @agent_ppo2.py:189][0m |          -0.0138 |           2.3770 |           4.3672 |
[32m[20230114 18:49:33 @agent_ppo2.py:189][0m |          -0.0132 |           2.2595 |           4.3673 |
[32m[20230114 18:49:33 @agent_ppo2.py:134][0m Policy update time: 0.82 s
[32m[20230114 18:49:33 @agent_ppo2.py:142][0m Average TRAINING episode reward: 257.04
[32m[20230114 18:49:33 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 258.95
[32m[20230114 18:49:33 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 273.62
[32m[20230114 18:49:33 @agent_ppo2.py:147][0m Total time:       7.48 min
[32m[20230114 18:49:33 @agent_ppo2.py:149][0m 610304 total steps have happened
[32m[20230114 18:49:33 @agent_ppo2.py:125][0m #------------------------ Iteration 298 --------------------------#
[32m[20230114 18:49:33 @agent_ppo2.py:131][0m Sampling time: 0.25 s by 4 slaves
[32m[20230114 18:49:34 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:49:34 @agent_ppo2.py:189][0m |           0.0033 |          14.8115 |           4.5536 |
[32m[20230114 18:49:34 @agent_ppo2.py:189][0m |          -0.0009 |          11.7611 |           4.5556 |
[32m[20230114 18:49:34 @agent_ppo2.py:189][0m |          -0.0062 |          10.2052 |           4.5520 |
[32m[20230114 18:49:34 @agent_ppo2.py:189][0m |          -0.0064 |           9.5132 |           4.5527 |
[32m[20230114 18:49:34 @agent_ppo2.py:189][0m |          -0.0064 |           8.7836 |           4.5542 |
[32m[20230114 18:49:34 @agent_ppo2.py:189][0m |          -0.0116 |           8.4356 |           4.5557 |
[32m[20230114 18:49:34 @agent_ppo2.py:189][0m |          -0.0109 |           8.1650 |           4.5543 |
[32m[20230114 18:49:34 @agent_ppo2.py:189][0m |          -0.0117 |           7.8781 |           4.5542 |
[32m[20230114 18:49:34 @agent_ppo2.py:189][0m |          -0.0081 |           7.8028 |           4.5561 |
[32m[20230114 18:49:34 @agent_ppo2.py:189][0m |          -0.0106 |           7.5926 |           4.5537 |
[32m[20230114 18:49:34 @agent_ppo2.py:134][0m Policy update time: 0.86 s
[32m[20230114 18:49:35 @agent_ppo2.py:142][0m Average TRAINING episode reward: 254.82
[32m[20230114 18:49:35 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 258.04
[32m[20230114 18:49:35 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 270.62
[32m[20230114 18:49:35 @agent_ppo2.py:147][0m Total time:       7.51 min
[32m[20230114 18:49:35 @agent_ppo2.py:149][0m 612352 total steps have happened
[32m[20230114 18:49:35 @agent_ppo2.py:125][0m #------------------------ Iteration 299 --------------------------#
[32m[20230114 18:49:35 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:49:35 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:49:35 @agent_ppo2.py:189][0m |           0.0014 |          23.9002 |           4.5378 |
[32m[20230114 18:49:35 @agent_ppo2.py:189][0m |          -0.0069 |          14.4698 |           4.5367 |
[32m[20230114 18:49:35 @agent_ppo2.py:189][0m |          -0.0090 |          13.2384 |           4.5315 |
[32m[20230114 18:49:35 @agent_ppo2.py:189][0m |          -0.0083 |          12.7844 |           4.5299 |
[32m[20230114 18:49:35 @agent_ppo2.py:189][0m |          -0.0060 |          12.4508 |           4.5296 |
[32m[20230114 18:49:35 @agent_ppo2.py:189][0m |          -0.0116 |          11.9619 |           4.5231 |
[32m[20230114 18:49:35 @agent_ppo2.py:189][0m |          -0.0124 |          11.5376 |           4.5240 |
[32m[20230114 18:49:35 @agent_ppo2.py:189][0m |          -0.0149 |          11.3495 |           4.5267 |
[32m[20230114 18:49:36 @agent_ppo2.py:189][0m |          -0.0147 |          11.1990 |           4.5291 |
[32m[20230114 18:49:36 @agent_ppo2.py:189][0m |          -0.0148 |          10.9958 |           4.5274 |
[32m[20230114 18:49:36 @agent_ppo2.py:134][0m Policy update time: 0.81 s
[32m[20230114 18:49:36 @agent_ppo2.py:142][0m Average TRAINING episode reward: 211.94
[32m[20230114 18:49:36 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 260.34
[32m[20230114 18:49:36 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 270.19
[32m[20230114 18:49:36 @agent_ppo2.py:147][0m Total time:       7.53 min
[32m[20230114 18:49:36 @agent_ppo2.py:149][0m 614400 total steps have happened
[32m[20230114 18:49:36 @agent_ppo2.py:125][0m #------------------------ Iteration 300 --------------------------#
[32m[20230114 18:49:36 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:49:36 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:49:36 @agent_ppo2.py:189][0m |          -0.0007 |          12.8056 |           4.4975 |
[32m[20230114 18:49:36 @agent_ppo2.py:189][0m |          -0.0063 |          11.2479 |           4.4901 |
[32m[20230114 18:49:36 @agent_ppo2.py:189][0m |          -0.0075 |          10.6110 |           4.4881 |
[32m[20230114 18:49:36 @agent_ppo2.py:189][0m |          -0.0090 |          10.1949 |           4.4887 |
[32m[20230114 18:49:37 @agent_ppo2.py:189][0m |          -0.0099 |           9.8665 |           4.4876 |
[32m[20230114 18:49:37 @agent_ppo2.py:189][0m |          -0.0107 |           9.5958 |           4.4887 |
[32m[20230114 18:49:37 @agent_ppo2.py:189][0m |          -0.0117 |           9.3583 |           4.4875 |
[32m[20230114 18:49:37 @agent_ppo2.py:189][0m |          -0.0128 |           9.1629 |           4.4848 |
[32m[20230114 18:49:37 @agent_ppo2.py:189][0m |          -0.0120 |           9.0304 |           4.4879 |
[32m[20230114 18:49:37 @agent_ppo2.py:189][0m |          -0.0130 |           8.8733 |           4.4851 |
[32m[20230114 18:49:37 @agent_ppo2.py:134][0m Policy update time: 0.83 s
[32m[20230114 18:49:37 @agent_ppo2.py:142][0m Average TRAINING episode reward: 255.09
[32m[20230114 18:49:37 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 259.90
[32m[20230114 18:49:37 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 273.44
[32m[20230114 18:49:37 @agent_ppo2.py:147][0m Total time:       7.55 min
[32m[20230114 18:49:37 @agent_ppo2.py:149][0m 616448 total steps have happened
[32m[20230114 18:49:37 @agent_ppo2.py:125][0m #------------------------ Iteration 301 --------------------------#
[32m[20230114 18:49:37 @agent_ppo2.py:131][0m Sampling time: 0.27 s by 4 slaves
[32m[20230114 18:49:37 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:49:38 @agent_ppo2.py:189][0m |          -0.0014 |          14.0784 |           4.4649 |
[32m[20230114 18:49:38 @agent_ppo2.py:189][0m |          -0.0050 |          12.5412 |           4.4579 |
[32m[20230114 18:49:38 @agent_ppo2.py:189][0m |          -0.0072 |          12.0979 |           4.4563 |
[32m[20230114 18:49:38 @agent_ppo2.py:189][0m |          -0.0066 |          11.8374 |           4.4552 |
[32m[20230114 18:49:38 @agent_ppo2.py:189][0m |          -0.0087 |          11.6253 |           4.4476 |
[32m[20230114 18:49:38 @agent_ppo2.py:189][0m |          -0.0086 |          11.4483 |           4.4557 |
[32m[20230114 18:49:38 @agent_ppo2.py:189][0m |          -0.0066 |          11.5500 |           4.4519 |
[32m[20230114 18:49:38 @agent_ppo2.py:189][0m |          -0.0111 |          11.0754 |           4.4534 |
[32m[20230114 18:49:38 @agent_ppo2.py:189][0m |          -0.0112 |          10.8994 |           4.4531 |
[32m[20230114 18:49:38 @agent_ppo2.py:189][0m |          -0.0109 |          10.7534 |           4.4492 |
[32m[20230114 18:49:38 @agent_ppo2.py:134][0m Policy update time: 0.87 s
[32m[20230114 18:49:38 @agent_ppo2.py:142][0m Average TRAINING episode reward: 256.25
[32m[20230114 18:49:38 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 260.36
[32m[20230114 18:49:38 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 271.64
[32m[20230114 18:49:38 @agent_ppo2.py:147][0m Total time:       7.57 min
[32m[20230114 18:49:38 @agent_ppo2.py:149][0m 618496 total steps have happened
[32m[20230114 18:49:38 @agent_ppo2.py:125][0m #------------------------ Iteration 302 --------------------------#
[32m[20230114 18:49:39 @agent_ppo2.py:131][0m Sampling time: 0.25 s by 4 slaves
[32m[20230114 18:49:39 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:49:39 @agent_ppo2.py:189][0m |          -0.0027 |          13.0041 |           4.5232 |
[32m[20230114 18:49:39 @agent_ppo2.py:189][0m |          -0.0067 |          12.1253 |           4.5144 |
[32m[20230114 18:49:39 @agent_ppo2.py:189][0m |          -0.0099 |          11.8193 |           4.5162 |
[32m[20230114 18:49:39 @agent_ppo2.py:189][0m |          -0.0106 |          11.5994 |           4.5142 |
[32m[20230114 18:49:39 @agent_ppo2.py:189][0m |          -0.0102 |          11.4706 |           4.5153 |
[32m[20230114 18:49:39 @agent_ppo2.py:189][0m |          -0.0130 |          11.3100 |           4.5152 |
[32m[20230114 18:49:39 @agent_ppo2.py:189][0m |          -0.0127 |          11.1774 |           4.5116 |
[32m[20230114 18:49:39 @agent_ppo2.py:189][0m |          -0.0117 |          11.2335 |           4.5171 |
[32m[20230114 18:49:39 @agent_ppo2.py:189][0m |          -0.0128 |          10.9869 |           4.5167 |
[32m[20230114 18:49:40 @agent_ppo2.py:189][0m |          -0.0133 |          10.8758 |           4.5181 |
[32m[20230114 18:49:40 @agent_ppo2.py:134][0m Policy update time: 0.86 s
[32m[20230114 18:49:40 @agent_ppo2.py:142][0m Average TRAINING episode reward: 254.98
[32m[20230114 18:49:40 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 257.47
[32m[20230114 18:49:40 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 270.71
[32m[20230114 18:49:40 @agent_ppo2.py:147][0m Total time:       7.59 min
[32m[20230114 18:49:40 @agent_ppo2.py:149][0m 620544 total steps have happened
[32m[20230114 18:49:40 @agent_ppo2.py:125][0m #------------------------ Iteration 303 --------------------------#
[32m[20230114 18:49:40 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:49:40 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:49:40 @agent_ppo2.py:189][0m |           0.0006 |          10.8667 |           4.6624 |
[32m[20230114 18:49:40 @agent_ppo2.py:189][0m |          -0.0049 |           9.0497 |           4.6537 |
[32m[20230114 18:49:40 @agent_ppo2.py:189][0m |          -0.0059 |           8.1326 |           4.6505 |
[32m[20230114 18:49:40 @agent_ppo2.py:189][0m |          -0.0077 |           7.1394 |           4.6520 |
[32m[20230114 18:49:40 @agent_ppo2.py:189][0m |          -0.0088 |           6.2644 |           4.6497 |
[32m[20230114 18:49:41 @agent_ppo2.py:189][0m |          -0.0103 |           5.5993 |           4.6487 |
[32m[20230114 18:49:41 @agent_ppo2.py:189][0m |          -0.0105 |           5.0077 |           4.6544 |
[32m[20230114 18:49:41 @agent_ppo2.py:189][0m |          -0.0108 |           4.7113 |           4.6545 |
[32m[20230114 18:49:41 @agent_ppo2.py:189][0m |          -0.0125 |           4.2642 |           4.6512 |
[32m[20230114 18:49:41 @agent_ppo2.py:189][0m |          -0.0120 |           3.9572 |           4.6550 |
[32m[20230114 18:49:41 @agent_ppo2.py:134][0m Policy update time: 0.88 s
[32m[20230114 18:49:41 @agent_ppo2.py:142][0m Average TRAINING episode reward: 254.98
[32m[20230114 18:49:41 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 258.01
[32m[20230114 18:49:41 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 107.82
[32m[20230114 18:49:41 @agent_ppo2.py:147][0m Total time:       7.62 min
[32m[20230114 18:49:41 @agent_ppo2.py:149][0m 622592 total steps have happened
[32m[20230114 18:49:41 @agent_ppo2.py:125][0m #------------------------ Iteration 304 --------------------------#
[32m[20230114 18:49:41 @agent_ppo2.py:131][0m Sampling time: 0.26 s by 4 slaves
[32m[20230114 18:49:41 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:49:42 @agent_ppo2.py:189][0m |           0.0006 |          12.7461 |           4.6145 |
[32m[20230114 18:49:42 @agent_ppo2.py:189][0m |          -0.0012 |          11.9620 |           4.6150 |
[32m[20230114 18:49:42 @agent_ppo2.py:189][0m |          -0.0038 |          11.6794 |           4.6095 |
[32m[20230114 18:49:42 @agent_ppo2.py:189][0m |          -0.0055 |          11.4637 |           4.6122 |
[32m[20230114 18:49:42 @agent_ppo2.py:189][0m |          -0.0060 |          11.3229 |           4.6090 |
[32m[20230114 18:49:42 @agent_ppo2.py:189][0m |          -0.0073 |          11.2144 |           4.6158 |
[32m[20230114 18:49:42 @agent_ppo2.py:189][0m |          -0.0064 |          11.0903 |           4.6136 |
[32m[20230114 18:49:42 @agent_ppo2.py:189][0m |          -0.0084 |          11.0026 |           4.6139 |
[32m[20230114 18:49:42 @agent_ppo2.py:189][0m |          -0.0082 |          10.9121 |           4.6173 |
[32m[20230114 18:49:42 @agent_ppo2.py:189][0m |          -0.0092 |          10.8403 |           4.6177 |
[32m[20230114 18:49:42 @agent_ppo2.py:134][0m Policy update time: 0.85 s
[32m[20230114 18:49:42 @agent_ppo2.py:142][0m Average TRAINING episode reward: 258.49
[32m[20230114 18:49:42 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 261.04
[32m[20230114 18:49:42 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 116.74
[32m[20230114 18:49:42 @agent_ppo2.py:147][0m Total time:       7.64 min
[32m[20230114 18:49:42 @agent_ppo2.py:149][0m 624640 total steps have happened
[32m[20230114 18:49:42 @agent_ppo2.py:125][0m #------------------------ Iteration 305 --------------------------#
[32m[20230114 18:49:43 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:49:43 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:49:43 @agent_ppo2.py:189][0m |           0.0015 |          11.7730 |           4.6503 |
[32m[20230114 18:49:43 @agent_ppo2.py:189][0m |          -0.0058 |          10.8311 |           4.6471 |
[32m[20230114 18:49:43 @agent_ppo2.py:189][0m |          -0.0062 |          10.3848 |           4.6471 |
[32m[20230114 18:49:43 @agent_ppo2.py:189][0m |          -0.0069 |          10.3171 |           4.6415 |
[32m[20230114 18:49:43 @agent_ppo2.py:189][0m |          -0.0110 |           9.7626 |           4.6431 |
[32m[20230114 18:49:43 @agent_ppo2.py:189][0m |          -0.0082 |           9.5588 |           4.6406 |
[32m[20230114 18:49:43 @agent_ppo2.py:189][0m |          -0.0118 |           9.3509 |           4.6423 |
[32m[20230114 18:49:43 @agent_ppo2.py:189][0m |          -0.0089 |           9.4428 |           4.6413 |
[32m[20230114 18:49:43 @agent_ppo2.py:189][0m |          -0.0119 |           9.0735 |           4.6391 |
[32m[20230114 18:49:43 @agent_ppo2.py:189][0m |          -0.0140 |           8.8947 |           4.6395 |
[32m[20230114 18:49:43 @agent_ppo2.py:134][0m Policy update time: 0.82 s
[32m[20230114 18:49:44 @agent_ppo2.py:142][0m Average TRAINING episode reward: 259.80
[32m[20230114 18:49:44 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 261.20
[32m[20230114 18:49:44 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 272.24
[32m[20230114 18:49:44 @agent_ppo2.py:147][0m Total time:       7.66 min
[32m[20230114 18:49:44 @agent_ppo2.py:149][0m 626688 total steps have happened
[32m[20230114 18:49:44 @agent_ppo2.py:125][0m #------------------------ Iteration 306 --------------------------#
[32m[20230114 18:49:44 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:49:44 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:49:44 @agent_ppo2.py:189][0m |           0.0013 |          12.6784 |           4.5918 |
[32m[20230114 18:49:44 @agent_ppo2.py:189][0m |          -0.0048 |          10.5999 |           4.5889 |
[32m[20230114 18:49:44 @agent_ppo2.py:189][0m |          -0.0076 |           9.8126 |           4.5920 |
[32m[20230114 18:49:44 @agent_ppo2.py:189][0m |          -0.0083 |           9.0920 |           4.5883 |
[32m[20230114 18:49:44 @agent_ppo2.py:189][0m |          -0.0103 |           8.1545 |           4.5872 |
[32m[20230114 18:49:44 @agent_ppo2.py:189][0m |          -0.0106 |           7.4137 |           4.5876 |
[32m[20230114 18:49:44 @agent_ppo2.py:189][0m |          -0.0106 |           6.8169 |           4.5853 |
[32m[20230114 18:49:45 @agent_ppo2.py:189][0m |          -0.0115 |           6.3333 |           4.5849 |
[32m[20230114 18:49:45 @agent_ppo2.py:189][0m |          -0.0109 |           5.9792 |           4.5858 |
[32m[20230114 18:49:45 @agent_ppo2.py:189][0m |          -0.0123 |           5.6126 |           4.5877 |
[32m[20230114 18:49:45 @agent_ppo2.py:134][0m Policy update time: 0.79 s
[32m[20230114 18:49:45 @agent_ppo2.py:142][0m Average TRAINING episode reward: 260.05
[32m[20230114 18:49:45 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 261.78
[32m[20230114 18:49:45 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 267.50
[32m[20230114 18:49:45 @agent_ppo2.py:147][0m Total time:       7.68 min
[32m[20230114 18:49:45 @agent_ppo2.py:149][0m 628736 total steps have happened
[32m[20230114 18:49:45 @agent_ppo2.py:125][0m #------------------------ Iteration 307 --------------------------#
[32m[20230114 18:49:45 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:49:45 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:49:45 @agent_ppo2.py:189][0m |          -0.0022 |          13.8733 |           4.6923 |
[32m[20230114 18:49:45 @agent_ppo2.py:189][0m |          -0.0070 |          11.7059 |           4.6904 |
[32m[20230114 18:49:45 @agent_ppo2.py:189][0m |          -0.0089 |          11.1472 |           4.6859 |
[32m[20230114 18:49:45 @agent_ppo2.py:189][0m |          -0.0067 |          11.1380 |           4.6833 |
[32m[20230114 18:49:46 @agent_ppo2.py:189][0m |          -0.0090 |          10.3391 |           4.6803 |
[32m[20230114 18:49:46 @agent_ppo2.py:189][0m |          -0.0102 |          10.0633 |           4.6788 |
[32m[20230114 18:49:46 @agent_ppo2.py:189][0m |          -0.0120 |           9.7834 |           4.6819 |
[32m[20230114 18:49:46 @agent_ppo2.py:189][0m |          -0.0127 |           9.5165 |           4.6762 |
[32m[20230114 18:49:46 @agent_ppo2.py:189][0m |          -0.0115 |           9.4058 |           4.6763 |
[32m[20230114 18:49:46 @agent_ppo2.py:189][0m |          -0.0111 |           9.2883 |           4.6766 |
[32m[20230114 18:49:46 @agent_ppo2.py:134][0m Policy update time: 0.82 s
[32m[20230114 18:49:46 @agent_ppo2.py:142][0m Average TRAINING episode reward: 256.83
[32m[20230114 18:49:46 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 258.66
[32m[20230114 18:49:46 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 269.28
[32m[20230114 18:49:46 @agent_ppo2.py:147][0m Total time:       7.70 min
[32m[20230114 18:49:46 @agent_ppo2.py:149][0m 630784 total steps have happened
[32m[20230114 18:49:46 @agent_ppo2.py:125][0m #------------------------ Iteration 308 --------------------------#
[32m[20230114 18:49:46 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:49:46 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:49:47 @agent_ppo2.py:189][0m |          -0.0006 |          13.8473 |           4.5423 |
[32m[20230114 18:49:47 @agent_ppo2.py:189][0m |          -0.0055 |          11.7443 |           4.5366 |
[32m[20230114 18:49:47 @agent_ppo2.py:189][0m |          -0.0081 |          10.3413 |           4.5309 |
[32m[20230114 18:49:47 @agent_ppo2.py:189][0m |          -0.0021 |           9.9673 |           4.5347 |
[32m[20230114 18:49:47 @agent_ppo2.py:189][0m |          -0.0032 |           8.7139 |           4.5312 |
[32m[20230114 18:49:47 @agent_ppo2.py:189][0m |          -0.0084 |           8.0374 |           4.5312 |
[32m[20230114 18:49:47 @agent_ppo2.py:189][0m |          -0.0128 |           7.5686 |           4.5299 |
[32m[20230114 18:49:47 @agent_ppo2.py:189][0m |          -0.0057 |           7.1632 |           4.5275 |
[32m[20230114 18:49:47 @agent_ppo2.py:189][0m |          -0.0033 |           7.0101 |           4.5303 |
[32m[20230114 18:49:47 @agent_ppo2.py:189][0m |          -0.0127 |           6.3286 |           4.5280 |
[32m[20230114 18:49:47 @agent_ppo2.py:134][0m Policy update time: 0.83 s
[32m[20230114 18:49:47 @agent_ppo2.py:142][0m Average TRAINING episode reward: 257.81
[32m[20230114 18:49:47 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 261.59
[32m[20230114 18:49:47 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 271.63
[32m[20230114 18:49:47 @agent_ppo2.py:147][0m Total time:       7.72 min
[32m[20230114 18:49:47 @agent_ppo2.py:149][0m 632832 total steps have happened
[32m[20230114 18:49:47 @agent_ppo2.py:125][0m #------------------------ Iteration 309 --------------------------#
[32m[20230114 18:49:48 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:49:48 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:49:48 @agent_ppo2.py:189][0m |          -0.0024 |           8.9926 |           4.5692 |
[32m[20230114 18:49:48 @agent_ppo2.py:189][0m |          -0.0073 |           5.2289 |           4.5675 |
[32m[20230114 18:49:48 @agent_ppo2.py:189][0m |          -0.0092 |           3.9911 |           4.5672 |
[32m[20230114 18:49:48 @agent_ppo2.py:189][0m |          -0.0124 |           3.5410 |           4.5648 |
[32m[20230114 18:49:48 @agent_ppo2.py:189][0m |          -0.0117 |           3.2018 |           4.5602 |
[32m[20230114 18:49:48 @agent_ppo2.py:189][0m |          -0.0135 |           3.0153 |           4.5670 |
[32m[20230114 18:49:48 @agent_ppo2.py:189][0m |          -0.0129 |           2.8818 |           4.5637 |
[32m[20230114 18:49:48 @agent_ppo2.py:189][0m |          -0.0041 |           2.9221 |           4.5621 |
[32m[20230114 18:49:48 @agent_ppo2.py:189][0m |          -0.0138 |           2.6403 |           4.5626 |
[32m[20230114 18:49:49 @agent_ppo2.py:189][0m |          -0.0157 |           2.5112 |           4.5644 |
[32m[20230114 18:49:49 @agent_ppo2.py:134][0m Policy update time: 0.82 s
[32m[20230114 18:49:49 @agent_ppo2.py:142][0m Average TRAINING episode reward: 258.55
[32m[20230114 18:49:49 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 263.44
[32m[20230114 18:49:49 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 269.52
[32m[20230114 18:49:49 @agent_ppo2.py:147][0m Total time:       7.74 min
[32m[20230114 18:49:49 @agent_ppo2.py:149][0m 634880 total steps have happened
[32m[20230114 18:49:49 @agent_ppo2.py:125][0m #------------------------ Iteration 310 --------------------------#
[32m[20230114 18:49:49 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:49:49 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:49:49 @agent_ppo2.py:189][0m |          -0.0003 |          12.2129 |           4.6287 |
[32m[20230114 18:49:49 @agent_ppo2.py:189][0m |          -0.0028 |           9.8291 |           4.6310 |
[32m[20230114 18:49:49 @agent_ppo2.py:189][0m |          -0.0042 |           8.8453 |           4.6298 |
[32m[20230114 18:49:49 @agent_ppo2.py:189][0m |          -0.0020 |           8.6789 |           4.6206 |
[32m[20230114 18:49:49 @agent_ppo2.py:189][0m |          -0.0072 |           8.1184 |           4.6242 |
[32m[20230114 18:49:49 @agent_ppo2.py:189][0m |          -0.0039 |           7.7532 |           4.6246 |
[32m[20230114 18:49:50 @agent_ppo2.py:189][0m |          -0.0077 |           7.5999 |           4.6223 |
[32m[20230114 18:49:50 @agent_ppo2.py:189][0m |          -0.0078 |           7.3324 |           4.6227 |
[32m[20230114 18:49:50 @agent_ppo2.py:189][0m |          -0.0111 |           7.1385 |           4.6208 |
[32m[20230114 18:49:50 @agent_ppo2.py:189][0m |          -0.0057 |           7.1450 |           4.6233 |
[32m[20230114 18:49:50 @agent_ppo2.py:134][0m Policy update time: 0.85 s
[32m[20230114 18:49:50 @agent_ppo2.py:142][0m Average TRAINING episode reward: 257.18
[32m[20230114 18:49:50 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 260.48
[32m[20230114 18:49:50 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 270.58
[32m[20230114 18:49:50 @agent_ppo2.py:147][0m Total time:       7.76 min
[32m[20230114 18:49:50 @agent_ppo2.py:149][0m 636928 total steps have happened
[32m[20230114 18:49:50 @agent_ppo2.py:125][0m #------------------------ Iteration 311 --------------------------#
[32m[20230114 18:49:50 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:49:50 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:49:50 @agent_ppo2.py:189][0m |           0.0005 |          14.8320 |           4.4485 |
[32m[20230114 18:49:51 @agent_ppo2.py:189][0m |           0.0053 |          12.8583 |           4.4421 |
[32m[20230114 18:49:51 @agent_ppo2.py:189][0m |          -0.0352 |          11.8822 |           4.4463 |
[32m[20230114 18:49:51 @agent_ppo2.py:189][0m |          -0.0134 |          11.3681 |           4.4475 |
[32m[20230114 18:49:51 @agent_ppo2.py:189][0m |          -0.0023 |          11.1683 |           4.4469 |
[32m[20230114 18:49:51 @agent_ppo2.py:189][0m |           0.0017 |          10.8001 |           4.4377 |
[32m[20230114 18:49:51 @agent_ppo2.py:189][0m |          -0.0131 |          10.5484 |           4.4432 |
[32m[20230114 18:49:51 @agent_ppo2.py:189][0m |           0.0061 |          10.3684 |           4.4487 |
[32m[20230114 18:49:51 @agent_ppo2.py:189][0m |          -0.0123 |          10.3790 |           4.4409 |
[32m[20230114 18:49:51 @agent_ppo2.py:189][0m |          -0.0114 |          10.0844 |           4.4538 |
[32m[20230114 18:49:51 @agent_ppo2.py:134][0m Policy update time: 0.87 s
[32m[20230114 18:49:51 @agent_ppo2.py:142][0m Average TRAINING episode reward: 253.60
[32m[20230114 18:49:51 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 259.12
[32m[20230114 18:49:51 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 267.64
[32m[20230114 18:49:51 @agent_ppo2.py:147][0m Total time:       7.79 min
[32m[20230114 18:49:51 @agent_ppo2.py:149][0m 638976 total steps have happened
[32m[20230114 18:49:51 @agent_ppo2.py:125][0m #------------------------ Iteration 312 --------------------------#
[32m[20230114 18:49:52 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:49:52 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:49:52 @agent_ppo2.py:189][0m |           0.0007 |          10.2918 |           4.6673 |
[32m[20230114 18:49:52 @agent_ppo2.py:189][0m |          -0.0045 |           8.4198 |           4.6644 |
[32m[20230114 18:49:52 @agent_ppo2.py:189][0m |          -0.0077 |           7.6132 |           4.6633 |
[32m[20230114 18:49:52 @agent_ppo2.py:189][0m |          -0.0064 |           7.1169 |           4.6620 |
[32m[20230114 18:49:52 @agent_ppo2.py:189][0m |          -0.0098 |           6.7088 |           4.6618 |
[32m[20230114 18:49:52 @agent_ppo2.py:189][0m |          -0.0106 |           6.4573 |           4.6586 |
[32m[20230114 18:49:52 @agent_ppo2.py:189][0m |          -0.0110 |           6.2040 |           4.6629 |
[32m[20230114 18:49:52 @agent_ppo2.py:189][0m |          -0.0109 |           5.9832 |           4.6612 |
[32m[20230114 18:49:52 @agent_ppo2.py:189][0m |          -0.0116 |           5.8435 |           4.6636 |
[32m[20230114 18:49:52 @agent_ppo2.py:189][0m |          -0.0119 |           5.7393 |           4.6625 |
[32m[20230114 18:49:52 @agent_ppo2.py:134][0m Policy update time: 0.81 s
[32m[20230114 18:49:53 @agent_ppo2.py:142][0m Average TRAINING episode reward: 257.65
[32m[20230114 18:49:53 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 260.79
[32m[20230114 18:49:53 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 270.56
[32m[20230114 18:49:53 @agent_ppo2.py:147][0m Total time:       7.81 min
[32m[20230114 18:49:53 @agent_ppo2.py:149][0m 641024 total steps have happened
[32m[20230114 18:49:53 @agent_ppo2.py:125][0m #------------------------ Iteration 313 --------------------------#
[32m[20230114 18:49:53 @agent_ppo2.py:131][0m Sampling time: 0.28 s by 4 slaves
[32m[20230114 18:49:53 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:49:53 @agent_ppo2.py:189][0m |          -0.0015 |          19.5417 |           4.5783 |
[32m[20230114 18:49:53 @agent_ppo2.py:189][0m |          -0.0069 |          14.5677 |           4.5701 |
[32m[20230114 18:49:53 @agent_ppo2.py:189][0m |          -0.0102 |          13.7428 |           4.5706 |
[32m[20230114 18:49:53 @agent_ppo2.py:189][0m |          -0.0081 |          13.1493 |           4.5690 |
[32m[20230114 18:49:53 @agent_ppo2.py:189][0m |          -0.0058 |          12.8795 |           4.5668 |
[32m[20230114 18:49:53 @agent_ppo2.py:189][0m |          -0.0118 |          12.3992 |           4.5686 |
[32m[20230114 18:49:54 @agent_ppo2.py:189][0m |          -0.0068 |          12.3406 |           4.5700 |
[32m[20230114 18:49:54 @agent_ppo2.py:189][0m |          -0.0110 |          12.0008 |           4.5711 |
[32m[20230114 18:49:54 @agent_ppo2.py:189][0m |          -0.0014 |          11.7178 |           4.5675 |
[32m[20230114 18:49:54 @agent_ppo2.py:189][0m |          -0.0094 |          11.4375 |           4.5681 |
[32m[20230114 18:49:54 @agent_ppo2.py:134][0m Policy update time: 0.91 s
[32m[20230114 18:49:54 @agent_ppo2.py:142][0m Average TRAINING episode reward: 197.01
[32m[20230114 18:49:54 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 260.48
[32m[20230114 18:49:54 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 268.11
[32m[20230114 18:49:54 @agent_ppo2.py:147][0m Total time:       7.83 min
[32m[20230114 18:49:54 @agent_ppo2.py:149][0m 643072 total steps have happened
[32m[20230114 18:49:54 @agent_ppo2.py:125][0m #------------------------ Iteration 314 --------------------------#
[32m[20230114 18:49:54 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:49:54 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:49:54 @agent_ppo2.py:189][0m |          -0.0010 |          19.6605 |           4.7563 |
[32m[20230114 18:49:55 @agent_ppo2.py:189][0m |          -0.0057 |          16.0121 |           4.7434 |
[32m[20230114 18:49:55 @agent_ppo2.py:189][0m |          -0.0062 |          15.1084 |           4.7513 |
[32m[20230114 18:49:55 @agent_ppo2.py:189][0m |          -0.0094 |          14.4056 |           4.7493 |
[32m[20230114 18:49:55 @agent_ppo2.py:189][0m |          -0.0089 |          14.0074 |           4.7475 |
[32m[20230114 18:49:55 @agent_ppo2.py:189][0m |          -0.0100 |          13.6014 |           4.7477 |
[32m[20230114 18:49:55 @agent_ppo2.py:189][0m |          -0.0107 |          13.2767 |           4.7483 |
[32m[20230114 18:49:55 @agent_ppo2.py:189][0m |          -0.0112 |          13.1083 |           4.7486 |
[32m[20230114 18:49:55 @agent_ppo2.py:189][0m |          -0.0098 |          13.0840 |           4.7457 |
[32m[20230114 18:49:55 @agent_ppo2.py:189][0m |          -0.0110 |          12.8719 |           4.7511 |
[32m[20230114 18:49:55 @agent_ppo2.py:134][0m Policy update time: 0.83 s
[32m[20230114 18:49:55 @agent_ppo2.py:142][0m Average TRAINING episode reward: 259.52
[32m[20230114 18:49:55 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 260.91
[32m[20230114 18:49:55 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 270.98
[32m[20230114 18:49:55 @agent_ppo2.py:147][0m Total time:       7.85 min
[32m[20230114 18:49:55 @agent_ppo2.py:149][0m 645120 total steps have happened
[32m[20230114 18:49:55 @agent_ppo2.py:125][0m #------------------------ Iteration 315 --------------------------#
[32m[20230114 18:49:56 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:49:56 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:49:56 @agent_ppo2.py:189][0m |          -0.0006 |          27.5536 |           4.6864 |
[32m[20230114 18:49:56 @agent_ppo2.py:189][0m |          -0.0068 |          20.1045 |           4.6803 |
[32m[20230114 18:49:56 @agent_ppo2.py:189][0m |          -0.0083 |          18.0201 |           4.6782 |
[32m[20230114 18:49:56 @agent_ppo2.py:189][0m |          -0.0097 |          16.9442 |           4.6735 |
[32m[20230114 18:49:56 @agent_ppo2.py:189][0m |          -0.0103 |          16.1094 |           4.6769 |
[32m[20230114 18:49:56 @agent_ppo2.py:189][0m |          -0.0108 |          15.5266 |           4.6703 |
[32m[20230114 18:49:56 @agent_ppo2.py:189][0m |          -0.0119 |          14.9056 |           4.6711 |
[32m[20230114 18:49:56 @agent_ppo2.py:189][0m |          -0.0124 |          14.5154 |           4.6716 |
[32m[20230114 18:49:56 @agent_ppo2.py:189][0m |          -0.0132 |          14.1183 |           4.6702 |
[32m[20230114 18:49:56 @agent_ppo2.py:189][0m |          -0.0132 |          13.7345 |           4.6711 |
[32m[20230114 18:49:56 @agent_ppo2.py:134][0m Policy update time: 0.80 s
[32m[20230114 18:49:57 @agent_ppo2.py:142][0m Average TRAINING episode reward: 219.85
[32m[20230114 18:49:57 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 260.42
[32m[20230114 18:49:57 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 272.54
[32m[20230114 18:49:57 @agent_ppo2.py:147][0m Total time:       7.87 min
[32m[20230114 18:49:57 @agent_ppo2.py:149][0m 647168 total steps have happened
[32m[20230114 18:49:57 @agent_ppo2.py:125][0m #------------------------ Iteration 316 --------------------------#
[32m[20230114 18:49:57 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:49:57 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:49:57 @agent_ppo2.py:189][0m |           0.0014 |          14.2062 |           4.6468 |
[32m[20230114 18:49:57 @agent_ppo2.py:189][0m |          -0.0022 |          12.1768 |           4.6467 |
[32m[20230114 18:49:57 @agent_ppo2.py:189][0m |          -0.0047 |          11.6016 |           4.6419 |
[32m[20230114 18:49:57 @agent_ppo2.py:189][0m |          -0.0060 |          11.2910 |           4.6399 |
[32m[20230114 18:49:57 @agent_ppo2.py:189][0m |          -0.0075 |          11.0880 |           4.6389 |
[32m[20230114 18:49:57 @agent_ppo2.py:189][0m |          -0.0075 |          10.8667 |           4.6340 |
[32m[20230114 18:49:57 @agent_ppo2.py:189][0m |          -0.0085 |          10.7510 |           4.6338 |
[32m[20230114 18:49:57 @agent_ppo2.py:189][0m |          -0.0088 |          10.5892 |           4.6349 |
[32m[20230114 18:49:58 @agent_ppo2.py:189][0m |          -0.0082 |          10.4616 |           4.6285 |
[32m[20230114 18:49:58 @agent_ppo2.py:189][0m |          -0.0095 |          10.4005 |           4.6322 |
[32m[20230114 18:49:58 @agent_ppo2.py:134][0m Policy update time: 0.82 s
[32m[20230114 18:49:58 @agent_ppo2.py:142][0m Average TRAINING episode reward: 258.24
[32m[20230114 18:49:58 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 261.56
[32m[20230114 18:49:58 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 266.41
[32m[20230114 18:49:58 @agent_ppo2.py:147][0m Total time:       7.90 min
[32m[20230114 18:49:58 @agent_ppo2.py:149][0m 649216 total steps have happened
[32m[20230114 18:49:58 @agent_ppo2.py:125][0m #------------------------ Iteration 317 --------------------------#
[32m[20230114 18:49:58 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:49:58 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:49:58 @agent_ppo2.py:189][0m |           0.0226 |          15.9437 |           4.6434 |
[32m[20230114 18:49:58 @agent_ppo2.py:189][0m |          -0.0016 |          12.5593 |           4.6371 |
[32m[20230114 18:49:58 @agent_ppo2.py:189][0m |          -0.0087 |          11.3408 |           4.6471 |
[32m[20230114 18:49:58 @agent_ppo2.py:189][0m |          -0.0127 |          10.7305 |           4.6523 |
[32m[20230114 18:49:59 @agent_ppo2.py:189][0m |          -0.0072 |          10.2177 |           4.6510 |
[32m[20230114 18:49:59 @agent_ppo2.py:189][0m |          -0.0048 |           9.8594 |           4.6490 |
[32m[20230114 18:49:59 @agent_ppo2.py:189][0m |          -0.0128 |           9.4753 |           4.6520 |
[32m[20230114 18:49:59 @agent_ppo2.py:189][0m |          -0.0121 |           9.1938 |           4.6539 |
[32m[20230114 18:49:59 @agent_ppo2.py:189][0m |          -0.0039 |           9.8042 |           4.6513 |
[32m[20230114 18:49:59 @agent_ppo2.py:189][0m |          -0.0081 |           9.5224 |           4.6502 |
[32m[20230114 18:49:59 @agent_ppo2.py:134][0m Policy update time: 0.74 s
[32m[20230114 18:49:59 @agent_ppo2.py:142][0m Average TRAINING episode reward: 189.71
[32m[20230114 18:49:59 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 257.88
[32m[20230114 18:49:59 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 272.09
[32m[20230114 18:49:59 @agent_ppo2.py:147][0m Total time:       7.92 min
[32m[20230114 18:49:59 @agent_ppo2.py:149][0m 651264 total steps have happened
[32m[20230114 18:49:59 @agent_ppo2.py:125][0m #------------------------ Iteration 318 --------------------------#
[32m[20230114 18:49:59 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:49:59 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:49:59 @agent_ppo2.py:189][0m |           0.0008 |          12.8989 |           4.6577 |
[32m[20230114 18:50:00 @agent_ppo2.py:189][0m |          -0.0026 |          11.8784 |           4.6604 |
[32m[20230114 18:50:00 @agent_ppo2.py:189][0m |           0.0013 |          11.7538 |           4.6568 |
[32m[20230114 18:50:00 @agent_ppo2.py:189][0m |          -0.0068 |          11.2586 |           4.6583 |
[32m[20230114 18:50:00 @agent_ppo2.py:189][0m |          -0.0094 |          10.9758 |           4.6603 |
[32m[20230114 18:50:00 @agent_ppo2.py:189][0m |          -0.0095 |          10.7524 |           4.6585 |
[32m[20230114 18:50:00 @agent_ppo2.py:189][0m |          -0.0106 |          10.5235 |           4.6576 |
[32m[20230114 18:50:00 @agent_ppo2.py:189][0m |          -0.0105 |          10.2653 |           4.6552 |
[32m[20230114 18:50:00 @agent_ppo2.py:189][0m |          -0.0154 |          10.0929 |           4.6569 |
[32m[20230114 18:50:00 @agent_ppo2.py:189][0m |          -0.0157 |           9.9462 |           4.6542 |
[32m[20230114 18:50:00 @agent_ppo2.py:134][0m Policy update time: 0.82 s
[32m[20230114 18:50:00 @agent_ppo2.py:142][0m Average TRAINING episode reward: 258.04
[32m[20230114 18:50:00 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 260.66
[32m[20230114 18:50:00 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 272.11
[32m[20230114 18:50:00 @agent_ppo2.py:147][0m Total time:       7.94 min
[32m[20230114 18:50:00 @agent_ppo2.py:149][0m 653312 total steps have happened
[32m[20230114 18:50:00 @agent_ppo2.py:125][0m #------------------------ Iteration 319 --------------------------#
[32m[20230114 18:50:01 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:50:01 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:50:01 @agent_ppo2.py:189][0m |          -0.0029 |          11.3581 |           4.6621 |
[32m[20230114 18:50:01 @agent_ppo2.py:189][0m |          -0.0060 |           9.9760 |           4.6528 |
[32m[20230114 18:50:01 @agent_ppo2.py:189][0m |          -0.0098 |           9.3120 |           4.6520 |
[32m[20230114 18:50:01 @agent_ppo2.py:189][0m |          -0.0133 |           8.9210 |           4.6536 |
[32m[20230114 18:50:01 @agent_ppo2.py:189][0m |          -0.0143 |           8.6538 |           4.6512 |
[32m[20230114 18:50:01 @agent_ppo2.py:189][0m |          -0.0108 |           8.5574 |           4.6566 |
[32m[20230114 18:50:01 @agent_ppo2.py:189][0m |          -0.0152 |           8.2228 |           4.6546 |
[32m[20230114 18:50:01 @agent_ppo2.py:189][0m |          -0.0153 |           8.0375 |           4.6573 |
[32m[20230114 18:50:01 @agent_ppo2.py:189][0m |          -0.0161 |           7.9141 |           4.6532 |
[32m[20230114 18:50:01 @agent_ppo2.py:189][0m |          -0.0163 |           7.7058 |           4.6523 |
[32m[20230114 18:50:01 @agent_ppo2.py:134][0m Policy update time: 0.82 s
[32m[20230114 18:50:02 @agent_ppo2.py:142][0m Average TRAINING episode reward: 260.28
[32m[20230114 18:50:02 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 264.17
[32m[20230114 18:50:02 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 269.10
[32m[20230114 18:50:02 @agent_ppo2.py:147][0m Total time:       7.96 min
[32m[20230114 18:50:02 @agent_ppo2.py:149][0m 655360 total steps have happened
[32m[20230114 18:50:02 @agent_ppo2.py:125][0m #------------------------ Iteration 320 --------------------------#
[32m[20230114 18:50:02 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:50:02 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:50:02 @agent_ppo2.py:189][0m |          -0.0032 |          13.3950 |           4.7158 |
[32m[20230114 18:50:02 @agent_ppo2.py:189][0m |          -0.0080 |          11.9024 |           4.7083 |
[32m[20230114 18:50:02 @agent_ppo2.py:189][0m |          -0.0083 |          11.3859 |           4.7077 |
[32m[20230114 18:50:02 @agent_ppo2.py:189][0m |          -0.0108 |          11.1784 |           4.7067 |
[32m[20230114 18:50:02 @agent_ppo2.py:189][0m |          -0.0103 |          11.0210 |           4.7039 |
[32m[20230114 18:50:02 @agent_ppo2.py:189][0m |          -0.0129 |          10.7477 |           4.7017 |
[32m[20230114 18:50:02 @agent_ppo2.py:189][0m |          -0.0135 |          10.6364 |           4.6977 |
[32m[20230114 18:50:02 @agent_ppo2.py:189][0m |          -0.0145 |          10.5090 |           4.6967 |
[32m[20230114 18:50:03 @agent_ppo2.py:189][0m |          -0.0153 |          10.4120 |           4.6957 |
[32m[20230114 18:50:03 @agent_ppo2.py:189][0m |          -0.0140 |          10.3199 |           4.6964 |
[32m[20230114 18:50:03 @agent_ppo2.py:134][0m Policy update time: 0.81 s
[32m[20230114 18:50:03 @agent_ppo2.py:142][0m Average TRAINING episode reward: 261.54
[32m[20230114 18:50:03 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 263.39
[32m[20230114 18:50:03 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 273.20
[32m[20230114 18:50:03 @agent_ppo2.py:147][0m Total time:       7.98 min
[32m[20230114 18:50:03 @agent_ppo2.py:149][0m 657408 total steps have happened
[32m[20230114 18:50:03 @agent_ppo2.py:125][0m #------------------------ Iteration 321 --------------------------#
[32m[20230114 18:50:03 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:50:03 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:50:03 @agent_ppo2.py:189][0m |           0.0024 |          13.0481 |           4.7473 |
[32m[20230114 18:50:03 @agent_ppo2.py:189][0m |          -0.0027 |          12.2464 |           4.7447 |
[32m[20230114 18:50:03 @agent_ppo2.py:189][0m |          -0.0034 |          11.9332 |           4.7528 |
[32m[20230114 18:50:04 @agent_ppo2.py:189][0m |          -0.0061 |          11.6388 |           4.7502 |
[32m[20230114 18:50:04 @agent_ppo2.py:189][0m |          -0.0065 |          11.4340 |           4.7548 |
[32m[20230114 18:50:04 @agent_ppo2.py:189][0m |          -0.0066 |          11.2554 |           4.7485 |
[32m[20230114 18:50:04 @agent_ppo2.py:189][0m |          -0.0080 |          11.0551 |           4.7557 |
[32m[20230114 18:50:04 @agent_ppo2.py:189][0m |          -0.0092 |          10.8732 |           4.7593 |
[32m[20230114 18:50:04 @agent_ppo2.py:189][0m |          -0.0086 |          10.7554 |           4.7565 |
[32m[20230114 18:50:04 @agent_ppo2.py:189][0m |          -0.0104 |          10.6098 |           4.7569 |
[32m[20230114 18:50:04 @agent_ppo2.py:134][0m Policy update time: 0.83 s
[32m[20230114 18:50:04 @agent_ppo2.py:142][0m Average TRAINING episode reward: 257.68
[32m[20230114 18:50:04 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 259.74
[32m[20230114 18:50:04 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 268.39
[32m[20230114 18:50:04 @agent_ppo2.py:147][0m Total time:       8.00 min
[32m[20230114 18:50:04 @agent_ppo2.py:149][0m 659456 total steps have happened
[32m[20230114 18:50:04 @agent_ppo2.py:125][0m #------------------------ Iteration 322 --------------------------#
[32m[20230114 18:50:04 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:50:04 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:50:05 @agent_ppo2.py:189][0m |          -0.0023 |          18.9199 |           4.6301 |
[32m[20230114 18:50:05 @agent_ppo2.py:189][0m |          -0.0068 |          15.7036 |           4.6206 |
[32m[20230114 18:50:05 @agent_ppo2.py:189][0m |          -0.0015 |          14.9137 |           4.6183 |
[32m[20230114 18:50:05 @agent_ppo2.py:189][0m |          -0.0100 |          13.7373 |           4.6166 |
[32m[20230114 18:50:05 @agent_ppo2.py:189][0m |          -0.0101 |          13.3403 |           4.6197 |
[32m[20230114 18:50:05 @agent_ppo2.py:189][0m |          -0.0147 |          12.7708 |           4.6188 |
[32m[20230114 18:50:05 @agent_ppo2.py:189][0m |          -0.0114 |          12.4809 |           4.6227 |
[32m[20230114 18:50:05 @agent_ppo2.py:189][0m |          -0.0100 |          12.1778 |           4.6169 |
[32m[20230114 18:50:05 @agent_ppo2.py:189][0m |          -0.0146 |          12.0678 |           4.6178 |
[32m[20230114 18:50:05 @agent_ppo2.py:189][0m |          -0.0138 |          11.7047 |           4.6162 |
[32m[20230114 18:50:05 @agent_ppo2.py:134][0m Policy update time: 0.86 s
[32m[20230114 18:50:05 @agent_ppo2.py:142][0m Average TRAINING episode reward: 211.25
[32m[20230114 18:50:05 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 263.64
[32m[20230114 18:50:05 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 267.85
[32m[20230114 18:50:05 @agent_ppo2.py:147][0m Total time:       8.02 min
[32m[20230114 18:50:05 @agent_ppo2.py:149][0m 661504 total steps have happened
[32m[20230114 18:50:05 @agent_ppo2.py:125][0m #------------------------ Iteration 323 --------------------------#
[32m[20230114 18:50:06 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:50:06 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:50:06 @agent_ppo2.py:189][0m |          -0.0007 |          13.0042 |           4.7244 |
[32m[20230114 18:50:06 @agent_ppo2.py:189][0m |          -0.0072 |          11.9772 |           4.7166 |
[32m[20230114 18:50:06 @agent_ppo2.py:189][0m |          -0.0086 |          11.6358 |           4.7130 |
[32m[20230114 18:50:06 @agent_ppo2.py:189][0m |          -0.0099 |          11.3150 |           4.7111 |
[32m[20230114 18:50:06 @agent_ppo2.py:189][0m |          -0.0116 |          11.0849 |           4.7107 |
[32m[20230114 18:50:06 @agent_ppo2.py:189][0m |          -0.0117 |          10.8950 |           4.7107 |
[32m[20230114 18:50:06 @agent_ppo2.py:189][0m |          -0.0119 |          10.7092 |           4.7111 |
[32m[20230114 18:50:06 @agent_ppo2.py:189][0m |          -0.0130 |          10.5401 |           4.7115 |
[32m[20230114 18:50:07 @agent_ppo2.py:189][0m |          -0.0122 |          10.3993 |           4.7102 |
[32m[20230114 18:50:07 @agent_ppo2.py:189][0m |          -0.0141 |          10.2318 |           4.7138 |
[32m[20230114 18:50:07 @agent_ppo2.py:134][0m Policy update time: 0.88 s
[32m[20230114 18:50:07 @agent_ppo2.py:142][0m Average TRAINING episode reward: 258.87
[32m[20230114 18:50:07 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 264.39
[32m[20230114 18:50:07 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 269.52
[32m[20230114 18:50:07 @agent_ppo2.py:147][0m Total time:       8.05 min
[32m[20230114 18:50:07 @agent_ppo2.py:149][0m 663552 total steps have happened
[32m[20230114 18:50:07 @agent_ppo2.py:125][0m #------------------------ Iteration 324 --------------------------#
[32m[20230114 18:50:07 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:50:07 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:50:07 @agent_ppo2.py:189][0m |          -0.0071 |          13.3513 |           4.6947 |
[32m[20230114 18:50:07 @agent_ppo2.py:189][0m |          -0.0049 |          10.2515 |           4.6857 |
[32m[20230114 18:50:07 @agent_ppo2.py:189][0m |           0.0479 |          15.9536 |           4.6855 |
[32m[20230114 18:50:08 @agent_ppo2.py:189][0m |          -0.0057 |           9.1858 |           4.6659 |
[32m[20230114 18:50:08 @agent_ppo2.py:189][0m |          -0.0102 |           8.3523 |           4.6687 |
[32m[20230114 18:50:08 @agent_ppo2.py:189][0m |          -0.0163 |           7.8030 |           4.6676 |
[32m[20230114 18:50:08 @agent_ppo2.py:189][0m |          -0.0113 |           7.4624 |           4.6672 |
[32m[20230114 18:50:08 @agent_ppo2.py:189][0m |          -0.0062 |           7.1706 |           4.6703 |
[32m[20230114 18:50:08 @agent_ppo2.py:189][0m |          -0.0122 |           6.8503 |           4.6700 |
[32m[20230114 18:50:08 @agent_ppo2.py:189][0m |          -0.0117 |           6.6274 |           4.6694 |
[32m[20230114 18:50:08 @agent_ppo2.py:134][0m Policy update time: 0.97 s
[32m[20230114 18:50:08 @agent_ppo2.py:142][0m Average TRAINING episode reward: 259.91
[32m[20230114 18:50:08 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 261.44
[32m[20230114 18:50:08 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 271.62
[32m[20230114 18:50:08 @agent_ppo2.py:147][0m Total time:       8.07 min
[32m[20230114 18:50:08 @agent_ppo2.py:149][0m 665600 total steps have happened
[32m[20230114 18:50:08 @agent_ppo2.py:125][0m #------------------------ Iteration 325 --------------------------#
[32m[20230114 18:50:09 @agent_ppo2.py:131][0m Sampling time: 0.31 s by 4 slaves
[32m[20230114 18:50:09 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:50:09 @agent_ppo2.py:189][0m |           0.0027 |          21.3959 |           4.6657 |
[32m[20230114 18:50:09 @agent_ppo2.py:189][0m |          -0.0039 |          16.4987 |           4.6539 |
[32m[20230114 18:50:09 @agent_ppo2.py:189][0m |          -0.0030 |          15.1130 |           4.6617 |
[32m[20230114 18:50:09 @agent_ppo2.py:189][0m |          -0.0098 |          13.8601 |           4.6596 |
[32m[20230114 18:50:09 @agent_ppo2.py:189][0m |          -0.0079 |          13.2129 |           4.6585 |
[32m[20230114 18:50:09 @agent_ppo2.py:189][0m |          -0.0101 |          12.2711 |           4.6594 |
[32m[20230114 18:50:09 @agent_ppo2.py:189][0m |          -0.0104 |          11.5697 |           4.6577 |
[32m[20230114 18:50:09 @agent_ppo2.py:189][0m |          -0.0117 |          11.1526 |           4.6561 |
[32m[20230114 18:50:09 @agent_ppo2.py:189][0m |          -0.0118 |          10.8215 |           4.6560 |
[32m[20230114 18:50:10 @agent_ppo2.py:189][0m |          -0.0111 |          10.5777 |           4.6568 |
[32m[20230114 18:50:10 @agent_ppo2.py:134][0m Policy update time: 0.94 s
[32m[20230114 18:50:10 @agent_ppo2.py:142][0m Average TRAINING episode reward: 202.57
[32m[20230114 18:50:10 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 262.36
[32m[20230114 18:50:10 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 267.16
[32m[20230114 18:50:10 @agent_ppo2.py:147][0m Total time:       8.09 min
[32m[20230114 18:50:10 @agent_ppo2.py:149][0m 667648 total steps have happened
[32m[20230114 18:50:10 @agent_ppo2.py:125][0m #------------------------ Iteration 326 --------------------------#
[32m[20230114 18:50:10 @agent_ppo2.py:131][0m Sampling time: 0.26 s by 4 slaves
[32m[20230114 18:50:10 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:50:10 @agent_ppo2.py:189][0m |           0.0055 |          16.7451 |           4.6688 |
[32m[20230114 18:50:10 @agent_ppo2.py:189][0m |          -0.0072 |          13.1456 |           4.6664 |
[32m[20230114 18:50:10 @agent_ppo2.py:189][0m |          -0.0013 |          12.5685 |           4.6621 |
[32m[20230114 18:50:10 @agent_ppo2.py:189][0m |          -0.0058 |          11.9264 |           4.6619 |
[32m[20230114 18:50:10 @agent_ppo2.py:189][0m |          -0.0090 |          11.5336 |           4.6606 |
[32m[20230114 18:50:11 @agent_ppo2.py:189][0m |          -0.0112 |          11.3309 |           4.6640 |
[32m[20230114 18:50:11 @agent_ppo2.py:189][0m |          -0.0119 |          11.1034 |           4.6593 |
[32m[20230114 18:50:11 @agent_ppo2.py:189][0m |          -0.0155 |          10.8560 |           4.6562 |
[32m[20230114 18:50:11 @agent_ppo2.py:189][0m |          -0.0124 |          10.6290 |           4.6577 |
[32m[20230114 18:50:11 @agent_ppo2.py:189][0m |          -0.0017 |          12.9196 |           4.6568 |
[32m[20230114 18:50:11 @agent_ppo2.py:134][0m Policy update time: 0.82 s
[32m[20230114 18:50:11 @agent_ppo2.py:142][0m Average TRAINING episode reward: 186.72
[32m[20230114 18:50:11 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 263.34
[32m[20230114 18:50:11 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 273.10
[32m[20230114 18:50:11 @agent_ppo2.py:147][0m Total time:       8.11 min
[32m[20230114 18:50:11 @agent_ppo2.py:149][0m 669696 total steps have happened
[32m[20230114 18:50:11 @agent_ppo2.py:125][0m #------------------------ Iteration 327 --------------------------#
[32m[20230114 18:50:11 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:50:11 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:50:11 @agent_ppo2.py:189][0m |           0.0005 |          14.4935 |           4.8049 |
[32m[20230114 18:50:11 @agent_ppo2.py:189][0m |          -0.0042 |          13.5679 |           4.8072 |
[32m[20230114 18:50:12 @agent_ppo2.py:189][0m |          -0.0064 |          13.0396 |           4.8018 |
[32m[20230114 18:50:12 @agent_ppo2.py:189][0m |          -0.0071 |          12.6245 |           4.8052 |
[32m[20230114 18:50:12 @agent_ppo2.py:189][0m |          -0.0078 |          12.1215 |           4.7996 |
[32m[20230114 18:50:12 @agent_ppo2.py:189][0m |          -0.0085 |          11.6375 |           4.8009 |
[32m[20230114 18:50:12 @agent_ppo2.py:189][0m |          -0.0093 |          11.2670 |           4.7984 |
[32m[20230114 18:50:12 @agent_ppo2.py:189][0m |          -0.0095 |          10.9495 |           4.7963 |
[32m[20230114 18:50:12 @agent_ppo2.py:189][0m |          -0.0094 |          10.6567 |           4.7976 |
[32m[20230114 18:50:12 @agent_ppo2.py:189][0m |          -0.0099 |          10.4452 |           4.7950 |
[32m[20230114 18:50:12 @agent_ppo2.py:134][0m Policy update time: 0.82 s
[32m[20230114 18:50:12 @agent_ppo2.py:142][0m Average TRAINING episode reward: 257.53
[32m[20230114 18:50:12 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 262.05
[32m[20230114 18:50:12 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 268.44
[32m[20230114 18:50:12 @agent_ppo2.py:147][0m Total time:       8.14 min
[32m[20230114 18:50:12 @agent_ppo2.py:149][0m 671744 total steps have happened
[32m[20230114 18:50:12 @agent_ppo2.py:125][0m #------------------------ Iteration 328 --------------------------#
[32m[20230114 18:50:13 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:50:13 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:50:13 @agent_ppo2.py:189][0m |           0.0013 |          10.5703 |           4.6954 |
[32m[20230114 18:50:13 @agent_ppo2.py:189][0m |          -0.0031 |           8.0436 |           4.6938 |
[32m[20230114 18:50:13 @agent_ppo2.py:189][0m |          -0.0060 |           7.1434 |           4.6944 |
[32m[20230114 18:50:13 @agent_ppo2.py:189][0m |          -0.0079 |           6.5902 |           4.6923 |
[32m[20230114 18:50:13 @agent_ppo2.py:189][0m |          -0.0102 |           6.2148 |           4.6884 |
[32m[20230114 18:50:13 @agent_ppo2.py:189][0m |          -0.0095 |           5.9624 |           4.6902 |
[32m[20230114 18:50:13 @agent_ppo2.py:189][0m |          -0.0110 |           5.7339 |           4.6891 |
[32m[20230114 18:50:13 @agent_ppo2.py:189][0m |          -0.0116 |           5.5106 |           4.6874 |
[32m[20230114 18:50:13 @agent_ppo2.py:189][0m |          -0.0126 |           5.3237 |           4.6903 |
[32m[20230114 18:50:13 @agent_ppo2.py:189][0m |          -0.0132 |           5.1604 |           4.6907 |
[32m[20230114 18:50:13 @agent_ppo2.py:134][0m Policy update time: 0.79 s
[32m[20230114 18:50:14 @agent_ppo2.py:142][0m Average TRAINING episode reward: 261.24
[32m[20230114 18:50:14 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 264.57
[32m[20230114 18:50:14 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 270.32
[32m[20230114 18:50:14 @agent_ppo2.py:147][0m Total time:       8.16 min
[32m[20230114 18:50:14 @agent_ppo2.py:149][0m 673792 total steps have happened
[32m[20230114 18:50:14 @agent_ppo2.py:125][0m #------------------------ Iteration 329 --------------------------#
[32m[20230114 18:50:14 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:50:14 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:50:14 @agent_ppo2.py:189][0m |           0.0009 |          14.4062 |           4.7671 |
[32m[20230114 18:50:14 @agent_ppo2.py:189][0m |          -0.0049 |          12.4875 |           4.7638 |
[32m[20230114 18:50:14 @agent_ppo2.py:189][0m |          -0.0042 |          11.9100 |           4.7615 |
[32m[20230114 18:50:14 @agent_ppo2.py:189][0m |          -0.0061 |          11.3744 |           4.7569 |
[32m[20230114 18:50:14 @agent_ppo2.py:189][0m |          -0.0069 |          10.9551 |           4.7566 |
[32m[20230114 18:50:14 @agent_ppo2.py:189][0m |          -0.0080 |          10.7222 |           4.7537 |
[32m[20230114 18:50:14 @agent_ppo2.py:189][0m |          -0.0076 |          10.4602 |           4.7512 |
[32m[20230114 18:50:14 @agent_ppo2.py:189][0m |          -0.0088 |          10.2589 |           4.7485 |
[32m[20230114 18:50:14 @agent_ppo2.py:189][0m |          -0.0094 |          10.0929 |           4.7451 |
[32m[20230114 18:50:15 @agent_ppo2.py:189][0m |          -0.0095 |           9.9281 |           4.7497 |
[32m[20230114 18:50:15 @agent_ppo2.py:134][0m Policy update time: 0.80 s
[32m[20230114 18:50:15 @agent_ppo2.py:142][0m Average TRAINING episode reward: 258.72
[32m[20230114 18:50:15 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 263.23
[32m[20230114 18:50:15 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 270.05
[32m[20230114 18:50:15 @agent_ppo2.py:147][0m Total time:       8.18 min
[32m[20230114 18:50:15 @agent_ppo2.py:149][0m 675840 total steps have happened
[32m[20230114 18:50:15 @agent_ppo2.py:125][0m #------------------------ Iteration 330 --------------------------#
[32m[20230114 18:50:15 @agent_ppo2.py:131][0m Sampling time: 0.29 s by 4 slaves
[32m[20230114 18:50:15 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:50:15 @agent_ppo2.py:189][0m |          -0.0014 |          24.0880 |           4.6780 |
[32m[20230114 18:50:15 @agent_ppo2.py:189][0m |          -0.0058 |          19.7649 |           4.6787 |
[32m[20230114 18:50:15 @agent_ppo2.py:189][0m |          -0.0062 |          18.0006 |           4.6774 |
[32m[20230114 18:50:16 @agent_ppo2.py:189][0m |          -0.0084 |          16.8669 |           4.6760 |
[32m[20230114 18:50:16 @agent_ppo2.py:189][0m |          -0.0085 |          16.1666 |           4.6788 |
[32m[20230114 18:50:16 @agent_ppo2.py:189][0m |          -0.0090 |          15.6011 |           4.6771 |
[32m[20230114 18:50:16 @agent_ppo2.py:189][0m |          -0.0112 |          14.9369 |           4.6802 |
[32m[20230114 18:50:16 @agent_ppo2.py:189][0m |          -0.0118 |          14.5200 |           4.6799 |
[32m[20230114 18:50:16 @agent_ppo2.py:189][0m |          -0.0115 |          14.1604 |           4.6799 |
[32m[20230114 18:50:16 @agent_ppo2.py:189][0m |          -0.0123 |          13.7745 |           4.6841 |
[32m[20230114 18:50:16 @agent_ppo2.py:134][0m Policy update time: 0.87 s
[32m[20230114 18:50:16 @agent_ppo2.py:142][0m Average TRAINING episode reward: 199.99
[32m[20230114 18:50:16 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 261.17
[32m[20230114 18:50:16 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 270.84
[32m[20230114 18:50:16 @agent_ppo2.py:147][0m Total time:       8.20 min
[32m[20230114 18:50:16 @agent_ppo2.py:149][0m 677888 total steps have happened
[32m[20230114 18:50:16 @agent_ppo2.py:125][0m #------------------------ Iteration 331 --------------------------#
[32m[20230114 18:50:16 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:50:16 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:50:17 @agent_ppo2.py:189][0m |          -0.0008 |          15.0240 |           4.7335 |
[32m[20230114 18:50:17 @agent_ppo2.py:189][0m |          -0.0076 |          13.2798 |           4.7281 |
[32m[20230114 18:50:17 @agent_ppo2.py:189][0m |          -0.0031 |          13.4066 |           4.7301 |
[32m[20230114 18:50:17 @agent_ppo2.py:189][0m |          -0.0081 |          12.3260 |           4.7301 |
[32m[20230114 18:50:17 @agent_ppo2.py:189][0m |          -0.0076 |          12.1015 |           4.7335 |
[32m[20230114 18:50:17 @agent_ppo2.py:189][0m |          -0.0123 |          11.9319 |           4.7310 |
[32m[20230114 18:50:17 @agent_ppo2.py:189][0m |          -0.0110 |          11.8864 |           4.7275 |
[32m[20230114 18:50:17 @agent_ppo2.py:189][0m |          -0.0079 |          11.9809 |           4.7303 |
[32m[20230114 18:50:17 @agent_ppo2.py:189][0m |          -0.0121 |          11.5496 |           4.7320 |
[32m[20230114 18:50:17 @agent_ppo2.py:189][0m |          -0.0125 |          11.5180 |           4.7283 |
[32m[20230114 18:50:17 @agent_ppo2.py:134][0m Policy update time: 0.82 s
[32m[20230114 18:50:17 @agent_ppo2.py:142][0m Average TRAINING episode reward: 257.98
[32m[20230114 18:50:17 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 262.52
[32m[20230114 18:50:17 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 269.50
[32m[20230114 18:50:17 @agent_ppo2.py:147][0m Total time:       8.22 min
[32m[20230114 18:50:17 @agent_ppo2.py:149][0m 679936 total steps have happened
[32m[20230114 18:50:17 @agent_ppo2.py:125][0m #------------------------ Iteration 332 --------------------------#
[32m[20230114 18:50:18 @agent_ppo2.py:131][0m Sampling time: 0.25 s by 4 slaves
[32m[20230114 18:50:18 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:50:18 @agent_ppo2.py:189][0m |           0.0000 |          12.6805 |           4.7012 |
[32m[20230114 18:50:18 @agent_ppo2.py:189][0m |          -0.0031 |          11.4974 |           4.6984 |
[32m[20230114 18:50:18 @agent_ppo2.py:189][0m |          -0.0043 |          10.7032 |           4.6947 |
[32m[20230114 18:50:18 @agent_ppo2.py:189][0m |          -0.0044 |          10.2136 |           4.6962 |
[32m[20230114 18:50:18 @agent_ppo2.py:189][0m |          -0.0068 |           9.8045 |           4.6919 |
[32m[20230114 18:50:18 @agent_ppo2.py:189][0m |          -0.0078 |           9.5622 |           4.6893 |
[32m[20230114 18:50:18 @agent_ppo2.py:189][0m |          -0.0074 |           9.2544 |           4.6906 |
[32m[20230114 18:50:18 @agent_ppo2.py:189][0m |          -0.0082 |           9.0081 |           4.6925 |
[32m[20230114 18:50:18 @agent_ppo2.py:189][0m |          -0.0089 |           8.8553 |           4.6964 |
[32m[20230114 18:50:18 @agent_ppo2.py:189][0m |          -0.0093 |           8.6545 |           4.6913 |
[32m[20230114 18:50:18 @agent_ppo2.py:134][0m Policy update time: 0.80 s
[32m[20230114 18:50:19 @agent_ppo2.py:142][0m Average TRAINING episode reward: 258.78
[32m[20230114 18:50:19 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 262.13
[32m[20230114 18:50:19 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 270.17
[32m[20230114 18:50:19 @agent_ppo2.py:147][0m Total time:       8.24 min
[32m[20230114 18:50:19 @agent_ppo2.py:149][0m 681984 total steps have happened
[32m[20230114 18:50:19 @agent_ppo2.py:125][0m #------------------------ Iteration 333 --------------------------#
[32m[20230114 18:50:19 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:50:19 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:50:19 @agent_ppo2.py:189][0m |          -0.0012 |          11.8978 |           4.6546 |
[32m[20230114 18:50:19 @agent_ppo2.py:189][0m |          -0.0115 |           9.7499 |           4.6500 |
[32m[20230114 18:50:19 @agent_ppo2.py:189][0m |          -0.0093 |           9.1491 |           4.6472 |
[32m[20230114 18:50:19 @agent_ppo2.py:189][0m |          -0.0085 |           8.7829 |           4.6450 |
[32m[20230114 18:50:19 @agent_ppo2.py:189][0m |          -0.0126 |           8.2972 |           4.6484 |
[32m[20230114 18:50:19 @agent_ppo2.py:189][0m |           0.0033 |           8.6857 |           4.6419 |
[32m[20230114 18:50:20 @agent_ppo2.py:189][0m |          -0.0103 |           7.7504 |           4.6403 |
[32m[20230114 18:50:20 @agent_ppo2.py:189][0m |          -0.0112 |           7.3384 |           4.6389 |
[32m[20230114 18:50:20 @agent_ppo2.py:189][0m |          -0.0127 |           7.1179 |           4.6415 |
[32m[20230114 18:50:20 @agent_ppo2.py:189][0m |          -0.0138 |           6.9064 |           4.6459 |
[32m[20230114 18:50:20 @agent_ppo2.py:134][0m Policy update time: 0.80 s
[32m[20230114 18:50:20 @agent_ppo2.py:142][0m Average TRAINING episode reward: 261.55
[32m[20230114 18:50:20 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 263.90
[32m[20230114 18:50:20 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 270.74
[32m[20230114 18:50:20 @agent_ppo2.py:147][0m Total time:       8.26 min
[32m[20230114 18:50:20 @agent_ppo2.py:149][0m 684032 total steps have happened
[32m[20230114 18:50:20 @agent_ppo2.py:125][0m #------------------------ Iteration 334 --------------------------#
[32m[20230114 18:50:20 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:50:20 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:50:20 @agent_ppo2.py:189][0m |           0.0021 |          13.0472 |           4.7555 |
[32m[20230114 18:50:20 @agent_ppo2.py:189][0m |          -0.0032 |          11.4456 |           4.7596 |
[32m[20230114 18:50:20 @agent_ppo2.py:189][0m |          -0.0054 |          10.9200 |           4.7551 |
[32m[20230114 18:50:21 @agent_ppo2.py:189][0m |          -0.0065 |          10.5950 |           4.7577 |
[32m[20230114 18:50:21 @agent_ppo2.py:189][0m |          -0.0071 |          10.3610 |           4.7564 |
[32m[20230114 18:50:21 @agent_ppo2.py:189][0m |          -0.0080 |          10.1716 |           4.7596 |
[32m[20230114 18:50:21 @agent_ppo2.py:189][0m |          -0.0083 |          10.0171 |           4.7540 |
[32m[20230114 18:50:21 @agent_ppo2.py:189][0m |          -0.0096 |           9.8996 |           4.7579 |
[32m[20230114 18:50:21 @agent_ppo2.py:189][0m |          -0.0081 |           9.9576 |           4.7571 |
[32m[20230114 18:50:21 @agent_ppo2.py:189][0m |          -0.0109 |           9.6298 |           4.7587 |
[32m[20230114 18:50:21 @agent_ppo2.py:134][0m Policy update time: 0.82 s
[32m[20230114 18:50:21 @agent_ppo2.py:142][0m Average TRAINING episode reward: 257.91
[32m[20230114 18:50:21 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 262.39
[32m[20230114 18:50:21 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 267.68
[32m[20230114 18:50:21 @agent_ppo2.py:147][0m Total time:       8.28 min
[32m[20230114 18:50:21 @agent_ppo2.py:149][0m 686080 total steps have happened
[32m[20230114 18:50:21 @agent_ppo2.py:125][0m #------------------------ Iteration 335 --------------------------#
[32m[20230114 18:50:21 @agent_ppo2.py:131][0m Sampling time: 0.25 s by 4 slaves
[32m[20230114 18:50:22 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:50:22 @agent_ppo2.py:189][0m |           0.0023 |          12.8065 |           4.8056 |
[32m[20230114 18:50:22 @agent_ppo2.py:189][0m |          -0.0029 |          11.2585 |           4.8056 |
[32m[20230114 18:50:22 @agent_ppo2.py:189][0m |          -0.0045 |          10.6493 |           4.8073 |
[32m[20230114 18:50:22 @agent_ppo2.py:189][0m |          -0.0056 |          10.1980 |           4.8067 |
[32m[20230114 18:50:22 @agent_ppo2.py:189][0m |          -0.0061 |           9.9063 |           4.8030 |
[32m[20230114 18:50:22 @agent_ppo2.py:189][0m |          -0.0064 |           9.7156 |           4.8103 |
[32m[20230114 18:50:22 @agent_ppo2.py:189][0m |          -0.0085 |           9.4226 |           4.8047 |
[32m[20230114 18:50:22 @agent_ppo2.py:189][0m |          -0.0086 |           9.2407 |           4.8020 |
[32m[20230114 18:50:22 @agent_ppo2.py:189][0m |          -0.0093 |           9.0449 |           4.8067 |
[32m[20230114 18:50:22 @agent_ppo2.py:189][0m |          -0.0084 |           8.9284 |           4.8017 |
[32m[20230114 18:50:22 @agent_ppo2.py:134][0m Policy update time: 0.79 s
[32m[20230114 18:50:22 @agent_ppo2.py:142][0m Average TRAINING episode reward: 260.18
[32m[20230114 18:50:22 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 263.85
[32m[20230114 18:50:22 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 271.83
[32m[20230114 18:50:22 @agent_ppo2.py:147][0m Total time:       8.31 min
[32m[20230114 18:50:22 @agent_ppo2.py:149][0m 688128 total steps have happened
[32m[20230114 18:50:22 @agent_ppo2.py:125][0m #------------------------ Iteration 336 --------------------------#
[32m[20230114 18:50:23 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:50:23 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:50:23 @agent_ppo2.py:189][0m |           0.0012 |          14.1986 |           4.8463 |
[32m[20230114 18:50:23 @agent_ppo2.py:189][0m |          -0.0042 |          12.3870 |           4.8381 |
[32m[20230114 18:50:23 @agent_ppo2.py:189][0m |          -0.0067 |          11.7834 |           4.8395 |
[32m[20230114 18:50:23 @agent_ppo2.py:189][0m |          -0.0075 |          11.3650 |           4.8357 |
[32m[20230114 18:50:23 @agent_ppo2.py:189][0m |          -0.0083 |          10.9807 |           4.8336 |
[32m[20230114 18:50:23 @agent_ppo2.py:189][0m |          -0.0096 |          10.5293 |           4.8303 |
[32m[20230114 18:50:23 @agent_ppo2.py:189][0m |          -0.0106 |          10.3178 |           4.8342 |
[32m[20230114 18:50:23 @agent_ppo2.py:189][0m |          -0.0113 |          10.0750 |           4.8293 |
[32m[20230114 18:50:23 @agent_ppo2.py:189][0m |          -0.0119 |           9.9000 |           4.8302 |
[32m[20230114 18:50:24 @agent_ppo2.py:189][0m |          -0.0129 |           9.7084 |           4.8330 |
[32m[20230114 18:50:24 @agent_ppo2.py:134][0m Policy update time: 0.83 s
[32m[20230114 18:50:24 @agent_ppo2.py:142][0m Average TRAINING episode reward: 257.61
[32m[20230114 18:50:24 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 261.00
[32m[20230114 18:50:24 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 271.16
[32m[20230114 18:50:24 @agent_ppo2.py:147][0m Total time:       8.33 min
[32m[20230114 18:50:24 @agent_ppo2.py:149][0m 690176 total steps have happened
[32m[20230114 18:50:24 @agent_ppo2.py:125][0m #------------------------ Iteration 337 --------------------------#
[32m[20230114 18:50:24 @agent_ppo2.py:131][0m Sampling time: 0.30 s by 4 slaves
[32m[20230114 18:50:24 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:50:24 @agent_ppo2.py:189][0m |           0.0011 |          16.9100 |           4.7474 |
[32m[20230114 18:50:24 @agent_ppo2.py:189][0m |          -0.0039 |          13.3366 |           4.7376 |
[32m[20230114 18:50:24 @agent_ppo2.py:189][0m |          -0.0053 |          12.7727 |           4.7312 |
[32m[20230114 18:50:24 @agent_ppo2.py:189][0m |          -0.0076 |          12.1994 |           4.7363 |
[32m[20230114 18:50:25 @agent_ppo2.py:189][0m |          -0.0082 |          11.8790 |           4.7274 |
[32m[20230114 18:50:25 @agent_ppo2.py:189][0m |          -0.0088 |          11.8816 |           4.7291 |
[32m[20230114 18:50:25 @agent_ppo2.py:189][0m |          -0.0107 |          11.4491 |           4.7157 |
[32m[20230114 18:50:25 @agent_ppo2.py:189][0m |          -0.0108 |          11.3161 |           4.7235 |
[32m[20230114 18:50:25 @agent_ppo2.py:189][0m |          -0.0071 |          11.8051 |           4.7217 |
[32m[20230114 18:50:25 @agent_ppo2.py:189][0m |          -0.0127 |          11.0174 |           4.7192 |
[32m[20230114 18:50:25 @agent_ppo2.py:134][0m Policy update time: 0.93 s
[32m[20230114 18:50:25 @agent_ppo2.py:142][0m Average TRAINING episode reward: 201.85
[32m[20230114 18:50:25 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 261.54
[32m[20230114 18:50:25 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 271.17
[32m[20230114 18:50:25 @agent_ppo2.py:147][0m Total time:       8.35 min
[32m[20230114 18:50:25 @agent_ppo2.py:149][0m 692224 total steps have happened
[32m[20230114 18:50:25 @agent_ppo2.py:125][0m #------------------------ Iteration 338 --------------------------#
[32m[20230114 18:50:25 @agent_ppo2.py:131][0m Sampling time: 0.25 s by 4 slaves
[32m[20230114 18:50:26 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:50:26 @agent_ppo2.py:189][0m |          -0.0009 |          11.4805 |           4.7902 |
[32m[20230114 18:50:26 @agent_ppo2.py:189][0m |          -0.0027 |          10.8374 |           4.7828 |
[32m[20230114 18:50:26 @agent_ppo2.py:189][0m |          -0.0019 |          10.6160 |           4.7791 |
[32m[20230114 18:50:26 @agent_ppo2.py:189][0m |          -0.0080 |          10.2224 |           4.7784 |
[32m[20230114 18:50:26 @agent_ppo2.py:189][0m |          -0.0080 |          10.0813 |           4.7749 |
[32m[20230114 18:50:26 @agent_ppo2.py:189][0m |          -0.0091 |           9.9644 |           4.7743 |
[32m[20230114 18:50:26 @agent_ppo2.py:189][0m |          -0.0091 |           9.8396 |           4.7741 |
[32m[20230114 18:50:26 @agent_ppo2.py:189][0m |          -0.0086 |           9.7679 |           4.7684 |
[32m[20230114 18:50:26 @agent_ppo2.py:189][0m |          -0.0023 |          10.2847 |           4.7715 |
[32m[20230114 18:50:26 @agent_ppo2.py:189][0m |          -0.0058 |           9.7060 |           4.7712 |
[32m[20230114 18:50:26 @agent_ppo2.py:134][0m Policy update time: 0.81 s
[32m[20230114 18:50:26 @agent_ppo2.py:142][0m Average TRAINING episode reward: 260.72
[32m[20230114 18:50:26 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 264.96
[32m[20230114 18:50:26 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 267.94
[32m[20230114 18:50:26 @agent_ppo2.py:147][0m Total time:       8.37 min
[32m[20230114 18:50:26 @agent_ppo2.py:149][0m 694272 total steps have happened
[32m[20230114 18:50:26 @agent_ppo2.py:125][0m #------------------------ Iteration 339 --------------------------#
[32m[20230114 18:50:27 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:50:27 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:50:27 @agent_ppo2.py:189][0m |          -0.0011 |          12.5104 |           4.7863 |
[32m[20230114 18:50:27 @agent_ppo2.py:189][0m |          -0.0037 |          11.1724 |           4.7838 |
[32m[20230114 18:50:27 @agent_ppo2.py:189][0m |          -0.0040 |          10.7932 |           4.7847 |
[32m[20230114 18:50:27 @agent_ppo2.py:189][0m |          -0.0038 |          10.4510 |           4.7849 |
[32m[20230114 18:50:27 @agent_ppo2.py:189][0m |          -0.0102 |          10.2085 |           4.7816 |
[32m[20230114 18:50:27 @agent_ppo2.py:189][0m |          -0.0098 |          10.0595 |           4.7841 |
[32m[20230114 18:50:27 @agent_ppo2.py:189][0m |          -0.0092 |           9.9076 |           4.7775 |
[32m[20230114 18:50:27 @agent_ppo2.py:189][0m |          -0.0127 |           9.7485 |           4.7815 |
[32m[20230114 18:50:27 @agent_ppo2.py:189][0m |          -0.0104 |           9.6436 |           4.7804 |
[32m[20230114 18:50:27 @agent_ppo2.py:189][0m |          -0.0144 |           9.5236 |           4.7805 |
[32m[20230114 18:50:27 @agent_ppo2.py:134][0m Policy update time: 0.80 s
[32m[20230114 18:50:28 @agent_ppo2.py:142][0m Average TRAINING episode reward: 261.03
[32m[20230114 18:50:28 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 263.67
[32m[20230114 18:50:28 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 273.24
[32m[20230114 18:50:28 @agent_ppo2.py:147][0m Total time:       8.39 min
[32m[20230114 18:50:28 @agent_ppo2.py:149][0m 696320 total steps have happened
[32m[20230114 18:50:28 @agent_ppo2.py:125][0m #------------------------ Iteration 340 --------------------------#
[32m[20230114 18:50:28 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:50:28 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:50:28 @agent_ppo2.py:189][0m |           0.0014 |          20.3589 |           4.7581 |
[32m[20230114 18:50:28 @agent_ppo2.py:189][0m |          -0.0051 |          15.9608 |           4.7508 |
[32m[20230114 18:50:28 @agent_ppo2.py:189][0m |          -0.0087 |          14.6208 |           4.7500 |
[32m[20230114 18:50:28 @agent_ppo2.py:189][0m |          -0.0094 |          13.8142 |           4.7517 |
[32m[20230114 18:50:28 @agent_ppo2.py:189][0m |          -0.0106 |          13.3003 |           4.7468 |
[32m[20230114 18:50:28 @agent_ppo2.py:189][0m |          -0.0121 |          12.9894 |           4.7547 |
[32m[20230114 18:50:29 @agent_ppo2.py:189][0m |          -0.0111 |          12.5549 |           4.7479 |
[32m[20230114 18:50:29 @agent_ppo2.py:189][0m |          -0.0131 |          12.3376 |           4.7520 |
[32m[20230114 18:50:29 @agent_ppo2.py:189][0m |          -0.0124 |          12.0152 |           4.7544 |
[32m[20230114 18:50:29 @agent_ppo2.py:189][0m |          -0.0131 |          11.7740 |           4.7520 |
[32m[20230114 18:50:29 @agent_ppo2.py:134][0m Policy update time: 0.83 s
[32m[20230114 18:50:29 @agent_ppo2.py:142][0m Average TRAINING episode reward: 224.45
[32m[20230114 18:50:29 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 259.92
[32m[20230114 18:50:29 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 270.87
[32m[20230114 18:50:29 @agent_ppo2.py:147][0m Total time:       8.41 min
[32m[20230114 18:50:29 @agent_ppo2.py:149][0m 698368 total steps have happened
[32m[20230114 18:50:29 @agent_ppo2.py:125][0m #------------------------ Iteration 341 --------------------------#
[32m[20230114 18:50:29 @agent_ppo2.py:131][0m Sampling time: 0.29 s by 4 slaves
[32m[20230114 18:50:29 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:50:29 @agent_ppo2.py:189][0m |           0.0008 |          13.7163 |           4.8536 |
[32m[20230114 18:50:29 @agent_ppo2.py:189][0m |          -0.0042 |          12.5242 |           4.8592 |
[32m[20230114 18:50:30 @agent_ppo2.py:189][0m |          -0.0058 |          12.0464 |           4.8557 |
[32m[20230114 18:50:30 @agent_ppo2.py:189][0m |          -0.0081 |          11.5818 |           4.8573 |
[32m[20230114 18:50:30 @agent_ppo2.py:189][0m |          -0.0085 |          11.2323 |           4.8552 |
[32m[20230114 18:50:30 @agent_ppo2.py:189][0m |          -0.0105 |          10.9203 |           4.8543 |
[32m[20230114 18:50:30 @agent_ppo2.py:189][0m |          -0.0108 |          10.6868 |           4.8534 |
[32m[20230114 18:50:30 @agent_ppo2.py:189][0m |          -0.0115 |          10.4612 |           4.8579 |
[32m[20230114 18:50:30 @agent_ppo2.py:189][0m |          -0.0119 |          10.1786 |           4.8529 |
[32m[20230114 18:50:30 @agent_ppo2.py:189][0m |          -0.0127 |          10.0345 |           4.8533 |
[32m[20230114 18:50:30 @agent_ppo2.py:134][0m Policy update time: 0.83 s
[32m[20230114 18:50:30 @agent_ppo2.py:142][0m Average TRAINING episode reward: 256.87
[32m[20230114 18:50:30 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 262.11
[32m[20230114 18:50:30 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 269.87
[32m[20230114 18:50:30 @agent_ppo2.py:147][0m Total time:       8.44 min
[32m[20230114 18:50:30 @agent_ppo2.py:149][0m 700416 total steps have happened
[32m[20230114 18:50:30 @agent_ppo2.py:125][0m #------------------------ Iteration 342 --------------------------#
[32m[20230114 18:50:31 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:50:31 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:50:31 @agent_ppo2.py:189][0m |          -0.0009 |          29.8538 |           4.8326 |
[32m[20230114 18:50:31 @agent_ppo2.py:189][0m |          -0.0079 |          23.2979 |           4.8360 |
[32m[20230114 18:50:31 @agent_ppo2.py:189][0m |          -0.0097 |          20.3683 |           4.8381 |
[32m[20230114 18:50:31 @agent_ppo2.py:189][0m |          -0.0097 |          18.3250 |           4.8398 |
[32m[20230114 18:50:31 @agent_ppo2.py:189][0m |          -0.0113 |          16.2453 |           4.8405 |
[32m[20230114 18:50:31 @agent_ppo2.py:189][0m |          -0.0104 |          15.0139 |           4.8435 |
[32m[20230114 18:50:31 @agent_ppo2.py:189][0m |          -0.0137 |          13.7575 |           4.8434 |
[32m[20230114 18:50:31 @agent_ppo2.py:189][0m |          -0.0122 |          12.9705 |           4.8440 |
[32m[20230114 18:50:31 @agent_ppo2.py:189][0m |          -0.0130 |          12.2289 |           4.8488 |
[32m[20230114 18:50:31 @agent_ppo2.py:189][0m |          -0.0145 |          11.6331 |           4.8471 |
[32m[20230114 18:50:31 @agent_ppo2.py:134][0m Policy update time: 0.80 s
[32m[20230114 18:50:32 @agent_ppo2.py:142][0m Average TRAINING episode reward: 223.83
[32m[20230114 18:50:32 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 262.27
[32m[20230114 18:50:32 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 274.84
[32m[20230114 18:50:32 @agent_ppo2.py:147][0m Total time:       8.46 min
[32m[20230114 18:50:32 @agent_ppo2.py:149][0m 702464 total steps have happened
[32m[20230114 18:50:32 @agent_ppo2.py:125][0m #------------------------ Iteration 343 --------------------------#
[32m[20230114 18:50:32 @agent_ppo2.py:131][0m Sampling time: 0.31 s by 4 slaves
[32m[20230114 18:50:32 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:50:32 @agent_ppo2.py:189][0m |           0.0008 |          28.4853 |           4.8597 |
[32m[20230114 18:50:32 @agent_ppo2.py:189][0m |          -0.0055 |          24.5795 |           4.8631 |
[32m[20230114 18:50:32 @agent_ppo2.py:189][0m |          -0.0078 |          22.4003 |           4.8638 |
[32m[20230114 18:50:32 @agent_ppo2.py:189][0m |          -0.0080 |          21.1548 |           4.8604 |
[32m[20230114 18:50:32 @agent_ppo2.py:189][0m |          -0.0091 |          20.2530 |           4.8591 |
[32m[20230114 18:50:32 @agent_ppo2.py:189][0m |          -0.0095 |          19.5653 |           4.8582 |
[32m[20230114 18:50:32 @agent_ppo2.py:189][0m |          -0.0104 |          18.9545 |           4.8559 |
[32m[20230114 18:50:33 @agent_ppo2.py:189][0m |          -0.0103 |          18.4726 |           4.8523 |
[32m[20230114 18:50:33 @agent_ppo2.py:189][0m |          -0.0111 |          17.9125 |           4.8561 |
[32m[20230114 18:50:33 @agent_ppo2.py:189][0m |          -0.0128 |          17.5610 |           4.8539 |
[32m[20230114 18:50:33 @agent_ppo2.py:134][0m Policy update time: 0.88 s
[32m[20230114 18:50:33 @agent_ppo2.py:142][0m Average TRAINING episode reward: 205.12
[32m[20230114 18:50:33 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 261.72
[32m[20230114 18:50:33 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 269.71
[32m[20230114 18:50:33 @agent_ppo2.py:147][0m Total time:       8.48 min
[32m[20230114 18:50:33 @agent_ppo2.py:149][0m 704512 total steps have happened
[32m[20230114 18:50:33 @agent_ppo2.py:125][0m #------------------------ Iteration 344 --------------------------#
[32m[20230114 18:50:33 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:50:33 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:50:33 @agent_ppo2.py:189][0m |          -0.0007 |          14.2495 |           4.8456 |
[32m[20230114 18:50:33 @agent_ppo2.py:189][0m |          -0.0044 |          12.7638 |           4.8441 |
[32m[20230114 18:50:33 @agent_ppo2.py:189][0m |          -0.0054 |          12.2747 |           4.8416 |
[32m[20230114 18:50:34 @agent_ppo2.py:189][0m |          -0.0073 |          11.9654 |           4.8412 |
[32m[20230114 18:50:34 @agent_ppo2.py:189][0m |          -0.0085 |          11.7226 |           4.8348 |
[32m[20230114 18:50:34 @agent_ppo2.py:189][0m |          -0.0083 |          11.5587 |           4.8402 |
[32m[20230114 18:50:34 @agent_ppo2.py:189][0m |          -0.0096 |          11.4222 |           4.8388 |
[32m[20230114 18:50:34 @agent_ppo2.py:189][0m |          -0.0100 |          11.2799 |           4.8356 |
[32m[20230114 18:50:34 @agent_ppo2.py:189][0m |          -0.0101 |          11.1936 |           4.8361 |
[32m[20230114 18:50:34 @agent_ppo2.py:189][0m |          -0.0108 |          11.0342 |           4.8359 |
[32m[20230114 18:50:34 @agent_ppo2.py:134][0m Policy update time: 0.79 s
[32m[20230114 18:50:34 @agent_ppo2.py:142][0m Average TRAINING episode reward: 260.75
[32m[20230114 18:50:34 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 262.11
[32m[20230114 18:50:34 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 271.43
[32m[20230114 18:50:34 @agent_ppo2.py:147][0m Total time:       8.50 min
[32m[20230114 18:50:34 @agent_ppo2.py:149][0m 706560 total steps have happened
[32m[20230114 18:50:34 @agent_ppo2.py:125][0m #------------------------ Iteration 345 --------------------------#
[32m[20230114 18:50:34 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:50:34 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:50:35 @agent_ppo2.py:189][0m |           0.0007 |          11.9799 |           4.8205 |
[32m[20230114 18:50:35 @agent_ppo2.py:189][0m |          -0.0022 |          10.9464 |           4.8162 |
[32m[20230114 18:50:35 @agent_ppo2.py:189][0m |          -0.0001 |          11.0525 |           4.8154 |
[32m[20230114 18:50:35 @agent_ppo2.py:189][0m |          -0.0047 |          10.4824 |           4.8090 |
[32m[20230114 18:50:35 @agent_ppo2.py:189][0m |          -0.0068 |          10.1553 |           4.8138 |
[32m[20230114 18:50:35 @agent_ppo2.py:189][0m |          -0.0072 |          10.0456 |           4.8120 |
[32m[20230114 18:50:35 @agent_ppo2.py:189][0m |          -0.0061 |           9.9227 |           4.8132 |
[32m[20230114 18:50:35 @agent_ppo2.py:189][0m |          -0.0085 |           9.8608 |           4.8142 |
[32m[20230114 18:50:35 @agent_ppo2.py:189][0m |          -0.0057 |           9.8630 |           4.8091 |
[32m[20230114 18:50:35 @agent_ppo2.py:189][0m |          -0.0054 |           9.7868 |           4.8068 |
[32m[20230114 18:50:35 @agent_ppo2.py:134][0m Policy update time: 0.82 s
[32m[20230114 18:50:35 @agent_ppo2.py:142][0m Average TRAINING episode reward: 259.17
[32m[20230114 18:50:35 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 261.61
[32m[20230114 18:50:35 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 273.07
[32m[20230114 18:50:35 @agent_ppo2.py:147][0m Total time:       8.52 min
[32m[20230114 18:50:35 @agent_ppo2.py:149][0m 708608 total steps have happened
[32m[20230114 18:50:35 @agent_ppo2.py:125][0m #------------------------ Iteration 346 --------------------------#
[32m[20230114 18:50:36 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:50:36 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:50:36 @agent_ppo2.py:189][0m |           0.0026 |          11.5901 |           4.7572 |
[32m[20230114 18:50:36 @agent_ppo2.py:189][0m |          -0.0029 |          10.3628 |           4.7488 |
[32m[20230114 18:50:36 @agent_ppo2.py:189][0m |          -0.0045 |           9.8679 |           4.7525 |
[32m[20230114 18:50:36 @agent_ppo2.py:189][0m |          -0.0055 |           9.6133 |           4.7524 |
[32m[20230114 18:50:36 @agent_ppo2.py:189][0m |          -0.0077 |           9.3077 |           4.7506 |
[32m[20230114 18:50:36 @agent_ppo2.py:189][0m |          -0.0079 |           9.1898 |           4.7511 |
[32m[20230114 18:50:36 @agent_ppo2.py:189][0m |          -0.0080 |           8.9894 |           4.7481 |
[32m[20230114 18:50:36 @agent_ppo2.py:189][0m |          -0.0103 |           8.7912 |           4.7473 |
[32m[20230114 18:50:36 @agent_ppo2.py:189][0m |          -0.0117 |           8.6703 |           4.7486 |
[32m[20230114 18:50:36 @agent_ppo2.py:189][0m |          -0.0118 |           8.5135 |           4.7502 |
[32m[20230114 18:50:36 @agent_ppo2.py:134][0m Policy update time: 0.82 s
[32m[20230114 18:50:37 @agent_ppo2.py:142][0m Average TRAINING episode reward: 258.19
[32m[20230114 18:50:37 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 260.32
[32m[20230114 18:50:37 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 265.94
[32m[20230114 18:50:37 @agent_ppo2.py:147][0m Total time:       8.54 min
[32m[20230114 18:50:37 @agent_ppo2.py:149][0m 710656 total steps have happened
[32m[20230114 18:50:37 @agent_ppo2.py:125][0m #------------------------ Iteration 347 --------------------------#
[32m[20230114 18:50:37 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:50:37 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:50:37 @agent_ppo2.py:189][0m |          -0.0006 |          12.8773 |           4.7657 |
[32m[20230114 18:50:37 @agent_ppo2.py:189][0m |           0.0002 |          12.8133 |           4.7676 |
[32m[20230114 18:50:37 @agent_ppo2.py:189][0m |          -0.0082 |          11.5213 |           4.7654 |
[32m[20230114 18:50:37 @agent_ppo2.py:189][0m |          -0.0099 |          11.2004 |           4.7652 |
[32m[20230114 18:50:37 @agent_ppo2.py:189][0m |          -0.0054 |          11.1711 |           4.7678 |
[32m[20230114 18:50:37 @agent_ppo2.py:189][0m |          -0.0115 |          10.7367 |           4.7674 |
[32m[20230114 18:50:38 @agent_ppo2.py:189][0m |          -0.0105 |          10.5752 |           4.7666 |
[32m[20230114 18:50:38 @agent_ppo2.py:189][0m |          -0.0120 |          10.3812 |           4.7695 |
[32m[20230114 18:50:38 @agent_ppo2.py:189][0m |          -0.0160 |          10.2689 |           4.7656 |
[32m[20230114 18:50:38 @agent_ppo2.py:189][0m |          -0.0134 |          10.1426 |           4.7682 |
[32m[20230114 18:50:38 @agent_ppo2.py:134][0m Policy update time: 0.80 s
[32m[20230114 18:50:38 @agent_ppo2.py:142][0m Average TRAINING episode reward: 260.66
[32m[20230114 18:50:38 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 262.12
[32m[20230114 18:50:38 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 271.76
[32m[20230114 18:50:38 @agent_ppo2.py:147][0m Total time:       8.56 min
[32m[20230114 18:50:38 @agent_ppo2.py:149][0m 712704 total steps have happened
[32m[20230114 18:50:38 @agent_ppo2.py:125][0m #------------------------ Iteration 348 --------------------------#
[32m[20230114 18:50:38 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:50:38 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:50:38 @agent_ppo2.py:189][0m |           0.0006 |          27.5772 |           4.9199 |
[32m[20230114 18:50:38 @agent_ppo2.py:189][0m |          -0.0060 |          15.7471 |           4.9240 |
[32m[20230114 18:50:38 @agent_ppo2.py:189][0m |          -0.0081 |          13.3638 |           4.9213 |
[32m[20230114 18:50:39 @agent_ppo2.py:189][0m |          -0.0101 |          12.4524 |           4.9196 |
[32m[20230114 18:50:39 @agent_ppo2.py:189][0m |          -0.0111 |          11.9950 |           4.9179 |
[32m[20230114 18:50:39 @agent_ppo2.py:189][0m |          -0.0121 |          11.4528 |           4.9186 |
[32m[20230114 18:50:39 @agent_ppo2.py:189][0m |          -0.0119 |          11.1191 |           4.9204 |
[32m[20230114 18:50:39 @agent_ppo2.py:189][0m |          -0.0128 |          10.8690 |           4.9225 |
[32m[20230114 18:50:39 @agent_ppo2.py:189][0m |          -0.0134 |          10.5722 |           4.9223 |
[32m[20230114 18:50:39 @agent_ppo2.py:189][0m |          -0.0135 |          10.3525 |           4.9238 |
[32m[20230114 18:50:39 @agent_ppo2.py:134][0m Policy update time: 0.76 s
[32m[20230114 18:50:39 @agent_ppo2.py:142][0m Average TRAINING episode reward: 223.25
[32m[20230114 18:50:39 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 261.43
[32m[20230114 18:50:39 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 270.58
[32m[20230114 18:50:39 @agent_ppo2.py:147][0m Total time:       8.58 min
[32m[20230114 18:50:39 @agent_ppo2.py:149][0m 714752 total steps have happened
[32m[20230114 18:50:39 @agent_ppo2.py:125][0m #------------------------ Iteration 349 --------------------------#
[32m[20230114 18:50:39 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:50:39 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:50:39 @agent_ppo2.py:189][0m |           0.0005 |          15.5992 |           4.9367 |
[32m[20230114 18:50:40 @agent_ppo2.py:189][0m |          -0.0052 |          13.5603 |           4.9224 |
[32m[20230114 18:50:40 @agent_ppo2.py:189][0m |          -0.0076 |          13.0317 |           4.9261 |
[32m[20230114 18:50:40 @agent_ppo2.py:189][0m |          -0.0080 |          12.7745 |           4.9249 |
[32m[20230114 18:50:40 @agent_ppo2.py:189][0m |          -0.0094 |          12.6110 |           4.9212 |
[32m[20230114 18:50:40 @agent_ppo2.py:189][0m |          -0.0101 |          12.4680 |           4.9263 |
[32m[20230114 18:50:40 @agent_ppo2.py:189][0m |          -0.0108 |          12.3752 |           4.9235 |
[32m[20230114 18:50:40 @agent_ppo2.py:189][0m |          -0.0107 |          12.2976 |           4.9226 |
[32m[20230114 18:50:40 @agent_ppo2.py:189][0m |          -0.0114 |          12.2469 |           4.9225 |
[32m[20230114 18:50:40 @agent_ppo2.py:189][0m |          -0.0111 |          12.2474 |           4.9187 |
[32m[20230114 18:50:40 @agent_ppo2.py:134][0m Policy update time: 0.80 s
[32m[20230114 18:50:40 @agent_ppo2.py:142][0m Average TRAINING episode reward: 260.37
[32m[20230114 18:50:40 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 262.02
[32m[20230114 18:50:40 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 272.57
[32m[20230114 18:50:40 @agent_ppo2.py:147][0m Total time:       8.60 min
[32m[20230114 18:50:40 @agent_ppo2.py:149][0m 716800 total steps have happened
[32m[20230114 18:50:40 @agent_ppo2.py:125][0m #------------------------ Iteration 350 --------------------------#
[32m[20230114 18:50:41 @agent_ppo2.py:131][0m Sampling time: 0.25 s by 4 slaves
[32m[20230114 18:50:41 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:50:41 @agent_ppo2.py:189][0m |           0.0020 |          14.1505 |           4.8941 |
[32m[20230114 18:50:41 @agent_ppo2.py:189][0m |          -0.0062 |          11.8642 |           4.8881 |
[32m[20230114 18:50:41 @agent_ppo2.py:189][0m |          -0.0078 |          11.1454 |           4.8905 |
[32m[20230114 18:50:41 @agent_ppo2.py:189][0m |          -0.0147 |          10.7648 |           4.8818 |
[32m[20230114 18:50:41 @agent_ppo2.py:189][0m |          -0.0072 |          10.3186 |           4.8841 |
[32m[20230114 18:50:41 @agent_ppo2.py:189][0m |          -0.0084 |          10.0186 |           4.8869 |
[32m[20230114 18:50:41 @agent_ppo2.py:189][0m |          -0.0110 |           9.7952 |           4.8831 |
[32m[20230114 18:50:41 @agent_ppo2.py:189][0m |          -0.0101 |           9.5668 |           4.8850 |
[32m[20230114 18:50:41 @agent_ppo2.py:189][0m |          -0.0125 |           9.3284 |           4.8837 |
[32m[20230114 18:50:41 @agent_ppo2.py:189][0m |          -0.0022 |           9.4328 |           4.8801 |
[32m[20230114 18:50:41 @agent_ppo2.py:134][0m Policy update time: 0.82 s
[32m[20230114 18:50:42 @agent_ppo2.py:142][0m Average TRAINING episode reward: 257.39
[32m[20230114 18:50:42 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 261.44
[32m[20230114 18:50:42 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 271.32
[32m[20230114 18:50:42 @agent_ppo2.py:147][0m Total time:       8.62 min
[32m[20230114 18:50:42 @agent_ppo2.py:149][0m 718848 total steps have happened
[32m[20230114 18:50:42 @agent_ppo2.py:125][0m #------------------------ Iteration 351 --------------------------#
[32m[20230114 18:50:42 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:50:42 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:50:42 @agent_ppo2.py:189][0m |          -0.0006 |          13.1452 |           4.9242 |
[32m[20230114 18:50:42 @agent_ppo2.py:189][0m |          -0.0027 |          12.7987 |           4.9197 |
[32m[20230114 18:50:42 @agent_ppo2.py:189][0m |          -0.0074 |          12.2122 |           4.9158 |
[32m[20230114 18:50:42 @agent_ppo2.py:189][0m |          -0.0055 |          12.0963 |           4.9160 |
[32m[20230114 18:50:42 @agent_ppo2.py:189][0m |          -0.0088 |          11.8541 |           4.9168 |
[32m[20230114 18:50:42 @agent_ppo2.py:189][0m |          -0.0103 |          11.7121 |           4.9084 |
[32m[20230114 18:50:42 @agent_ppo2.py:189][0m |          -0.0088 |          11.6823 |           4.9113 |
[32m[20230114 18:50:43 @agent_ppo2.py:189][0m |          -0.0121 |          11.4591 |           4.9102 |
[32m[20230114 18:50:43 @agent_ppo2.py:189][0m |          -0.0110 |          11.3535 |           4.9153 |
[32m[20230114 18:50:43 @agent_ppo2.py:189][0m |          -0.0119 |          11.2164 |           4.9143 |
[32m[20230114 18:50:43 @agent_ppo2.py:134][0m Policy update time: 0.82 s
[32m[20230114 18:50:43 @agent_ppo2.py:142][0m Average TRAINING episode reward: 259.17
[32m[20230114 18:50:43 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 263.45
[32m[20230114 18:50:43 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 274.25
[32m[20230114 18:50:43 @agent_ppo2.py:147][0m Total time:       8.65 min
[32m[20230114 18:50:43 @agent_ppo2.py:149][0m 720896 total steps have happened
[32m[20230114 18:50:43 @agent_ppo2.py:125][0m #------------------------ Iteration 352 --------------------------#
[32m[20230114 18:50:43 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:50:43 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:50:43 @agent_ppo2.py:189][0m |          -0.0007 |          31.3309 |           4.9914 |
[32m[20230114 18:50:43 @agent_ppo2.py:189][0m |          -0.0066 |          24.6078 |           4.9801 |
[32m[20230114 18:50:43 @agent_ppo2.py:189][0m |          -0.0086 |          21.2705 |           4.9802 |
[32m[20230114 18:50:43 @agent_ppo2.py:189][0m |          -0.0102 |          18.6907 |           4.9843 |
[32m[20230114 18:50:44 @agent_ppo2.py:189][0m |          -0.0104 |          17.0612 |           4.9832 |
[32m[20230114 18:50:44 @agent_ppo2.py:189][0m |          -0.0108 |          15.2327 |           4.9832 |
[32m[20230114 18:50:44 @agent_ppo2.py:189][0m |          -0.0126 |          14.0228 |           4.9801 |
[32m[20230114 18:50:44 @agent_ppo2.py:189][0m |          -0.0129 |          13.0789 |           4.9776 |
[32m[20230114 18:50:44 @agent_ppo2.py:189][0m |          -0.0143 |          12.3786 |           4.9815 |
[32m[20230114 18:50:44 @agent_ppo2.py:189][0m |          -0.0136 |          11.8677 |           4.9788 |
[32m[20230114 18:50:44 @agent_ppo2.py:134][0m Policy update time: 0.73 s
[32m[20230114 18:50:44 @agent_ppo2.py:142][0m Average TRAINING episode reward: 212.44
[32m[20230114 18:50:44 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 262.35
[32m[20230114 18:50:44 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 271.90
[32m[20230114 18:50:44 @agent_ppo2.py:147][0m Total time:       8.67 min
[32m[20230114 18:50:44 @agent_ppo2.py:149][0m 722944 total steps have happened
[32m[20230114 18:50:44 @agent_ppo2.py:125][0m #------------------------ Iteration 353 --------------------------#
[32m[20230114 18:50:44 @agent_ppo2.py:131][0m Sampling time: 0.25 s by 4 slaves
[32m[20230114 18:50:44 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:50:44 @agent_ppo2.py:189][0m |          -0.0006 |          14.5846 |           4.8431 |
[32m[20230114 18:50:45 @agent_ppo2.py:189][0m |          -0.0062 |          13.6889 |           4.8342 |
[32m[20230114 18:50:45 @agent_ppo2.py:189][0m |          -0.0074 |          13.4385 |           4.8337 |
[32m[20230114 18:50:45 @agent_ppo2.py:189][0m |          -0.0076 |          13.4113 |           4.8296 |
[32m[20230114 18:50:45 @agent_ppo2.py:189][0m |          -0.0100 |          13.2611 |           4.8254 |
[32m[20230114 18:50:45 @agent_ppo2.py:189][0m |          -0.0097 |          13.2011 |           4.8254 |
[32m[20230114 18:50:45 @agent_ppo2.py:189][0m |          -0.0105 |          13.0554 |           4.8217 |
[32m[20230114 18:50:45 @agent_ppo2.py:189][0m |          -0.0100 |          13.0603 |           4.8281 |
[32m[20230114 18:50:45 @agent_ppo2.py:189][0m |          -0.0121 |          12.8191 |           4.8194 |
[32m[20230114 18:50:45 @agent_ppo2.py:189][0m |          -0.0132 |          12.6618 |           4.8177 |
[32m[20230114 18:50:45 @agent_ppo2.py:134][0m Policy update time: 0.80 s
[32m[20230114 18:50:45 @agent_ppo2.py:142][0m Average TRAINING episode reward: 259.93
[32m[20230114 18:50:45 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 262.99
[32m[20230114 18:50:45 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 269.58
[32m[20230114 18:50:45 @agent_ppo2.py:147][0m Total time:       8.69 min
[32m[20230114 18:50:45 @agent_ppo2.py:149][0m 724992 total steps have happened
[32m[20230114 18:50:45 @agent_ppo2.py:125][0m #------------------------ Iteration 354 --------------------------#
[32m[20230114 18:50:46 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:50:46 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:50:46 @agent_ppo2.py:189][0m |          -0.0014 |          12.9407 |           4.8957 |
[32m[20230114 18:50:46 @agent_ppo2.py:189][0m |          -0.0043 |          10.8251 |           4.8819 |
[32m[20230114 18:50:46 @agent_ppo2.py:189][0m |          -0.0089 |           9.8435 |           4.8802 |
[32m[20230114 18:50:46 @agent_ppo2.py:189][0m |          -0.0086 |           9.2767 |           4.8812 |
[32m[20230114 18:50:46 @agent_ppo2.py:189][0m |          -0.0072 |           9.0547 |           4.8769 |
[32m[20230114 18:50:46 @agent_ppo2.py:189][0m |          -0.0096 |           8.5078 |           4.8764 |
[32m[20230114 18:50:46 @agent_ppo2.py:189][0m |          -0.0129 |           8.2005 |           4.8747 |
[32m[20230114 18:50:46 @agent_ppo2.py:189][0m |          -0.0136 |           7.8361 |           4.8763 |
[32m[20230114 18:50:46 @agent_ppo2.py:189][0m |          -0.0130 |           7.6245 |           4.8725 |
[32m[20230114 18:50:46 @agent_ppo2.py:189][0m |          -0.0127 |           7.4043 |           4.8723 |
[32m[20230114 18:50:46 @agent_ppo2.py:134][0m Policy update time: 0.78 s
[32m[20230114 18:50:47 @agent_ppo2.py:142][0m Average TRAINING episode reward: 262.01
[32m[20230114 18:50:47 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 264.10
[32m[20230114 18:50:47 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 268.68
[32m[20230114 18:50:47 @agent_ppo2.py:147][0m Total time:       8.71 min
[32m[20230114 18:50:47 @agent_ppo2.py:149][0m 727040 total steps have happened
[32m[20230114 18:50:47 @agent_ppo2.py:125][0m #------------------------ Iteration 355 --------------------------#
[32m[20230114 18:50:47 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:50:47 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:50:47 @agent_ppo2.py:189][0m |          -0.0022 |          18.2951 |           4.8476 |
[32m[20230114 18:50:47 @agent_ppo2.py:189][0m |          -0.0021 |          12.4478 |           4.8467 |
[32m[20230114 18:50:47 @agent_ppo2.py:189][0m |          -0.0076 |           9.8902 |           4.8443 |
[32m[20230114 18:50:47 @agent_ppo2.py:189][0m |          -0.0074 |           8.1696 |           4.8415 |
[32m[20230114 18:50:47 @agent_ppo2.py:189][0m |          -0.0099 |           7.0917 |           4.8378 |
[32m[20230114 18:50:47 @agent_ppo2.py:189][0m |          -0.0121 |           6.5580 |           4.8361 |
[32m[20230114 18:50:47 @agent_ppo2.py:189][0m |          -0.0110 |           6.0843 |           4.8363 |
[32m[20230114 18:50:47 @agent_ppo2.py:189][0m |          -0.0121 |           5.7549 |           4.8377 |
[32m[20230114 18:50:47 @agent_ppo2.py:189][0m |          -0.0114 |           5.4239 |           4.8383 |
[32m[20230114 18:50:48 @agent_ppo2.py:189][0m |          -0.0134 |           5.1956 |           4.8339 |
[32m[20230114 18:50:48 @agent_ppo2.py:134][0m Policy update time: 0.76 s
[32m[20230114 18:50:48 @agent_ppo2.py:142][0m Average TRAINING episode reward: 228.14
[32m[20230114 18:50:48 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 263.05
[32m[20230114 18:50:48 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 269.08
[32m[20230114 18:50:48 @agent_ppo2.py:147][0m Total time:       8.73 min
[32m[20230114 18:50:48 @agent_ppo2.py:149][0m 729088 total steps have happened
[32m[20230114 18:50:48 @agent_ppo2.py:125][0m #------------------------ Iteration 356 --------------------------#
[32m[20230114 18:50:48 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:50:48 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:50:48 @agent_ppo2.py:189][0m |          -0.0019 |          16.7948 |           4.9617 |
[32m[20230114 18:50:48 @agent_ppo2.py:189][0m |          -0.0062 |          14.9005 |           4.9577 |
[32m[20230114 18:50:48 @agent_ppo2.py:189][0m |          -0.0057 |          14.4629 |           4.9600 |
[32m[20230114 18:50:48 @agent_ppo2.py:189][0m |          -0.0059 |          14.1094 |           4.9595 |
[32m[20230114 18:50:48 @agent_ppo2.py:189][0m |          -0.0126 |          13.7785 |           4.9615 |
[32m[20230114 18:50:48 @agent_ppo2.py:189][0m |          -0.0119 |          13.5916 |           4.9637 |
[32m[20230114 18:50:49 @agent_ppo2.py:189][0m |          -0.0127 |          13.3910 |           4.9665 |
[32m[20230114 18:50:49 @agent_ppo2.py:189][0m |          -0.0133 |          13.2175 |           4.9732 |
[32m[20230114 18:50:49 @agent_ppo2.py:189][0m |          -0.0119 |          13.2947 |           4.9690 |
[32m[20230114 18:50:49 @agent_ppo2.py:189][0m |          -0.0118 |          12.9345 |           4.9770 |
[32m[20230114 18:50:49 @agent_ppo2.py:134][0m Policy update time: 0.80 s
[32m[20230114 18:50:49 @agent_ppo2.py:142][0m Average TRAINING episode reward: 258.56
[32m[20230114 18:50:49 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 262.07
[32m[20230114 18:50:49 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 265.69
[32m[20230114 18:50:49 @agent_ppo2.py:147][0m Total time:       8.75 min
[32m[20230114 18:50:49 @agent_ppo2.py:149][0m 731136 total steps have happened
[32m[20230114 18:50:49 @agent_ppo2.py:125][0m #------------------------ Iteration 357 --------------------------#
[32m[20230114 18:50:49 @agent_ppo2.py:131][0m Sampling time: 0.28 s by 4 slaves
[32m[20230114 18:50:49 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:50:49 @agent_ppo2.py:189][0m |          -0.0016 |          27.3061 |           5.0100 |
[32m[20230114 18:50:50 @agent_ppo2.py:189][0m |          -0.0089 |          23.2887 |           4.9941 |
[32m[20230114 18:50:50 @agent_ppo2.py:189][0m |          -0.0108 |          21.9829 |           4.9911 |
[32m[20230114 18:50:50 @agent_ppo2.py:189][0m |          -0.0120 |          21.2991 |           4.9957 |
[32m[20230114 18:50:50 @agent_ppo2.py:189][0m |          -0.0133 |          20.6367 |           4.9915 |
[32m[20230114 18:50:50 @agent_ppo2.py:189][0m |          -0.0137 |          20.0226 |           4.9922 |
[32m[20230114 18:50:50 @agent_ppo2.py:189][0m |          -0.0142 |          19.6164 |           4.9913 |
[32m[20230114 18:50:50 @agent_ppo2.py:189][0m |          -0.0144 |          19.1905 |           4.9905 |
[32m[20230114 18:50:50 @agent_ppo2.py:189][0m |          -0.0156 |          18.6114 |           4.9900 |
[32m[20230114 18:50:50 @agent_ppo2.py:189][0m |          -0.0157 |          18.3141 |           4.9863 |
[32m[20230114 18:50:50 @agent_ppo2.py:134][0m Policy update time: 0.87 s
[32m[20230114 18:50:50 @agent_ppo2.py:142][0m Average TRAINING episode reward: 204.21
[32m[20230114 18:50:50 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 263.09
[32m[20230114 18:50:50 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 271.36
[32m[20230114 18:50:50 @agent_ppo2.py:147][0m Total time:       8.77 min
[32m[20230114 18:50:50 @agent_ppo2.py:149][0m 733184 total steps have happened
[32m[20230114 18:50:50 @agent_ppo2.py:125][0m #------------------------ Iteration 358 --------------------------#
[32m[20230114 18:50:51 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:50:51 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:50:51 @agent_ppo2.py:189][0m |           0.0025 |          15.8388 |           4.9859 |
[32m[20230114 18:50:51 @agent_ppo2.py:189][0m |          -0.0062 |          14.3551 |           4.9824 |
[32m[20230114 18:50:51 @agent_ppo2.py:189][0m |          -0.0093 |          13.8151 |           4.9828 |
[32m[20230114 18:50:51 @agent_ppo2.py:189][0m |          -0.0059 |          13.4731 |           4.9789 |
[32m[20230114 18:50:51 @agent_ppo2.py:189][0m |          -0.0106 |          13.2730 |           4.9831 |
[32m[20230114 18:50:51 @agent_ppo2.py:189][0m |          -0.0090 |          13.0408 |           4.9780 |
[32m[20230114 18:50:51 @agent_ppo2.py:189][0m |          -0.0127 |          12.8425 |           4.9830 |
[32m[20230114 18:50:51 @agent_ppo2.py:189][0m |          -0.0102 |          12.6830 |           4.9842 |
[32m[20230114 18:50:51 @agent_ppo2.py:189][0m |          -0.0116 |          12.6287 |           4.9803 |
[32m[20230114 18:50:51 @agent_ppo2.py:189][0m |          -0.0121 |          12.4285 |           4.9881 |
[32m[20230114 18:50:51 @agent_ppo2.py:134][0m Policy update time: 0.79 s
[32m[20230114 18:50:52 @agent_ppo2.py:142][0m Average TRAINING episode reward: 261.15
[32m[20230114 18:50:52 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 264.38
[32m[20230114 18:50:52 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 260.66
[32m[20230114 18:50:52 @agent_ppo2.py:147][0m Total time:       8.79 min
[32m[20230114 18:50:52 @agent_ppo2.py:149][0m 735232 total steps have happened
[32m[20230114 18:50:52 @agent_ppo2.py:125][0m #------------------------ Iteration 359 --------------------------#
[32m[20230114 18:50:52 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:50:52 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:50:52 @agent_ppo2.py:189][0m |          -0.0002 |          17.6503 |           4.9022 |
[32m[20230114 18:50:52 @agent_ppo2.py:189][0m |          -0.0014 |          11.2500 |           4.8934 |
[32m[20230114 18:50:52 @agent_ppo2.py:189][0m |          -0.0084 |          10.0063 |           4.8822 |
[32m[20230114 18:50:52 @agent_ppo2.py:189][0m |          -0.0050 |           9.0502 |           4.8807 |
[32m[20230114 18:50:52 @agent_ppo2.py:189][0m |          -0.0122 |           8.4494 |           4.8832 |
[32m[20230114 18:50:52 @agent_ppo2.py:189][0m |          -0.0122 |           8.0198 |           4.8772 |
[32m[20230114 18:50:52 @agent_ppo2.py:189][0m |          -0.0136 |           7.5272 |           4.8722 |
[32m[20230114 18:50:52 @agent_ppo2.py:189][0m |          -0.0147 |           7.0841 |           4.8738 |
[32m[20230114 18:50:52 @agent_ppo2.py:189][0m |          -0.0146 |           6.7412 |           4.8699 |
[32m[20230114 18:50:53 @agent_ppo2.py:189][0m |          -0.0087 |           6.5337 |           4.8695 |
[32m[20230114 18:50:53 @agent_ppo2.py:134][0m Policy update time: 0.71 s
[32m[20230114 18:50:53 @agent_ppo2.py:142][0m Average TRAINING episode reward: 198.18
[32m[20230114 18:50:53 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 259.91
[32m[20230114 18:50:53 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 268.56
[32m[20230114 18:50:53 @agent_ppo2.py:147][0m Total time:       8.81 min
[32m[20230114 18:50:53 @agent_ppo2.py:149][0m 737280 total steps have happened
[32m[20230114 18:50:53 @agent_ppo2.py:125][0m #------------------------ Iteration 360 --------------------------#
[32m[20230114 18:50:53 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:50:53 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:50:53 @agent_ppo2.py:189][0m |           0.0031 |          21.1021 |           4.9068 |
[32m[20230114 18:50:53 @agent_ppo2.py:189][0m |          -0.0016 |          17.5453 |           4.9121 |
[32m[20230114 18:50:53 @agent_ppo2.py:189][0m |          -0.0040 |          16.1805 |           4.9118 |
[32m[20230114 18:50:53 @agent_ppo2.py:189][0m |          -0.0071 |          15.4018 |           4.9126 |
[32m[20230114 18:50:53 @agent_ppo2.py:189][0m |          -0.0073 |          14.7292 |           4.9124 |
[32m[20230114 18:50:54 @agent_ppo2.py:189][0m |          -0.0096 |          14.1584 |           4.9112 |
[32m[20230114 18:50:54 @agent_ppo2.py:189][0m |          -0.0087 |          13.6871 |           4.9093 |
[32m[20230114 18:50:54 @agent_ppo2.py:189][0m |          -0.0107 |          13.1517 |           4.9156 |
[32m[20230114 18:50:54 @agent_ppo2.py:189][0m |          -0.0116 |          12.7165 |           4.9168 |
[32m[20230114 18:50:54 @agent_ppo2.py:189][0m |          -0.0114 |          12.3263 |           4.9206 |
[32m[20230114 18:50:54 @agent_ppo2.py:134][0m Policy update time: 0.80 s
[32m[20230114 18:50:54 @agent_ppo2.py:142][0m Average TRAINING episode reward: 259.65
[32m[20230114 18:50:54 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 264.42
[32m[20230114 18:50:54 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 268.33
[32m[20230114 18:50:54 @agent_ppo2.py:147][0m Total time:       8.83 min
[32m[20230114 18:50:54 @agent_ppo2.py:149][0m 739328 total steps have happened
[32m[20230114 18:50:54 @agent_ppo2.py:125][0m #------------------------ Iteration 361 --------------------------#
[32m[20230114 18:50:54 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:50:54 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:50:54 @agent_ppo2.py:189][0m |           0.0052 |          15.8845 |           4.9839 |
[32m[20230114 18:50:55 @agent_ppo2.py:189][0m |          -0.0016 |          14.2262 |           4.9838 |
[32m[20230114 18:50:55 @agent_ppo2.py:189][0m |          -0.0080 |          13.8259 |           4.9765 |
[32m[20230114 18:50:55 @agent_ppo2.py:189][0m |          -0.0062 |          13.6316 |           4.9776 |
[32m[20230114 18:50:55 @agent_ppo2.py:189][0m |          -0.0067 |          13.6387 |           4.9783 |
[32m[20230114 18:50:55 @agent_ppo2.py:189][0m |          -0.0094 |          13.1813 |           4.9766 |
[32m[20230114 18:50:55 @agent_ppo2.py:189][0m |          -0.0065 |          13.4979 |           4.9694 |
[32m[20230114 18:50:55 @agent_ppo2.py:189][0m |          -0.0104 |          12.9346 |           4.9730 |
[32m[20230114 18:50:55 @agent_ppo2.py:189][0m |          -0.0113 |          12.7395 |           4.9797 |
[32m[20230114 18:50:55 @agent_ppo2.py:189][0m |          -0.0117 |          12.6639 |           4.9727 |
[32m[20230114 18:50:55 @agent_ppo2.py:134][0m Policy update time: 0.79 s
[32m[20230114 18:50:55 @agent_ppo2.py:142][0m Average TRAINING episode reward: 258.71
[32m[20230114 18:50:55 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 263.01
[32m[20230114 18:50:55 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 267.55
[32m[20230114 18:50:55 @agent_ppo2.py:147][0m Total time:       8.85 min
[32m[20230114 18:50:55 @agent_ppo2.py:149][0m 741376 total steps have happened
[32m[20230114 18:50:55 @agent_ppo2.py:125][0m #------------------------ Iteration 362 --------------------------#
[32m[20230114 18:50:56 @agent_ppo2.py:131][0m Sampling time: 0.25 s by 4 slaves
[32m[20230114 18:50:56 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:50:56 @agent_ppo2.py:189][0m |           0.0005 |          13.9129 |           4.9648 |
[32m[20230114 18:50:56 @agent_ppo2.py:189][0m |          -0.0037 |          11.8470 |           4.9630 |
[32m[20230114 18:50:56 @agent_ppo2.py:189][0m |          -0.0061 |          10.7458 |           4.9563 |
[32m[20230114 18:50:56 @agent_ppo2.py:189][0m |          -0.0072 |          10.0487 |           4.9614 |
[32m[20230114 18:50:56 @agent_ppo2.py:189][0m |          -0.0084 |           9.4241 |           4.9579 |
[32m[20230114 18:50:56 @agent_ppo2.py:189][0m |          -0.0099 |           8.9149 |           4.9565 |
[32m[20230114 18:50:56 @agent_ppo2.py:189][0m |          -0.0095 |           8.4916 |           4.9569 |
[32m[20230114 18:50:56 @agent_ppo2.py:189][0m |          -0.0108 |           8.1126 |           4.9572 |
[32m[20230114 18:50:56 @agent_ppo2.py:189][0m |          -0.0109 |           7.7475 |           4.9549 |
[32m[20230114 18:50:56 @agent_ppo2.py:189][0m |          -0.0117 |           7.4391 |           4.9531 |
[32m[20230114 18:50:56 @agent_ppo2.py:134][0m Policy update time: 0.79 s
[32m[20230114 18:50:57 @agent_ppo2.py:142][0m Average TRAINING episode reward: 258.71
[32m[20230114 18:50:57 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 260.79
[32m[20230114 18:50:57 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 269.05
[32m[20230114 18:50:57 @agent_ppo2.py:147][0m Total time:       8.87 min
[32m[20230114 18:50:57 @agent_ppo2.py:149][0m 743424 total steps have happened
[32m[20230114 18:50:57 @agent_ppo2.py:125][0m #------------------------ Iteration 363 --------------------------#
[32m[20230114 18:50:57 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:50:57 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:50:57 @agent_ppo2.py:189][0m |           0.0016 |          14.6218 |           4.8841 |
[32m[20230114 18:50:57 @agent_ppo2.py:189][0m |          -0.0021 |          13.5735 |           4.8808 |
[32m[20230114 18:50:57 @agent_ppo2.py:189][0m |          -0.0056 |          12.9173 |           4.8801 |
[32m[20230114 18:50:57 @agent_ppo2.py:189][0m |          -0.0081 |          12.5326 |           4.8725 |
[32m[20230114 18:50:57 @agent_ppo2.py:189][0m |          -0.0072 |          12.3330 |           4.8741 |
[32m[20230114 18:50:57 @agent_ppo2.py:189][0m |          -0.0068 |          12.3073 |           4.8706 |
[32m[20230114 18:50:57 @agent_ppo2.py:189][0m |          -0.0076 |          12.0882 |           4.8718 |
[32m[20230114 18:50:57 @agent_ppo2.py:189][0m |          -0.0058 |          12.4611 |           4.8726 |
[32m[20230114 18:50:57 @agent_ppo2.py:189][0m |          -0.0092 |          12.0272 |           4.8709 |
[32m[20230114 18:50:58 @agent_ppo2.py:189][0m |          -0.0108 |          11.7385 |           4.8649 |
[32m[20230114 18:50:58 @agent_ppo2.py:134][0m Policy update time: 0.78 s
[32m[20230114 18:50:58 @agent_ppo2.py:142][0m Average TRAINING episode reward: 260.42
[32m[20230114 18:50:58 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 262.75
[32m[20230114 18:50:58 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 264.99
[32m[20230114 18:50:58 @agent_ppo2.py:147][0m Total time:       8.89 min
[32m[20230114 18:50:58 @agent_ppo2.py:149][0m 745472 total steps have happened
[32m[20230114 18:50:58 @agent_ppo2.py:125][0m #------------------------ Iteration 364 --------------------------#
[32m[20230114 18:50:58 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:50:58 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:50:58 @agent_ppo2.py:189][0m |          -0.0029 |          11.4346 |           4.9720 |
[32m[20230114 18:50:58 @agent_ppo2.py:189][0m |          -0.0130 |           9.6481 |           4.9652 |
[32m[20230114 18:50:58 @agent_ppo2.py:189][0m |           0.0020 |           9.5432 |           4.9650 |
[32m[20230114 18:50:58 @agent_ppo2.py:189][0m |          -0.0102 |           8.2117 |           4.9642 |
[32m[20230114 18:50:58 @agent_ppo2.py:189][0m |          -0.0063 |           7.8035 |           4.9694 |
[32m[20230114 18:50:58 @agent_ppo2.py:189][0m |          -0.0112 |           7.3140 |           4.9661 |
[32m[20230114 18:50:59 @agent_ppo2.py:189][0m |          -0.0112 |           6.9203 |           4.9677 |
[32m[20230114 18:50:59 @agent_ppo2.py:189][0m |          -0.0080 |           6.7028 |           4.9649 |
[32m[20230114 18:50:59 @agent_ppo2.py:189][0m |          -0.0102 |           6.1983 |           4.9648 |
[32m[20230114 18:50:59 @agent_ppo2.py:189][0m |          -0.0131 |           5.9242 |           4.9708 |
[32m[20230114 18:50:59 @agent_ppo2.py:134][0m Policy update time: 0.79 s
[32m[20230114 18:50:59 @agent_ppo2.py:142][0m Average TRAINING episode reward: 259.26
[32m[20230114 18:50:59 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 261.40
[32m[20230114 18:50:59 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 267.10
[32m[20230114 18:50:59 @agent_ppo2.py:147][0m Total time:       8.91 min
[32m[20230114 18:50:59 @agent_ppo2.py:149][0m 747520 total steps have happened
[32m[20230114 18:50:59 @agent_ppo2.py:125][0m #------------------------ Iteration 365 --------------------------#
[32m[20230114 18:50:59 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:50:59 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:50:59 @agent_ppo2.py:189][0m |          -0.0015 |          29.6546 |           4.9960 |
[32m[20230114 18:50:59 @agent_ppo2.py:189][0m |          -0.0078 |          20.2428 |           4.9946 |
[32m[20230114 18:51:00 @agent_ppo2.py:189][0m |          -0.0102 |          18.1989 |           4.9884 |
[32m[20230114 18:51:00 @agent_ppo2.py:189][0m |          -0.0094 |          16.6746 |           4.9849 |
[32m[20230114 18:51:00 @agent_ppo2.py:189][0m |          -0.0114 |          16.0279 |           4.9873 |
[32m[20230114 18:51:00 @agent_ppo2.py:189][0m |          -0.0120 |          15.4550 |           4.9842 |
[32m[20230114 18:51:00 @agent_ppo2.py:189][0m |          -0.0128 |          14.8506 |           4.9849 |
[32m[20230114 18:51:00 @agent_ppo2.py:189][0m |          -0.0134 |          14.5474 |           4.9843 |
[32m[20230114 18:51:00 @agent_ppo2.py:189][0m |          -0.0134 |          14.2811 |           4.9853 |
[32m[20230114 18:51:00 @agent_ppo2.py:189][0m |          -0.0143 |          13.9847 |           4.9881 |
[32m[20230114 18:51:00 @agent_ppo2.py:134][0m Policy update time: 0.76 s
[32m[20230114 18:51:00 @agent_ppo2.py:142][0m Average TRAINING episode reward: 224.81
[32m[20230114 18:51:00 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 262.34
[32m[20230114 18:51:00 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 267.00
[32m[20230114 18:51:00 @agent_ppo2.py:147][0m Total time:       8.93 min
[32m[20230114 18:51:00 @agent_ppo2.py:149][0m 749568 total steps have happened
[32m[20230114 18:51:00 @agent_ppo2.py:125][0m #------------------------ Iteration 366 --------------------------#
[32m[20230114 18:51:00 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:51:01 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:51:01 @agent_ppo2.py:189][0m |           0.0017 |          14.5123 |           4.9414 |
[32m[20230114 18:51:01 @agent_ppo2.py:189][0m |          -0.0039 |          12.0736 |           4.9318 |
[32m[20230114 18:51:01 @agent_ppo2.py:189][0m |          -0.0029 |          11.3506 |           4.9284 |
[32m[20230114 18:51:01 @agent_ppo2.py:189][0m |          -0.0054 |          10.9005 |           4.9283 |
[32m[20230114 18:51:01 @agent_ppo2.py:189][0m |          -0.0092 |          10.6191 |           4.9270 |
[32m[20230114 18:51:01 @agent_ppo2.py:189][0m |          -0.0081 |          10.3025 |           4.9307 |
[32m[20230114 18:51:01 @agent_ppo2.py:189][0m |          -0.0091 |           9.8975 |           4.9295 |
[32m[20230114 18:51:01 @agent_ppo2.py:189][0m |          -0.0039 |           9.6310 |           4.9316 |
[32m[20230114 18:51:01 @agent_ppo2.py:189][0m |          -0.0136 |           9.5320 |           4.9301 |
[32m[20230114 18:51:01 @agent_ppo2.py:189][0m |          -0.0136 |           9.1565 |           4.9289 |
[32m[20230114 18:51:01 @agent_ppo2.py:134][0m Policy update time: 0.79 s
[32m[20230114 18:51:01 @agent_ppo2.py:142][0m Average TRAINING episode reward: 260.05
[32m[20230114 18:51:01 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 261.13
[32m[20230114 18:51:01 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 264.95
[32m[20230114 18:51:01 @agent_ppo2.py:147][0m Total time:       8.95 min
[32m[20230114 18:51:01 @agent_ppo2.py:149][0m 751616 total steps have happened
[32m[20230114 18:51:01 @agent_ppo2.py:125][0m #------------------------ Iteration 367 --------------------------#
[32m[20230114 18:51:02 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:51:02 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:51:02 @agent_ppo2.py:189][0m |           0.0010 |          15.0627 |           4.9356 |
[32m[20230114 18:51:02 @agent_ppo2.py:189][0m |          -0.0065 |          13.3618 |           4.9300 |
[32m[20230114 18:51:02 @agent_ppo2.py:189][0m |          -0.0076 |          12.6753 |           4.9260 |
[32m[20230114 18:51:02 @agent_ppo2.py:189][0m |          -0.0087 |          12.1877 |           4.9274 |
[32m[20230114 18:51:02 @agent_ppo2.py:189][0m |          -0.0108 |          11.9075 |           4.9312 |
[32m[20230114 18:51:02 @agent_ppo2.py:189][0m |          -0.0109 |          11.6068 |           4.9251 |
[32m[20230114 18:51:02 @agent_ppo2.py:189][0m |          -0.0122 |          11.4644 |           4.9232 |
[32m[20230114 18:51:02 @agent_ppo2.py:189][0m |          -0.0124 |          11.3048 |           4.9282 |
[32m[20230114 18:51:02 @agent_ppo2.py:189][0m |          -0.0120 |          11.1798 |           4.9245 |
[32m[20230114 18:51:02 @agent_ppo2.py:189][0m |          -0.0129 |          11.1475 |           4.9206 |
[32m[20230114 18:51:02 @agent_ppo2.py:134][0m Policy update time: 0.80 s
[32m[20230114 18:51:03 @agent_ppo2.py:142][0m Average TRAINING episode reward: 258.69
[32m[20230114 18:51:03 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 261.53
[32m[20230114 18:51:03 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 266.28
[32m[20230114 18:51:03 @agent_ppo2.py:147][0m Total time:       8.98 min
[32m[20230114 18:51:03 @agent_ppo2.py:149][0m 753664 total steps have happened
[32m[20230114 18:51:03 @agent_ppo2.py:125][0m #------------------------ Iteration 368 --------------------------#
[32m[20230114 18:51:03 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:51:03 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:51:03 @agent_ppo2.py:189][0m |           0.0023 |          32.0630 |           4.9992 |
[32m[20230114 18:51:03 @agent_ppo2.py:189][0m |          -0.0033 |          25.3121 |           4.9963 |
[32m[20230114 18:51:03 @agent_ppo2.py:189][0m |          -0.0063 |          23.0089 |           4.9909 |
[32m[20230114 18:51:03 @agent_ppo2.py:189][0m |          -0.0081 |          21.7048 |           4.9909 |
[32m[20230114 18:51:03 @agent_ppo2.py:189][0m |          -0.0087 |          20.5333 |           4.9900 |
[32m[20230114 18:51:03 @agent_ppo2.py:189][0m |          -0.0103 |          19.6283 |           4.9924 |
[32m[20230114 18:51:04 @agent_ppo2.py:189][0m |          -0.0106 |          18.9306 |           4.9904 |
[32m[20230114 18:51:04 @agent_ppo2.py:189][0m |          -0.0118 |          18.3262 |           4.9917 |
[32m[20230114 18:51:04 @agent_ppo2.py:189][0m |          -0.0117 |          17.8319 |           4.9921 |
[32m[20230114 18:51:04 @agent_ppo2.py:189][0m |          -0.0125 |          17.4686 |           4.9919 |
[32m[20230114 18:51:04 @agent_ppo2.py:134][0m Policy update time: 0.74 s
[32m[20230114 18:51:04 @agent_ppo2.py:142][0m Average TRAINING episode reward: 215.63
[32m[20230114 18:51:04 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 261.71
[32m[20230114 18:51:04 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 266.90
[32m[20230114 18:51:04 @agent_ppo2.py:147][0m Total time:       9.00 min
[32m[20230114 18:51:04 @agent_ppo2.py:149][0m 755712 total steps have happened
[32m[20230114 18:51:04 @agent_ppo2.py:125][0m #------------------------ Iteration 369 --------------------------#
[32m[20230114 18:51:04 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:51:04 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:51:04 @agent_ppo2.py:189][0m |          -0.0023 |          23.0831 |           4.9895 |
[32m[20230114 18:51:04 @agent_ppo2.py:189][0m |          -0.0104 |          18.0768 |           4.9770 |
[32m[20230114 18:51:04 @agent_ppo2.py:189][0m |          -0.0134 |          16.2874 |           4.9756 |
[32m[20230114 18:51:04 @agent_ppo2.py:189][0m |          -0.0153 |          15.1584 |           4.9705 |
[32m[20230114 18:51:05 @agent_ppo2.py:189][0m |          -0.0124 |          14.8894 |           4.9689 |
[32m[20230114 18:51:05 @agent_ppo2.py:189][0m |          -0.0138 |          14.0087 |           4.9708 |
[32m[20230114 18:51:05 @agent_ppo2.py:189][0m |          -0.0167 |          13.6108 |           4.9702 |
[32m[20230114 18:51:05 @agent_ppo2.py:189][0m |          -0.0138 |          13.6690 |           4.9668 |
[32m[20230114 18:51:05 @agent_ppo2.py:189][0m |          -0.0200 |          13.0419 |           4.9683 |
[32m[20230114 18:51:05 @agent_ppo2.py:189][0m |          -0.0191 |          12.8101 |           4.9702 |
[32m[20230114 18:51:05 @agent_ppo2.py:134][0m Policy update time: 0.70 s
[32m[20230114 18:51:05 @agent_ppo2.py:142][0m Average TRAINING episode reward: 196.31
[32m[20230114 18:51:05 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 259.29
[32m[20230114 18:51:05 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 260.18
[32m[20230114 18:51:05 @agent_ppo2.py:147][0m Total time:       9.02 min
[32m[20230114 18:51:05 @agent_ppo2.py:149][0m 757760 total steps have happened
[32m[20230114 18:51:05 @agent_ppo2.py:125][0m #------------------------ Iteration 370 --------------------------#
[32m[20230114 18:51:05 @agent_ppo2.py:131][0m Sampling time: 0.25 s by 4 slaves
[32m[20230114 18:51:05 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:51:05 @agent_ppo2.py:189][0m |          -0.0012 |          12.7740 |           5.0458 |
[32m[20230114 18:51:06 @agent_ppo2.py:189][0m |          -0.0077 |          10.1677 |           5.0461 |
[32m[20230114 18:51:06 @agent_ppo2.py:189][0m |          -0.0095 |           9.5365 |           5.0417 |
[32m[20230114 18:51:06 @agent_ppo2.py:189][0m |          -0.0102 |           9.1351 |           5.0376 |
[32m[20230114 18:51:06 @agent_ppo2.py:189][0m |          -0.0114 |           8.8459 |           5.0393 |
[32m[20230114 18:51:06 @agent_ppo2.py:189][0m |          -0.0116 |           8.6235 |           5.0359 |
[32m[20230114 18:51:06 @agent_ppo2.py:189][0m |          -0.0126 |           8.3285 |           5.0379 |
[32m[20230114 18:51:06 @agent_ppo2.py:189][0m |          -0.0132 |           8.1202 |           5.0330 |
[32m[20230114 18:51:06 @agent_ppo2.py:189][0m |          -0.0132 |           7.9890 |           5.0383 |
[32m[20230114 18:51:06 @agent_ppo2.py:189][0m |          -0.0140 |           7.8003 |           5.0364 |
[32m[20230114 18:51:06 @agent_ppo2.py:134][0m Policy update time: 0.80 s
[32m[20230114 18:51:06 @agent_ppo2.py:142][0m Average TRAINING episode reward: 257.07
[32m[20230114 18:51:06 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 261.67
[32m[20230114 18:51:06 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 270.93
[32m[20230114 18:51:06 @agent_ppo2.py:147][0m Total time:       9.04 min
[32m[20230114 18:51:06 @agent_ppo2.py:149][0m 759808 total steps have happened
[32m[20230114 18:51:06 @agent_ppo2.py:125][0m #------------------------ Iteration 371 --------------------------#
[32m[20230114 18:51:07 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:51:07 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:51:07 @agent_ppo2.py:189][0m |           0.0006 |          14.6672 |           4.9444 |
[32m[20230114 18:51:07 @agent_ppo2.py:189][0m |          -0.0063 |          12.2750 |           4.9274 |
[32m[20230114 18:51:07 @agent_ppo2.py:189][0m |          -0.0054 |          11.9551 |           4.9297 |
[32m[20230114 18:51:07 @agent_ppo2.py:189][0m |          -0.0105 |          11.0967 |           4.9306 |
[32m[20230114 18:51:07 @agent_ppo2.py:189][0m |          -0.0110 |          10.6728 |           4.9326 |
[32m[20230114 18:51:07 @agent_ppo2.py:189][0m |          -0.0114 |          10.3955 |           4.9313 |
[32m[20230114 18:51:07 @agent_ppo2.py:189][0m |          -0.0074 |          10.6496 |           4.9290 |
[32m[20230114 18:51:07 @agent_ppo2.py:189][0m |          -0.0135 |           9.8633 |           4.9215 |
[32m[20230114 18:51:07 @agent_ppo2.py:189][0m |          -0.0138 |           9.6338 |           4.9345 |
[32m[20230114 18:51:07 @agent_ppo2.py:189][0m |          -0.0169 |           9.5214 |           4.9313 |
[32m[20230114 18:51:07 @agent_ppo2.py:134][0m Policy update time: 0.79 s
[32m[20230114 18:51:08 @agent_ppo2.py:142][0m Average TRAINING episode reward: 259.46
[32m[20230114 18:51:08 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 262.37
[32m[20230114 18:51:08 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 268.98
[32m[20230114 18:51:08 @agent_ppo2.py:147][0m Total time:       9.06 min
[32m[20230114 18:51:08 @agent_ppo2.py:149][0m 761856 total steps have happened
[32m[20230114 18:51:08 @agent_ppo2.py:125][0m #------------------------ Iteration 372 --------------------------#
[32m[20230114 18:51:08 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:51:08 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:51:08 @agent_ppo2.py:189][0m |           0.0012 |          14.1371 |           5.1843 |
[32m[20230114 18:51:08 @agent_ppo2.py:189][0m |          -0.0042 |          12.6661 |           5.1765 |
[32m[20230114 18:51:08 @agent_ppo2.py:189][0m |          -0.0057 |          11.4223 |           5.1843 |
[32m[20230114 18:51:08 @agent_ppo2.py:189][0m |          -0.0074 |          10.5625 |           5.1752 |
[32m[20230114 18:51:08 @agent_ppo2.py:189][0m |          -0.0085 |          10.0595 |           5.1729 |
[32m[20230114 18:51:08 @agent_ppo2.py:189][0m |          -0.0088 |           9.6686 |           5.1698 |
[32m[20230114 18:51:08 @agent_ppo2.py:189][0m |          -0.0099 |           9.4450 |           5.1728 |
[32m[20230114 18:51:08 @agent_ppo2.py:189][0m |          -0.0099 |           9.1912 |           5.1682 |
[32m[20230114 18:51:09 @agent_ppo2.py:189][0m |          -0.0108 |           8.9953 |           5.1705 |
[32m[20230114 18:51:09 @agent_ppo2.py:189][0m |          -0.0117 |           8.8058 |           5.1663 |
[32m[20230114 18:51:09 @agent_ppo2.py:134][0m Policy update time: 0.80 s
[32m[20230114 18:51:09 @agent_ppo2.py:142][0m Average TRAINING episode reward: 260.51
[32m[20230114 18:51:09 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 263.49
[32m[20230114 18:51:09 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 269.55
[32m[20230114 18:51:09 @agent_ppo2.py:147][0m Total time:       9.08 min
[32m[20230114 18:51:09 @agent_ppo2.py:149][0m 763904 total steps have happened
[32m[20230114 18:51:09 @agent_ppo2.py:125][0m #------------------------ Iteration 373 --------------------------#
[32m[20230114 18:51:09 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:51:09 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:51:09 @agent_ppo2.py:189][0m |           0.0026 |          13.2017 |           4.9248 |
[32m[20230114 18:51:09 @agent_ppo2.py:189][0m |          -0.0064 |          12.1361 |           4.9204 |
[32m[20230114 18:51:09 @agent_ppo2.py:189][0m |          -0.0065 |          11.6330 |           4.9171 |
[32m[20230114 18:51:09 @agent_ppo2.py:189][0m |          -0.0074 |          11.2737 |           4.9121 |
[32m[20230114 18:51:09 @agent_ppo2.py:189][0m |          -0.0076 |          10.9762 |           4.9151 |
[32m[20230114 18:51:10 @agent_ppo2.py:189][0m |          -0.0093 |          10.6896 |           4.9130 |
[32m[20230114 18:51:10 @agent_ppo2.py:189][0m |          -0.0119 |          10.4443 |           4.9058 |
[32m[20230114 18:51:10 @agent_ppo2.py:189][0m |          -0.0086 |          10.3086 |           4.9098 |
[32m[20230114 18:51:10 @agent_ppo2.py:189][0m |          -0.0107 |          10.1328 |           4.9071 |
[32m[20230114 18:51:10 @agent_ppo2.py:189][0m |          -0.0102 |           9.9798 |           4.9115 |
[32m[20230114 18:51:10 @agent_ppo2.py:134][0m Policy update time: 0.78 s
[32m[20230114 18:51:10 @agent_ppo2.py:142][0m Average TRAINING episode reward: 262.49
[32m[20230114 18:51:10 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 266.60
[32m[20230114 18:51:10 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 266.93
[32m[20230114 18:51:10 @agent_ppo2.py:147][0m Total time:       9.10 min
[32m[20230114 18:51:10 @agent_ppo2.py:149][0m 765952 total steps have happened
[32m[20230114 18:51:10 @agent_ppo2.py:125][0m #------------------------ Iteration 374 --------------------------#
[32m[20230114 18:51:10 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:51:10 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:51:10 @agent_ppo2.py:189][0m |           0.0036 |          13.8767 |           4.9737 |
[32m[20230114 18:51:10 @agent_ppo2.py:189][0m |          -0.0041 |          12.3971 |           4.9626 |
[32m[20230114 18:51:11 @agent_ppo2.py:189][0m |          -0.0066 |          11.8781 |           4.9625 |
[32m[20230114 18:51:11 @agent_ppo2.py:189][0m |          -0.0079 |          11.5454 |           4.9620 |
[32m[20230114 18:51:11 @agent_ppo2.py:189][0m |          -0.0090 |          11.2813 |           4.9584 |
[32m[20230114 18:51:11 @agent_ppo2.py:189][0m |          -0.0098 |          11.0286 |           4.9647 |
[32m[20230114 18:51:11 @agent_ppo2.py:189][0m |          -0.0090 |          10.8753 |           4.9614 |
[32m[20230114 18:51:11 @agent_ppo2.py:189][0m |          -0.0095 |          10.7560 |           4.9615 |
[32m[20230114 18:51:11 @agent_ppo2.py:189][0m |          -0.0110 |          10.4961 |           4.9599 |
[32m[20230114 18:51:11 @agent_ppo2.py:189][0m |          -0.0123 |          10.3343 |           4.9601 |
[32m[20230114 18:51:11 @agent_ppo2.py:134][0m Policy update time: 0.78 s
[32m[20230114 18:51:11 @agent_ppo2.py:142][0m Average TRAINING episode reward: 261.18
[32m[20230114 18:51:11 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 263.52
[32m[20230114 18:51:11 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 264.85
[32m[20230114 18:51:11 @agent_ppo2.py:147][0m Total time:       9.12 min
[32m[20230114 18:51:11 @agent_ppo2.py:149][0m 768000 total steps have happened
[32m[20230114 18:51:11 @agent_ppo2.py:125][0m #------------------------ Iteration 375 --------------------------#
[32m[20230114 18:51:11 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:51:12 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:51:12 @agent_ppo2.py:189][0m |          -0.0022 |          14.8203 |           4.9918 |
[32m[20230114 18:51:12 @agent_ppo2.py:189][0m |          -0.0028 |          13.6920 |           4.9760 |
[32m[20230114 18:51:12 @agent_ppo2.py:189][0m |          -0.0055 |          13.1825 |           4.9793 |
[32m[20230114 18:51:12 @agent_ppo2.py:189][0m |           0.0085 |          13.5435 |           4.9850 |
[32m[20230114 18:51:12 @agent_ppo2.py:189][0m |          -0.0092 |          12.3290 |           4.9661 |
[32m[20230114 18:51:12 @agent_ppo2.py:189][0m |          -0.0078 |          12.0056 |           4.9777 |
[32m[20230114 18:51:12 @agent_ppo2.py:189][0m |          -0.0099 |          11.5810 |           4.9768 |
[32m[20230114 18:51:12 @agent_ppo2.py:189][0m |          -0.0029 |          11.5783 |           4.9808 |
[32m[20230114 18:51:12 @agent_ppo2.py:189][0m |          -0.0097 |          11.0059 |           4.9793 |
[32m[20230114 18:51:12 @agent_ppo2.py:189][0m |          -0.0121 |          10.7593 |           4.9774 |
[32m[20230114 18:51:12 @agent_ppo2.py:134][0m Policy update time: 0.79 s
[32m[20230114 18:51:13 @agent_ppo2.py:142][0m Average TRAINING episode reward: 260.82
[32m[20230114 18:51:13 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 263.48
[32m[20230114 18:51:13 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 268.60
[32m[20230114 18:51:13 @agent_ppo2.py:147][0m Total time:       9.14 min
[32m[20230114 18:51:13 @agent_ppo2.py:149][0m 770048 total steps have happened
[32m[20230114 18:51:13 @agent_ppo2.py:125][0m #------------------------ Iteration 376 --------------------------#
[32m[20230114 18:51:13 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:51:13 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:51:13 @agent_ppo2.py:189][0m |          -0.0001 |          14.1746 |           5.0539 |
[32m[20230114 18:51:13 @agent_ppo2.py:189][0m |          -0.0036 |          12.7600 |           5.0521 |
[32m[20230114 18:51:13 @agent_ppo2.py:189][0m |          -0.0052 |          12.1429 |           5.0468 |
[32m[20230114 18:51:13 @agent_ppo2.py:189][0m |          -0.0074 |          11.6306 |           5.0425 |
[32m[20230114 18:51:13 @agent_ppo2.py:189][0m |          -0.0079 |          11.2056 |           5.0379 |
[32m[20230114 18:51:13 @agent_ppo2.py:189][0m |          -0.0088 |          10.7887 |           5.0423 |
[32m[20230114 18:51:13 @agent_ppo2.py:189][0m |          -0.0096 |          10.4549 |           5.0369 |
[32m[20230114 18:51:13 @agent_ppo2.py:189][0m |          -0.0112 |           9.9881 |           5.0384 |
[32m[20230114 18:51:13 @agent_ppo2.py:189][0m |          -0.0105 |           9.6645 |           5.0356 |
[32m[20230114 18:51:14 @agent_ppo2.py:189][0m |          -0.0115 |           9.3040 |           5.0330 |
[32m[20230114 18:51:14 @agent_ppo2.py:134][0m Policy update time: 0.78 s
[32m[20230114 18:51:14 @agent_ppo2.py:142][0m Average TRAINING episode reward: 260.29
[32m[20230114 18:51:14 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 264.26
[32m[20230114 18:51:14 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 268.94
[32m[20230114 18:51:14 @agent_ppo2.py:147][0m Total time:       9.16 min
[32m[20230114 18:51:14 @agent_ppo2.py:149][0m 772096 total steps have happened
[32m[20230114 18:51:14 @agent_ppo2.py:125][0m #------------------------ Iteration 377 --------------------------#
[32m[20230114 18:51:14 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:51:14 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:51:14 @agent_ppo2.py:189][0m |           0.0009 |          28.8088 |           5.0431 |
[32m[20230114 18:51:14 @agent_ppo2.py:189][0m |          -0.0057 |          22.9587 |           5.0331 |
[32m[20230114 18:51:14 @agent_ppo2.py:189][0m |          -0.0080 |          20.6103 |           5.0385 |
[32m[20230114 18:51:14 @agent_ppo2.py:189][0m |          -0.0097 |          19.3865 |           5.0304 |
[32m[20230114 18:51:14 @agent_ppo2.py:189][0m |          -0.0104 |          18.4027 |           5.0316 |
[32m[20230114 18:51:14 @agent_ppo2.py:189][0m |          -0.0108 |          17.5572 |           5.0293 |
[32m[20230114 18:51:14 @agent_ppo2.py:189][0m |          -0.0116 |          16.7513 |           5.0248 |
[32m[20230114 18:51:15 @agent_ppo2.py:189][0m |          -0.0133 |          16.1108 |           5.0282 |
[32m[20230114 18:51:15 @agent_ppo2.py:189][0m |          -0.0136 |          15.2363 |           5.0251 |
[32m[20230114 18:51:15 @agent_ppo2.py:189][0m |          -0.0138 |          14.8858 |           5.0280 |
[32m[20230114 18:51:15 @agent_ppo2.py:134][0m Policy update time: 0.74 s
[32m[20230114 18:51:15 @agent_ppo2.py:142][0m Average TRAINING episode reward: 220.98
[32m[20230114 18:51:15 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 263.60
[32m[20230114 18:51:15 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 271.44
[32m[20230114 18:51:15 @agent_ppo2.py:147][0m Total time:       9.18 min
[32m[20230114 18:51:15 @agent_ppo2.py:149][0m 774144 total steps have happened
[32m[20230114 18:51:15 @agent_ppo2.py:125][0m #------------------------ Iteration 378 --------------------------#
[32m[20230114 18:51:15 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:51:15 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:51:15 @agent_ppo2.py:189][0m |          -0.0011 |          15.0845 |           5.0827 |
[32m[20230114 18:51:15 @agent_ppo2.py:189][0m |          -0.0057 |          13.5320 |           5.0674 |
[32m[20230114 18:51:15 @agent_ppo2.py:189][0m |          -0.0081 |          12.8680 |           5.0623 |
[32m[20230114 18:51:15 @agent_ppo2.py:189][0m |          -0.0095 |          12.4335 |           5.0692 |
[32m[20230114 18:51:16 @agent_ppo2.py:189][0m |          -0.0096 |          12.1149 |           5.0641 |
[32m[20230114 18:51:16 @agent_ppo2.py:189][0m |          -0.0092 |          11.9103 |           5.0625 |
[32m[20230114 18:51:16 @agent_ppo2.py:189][0m |          -0.0094 |          11.7508 |           5.0673 |
[32m[20230114 18:51:16 @agent_ppo2.py:189][0m |          -0.0100 |          11.7095 |           5.0643 |
[32m[20230114 18:51:16 @agent_ppo2.py:189][0m |          -0.0119 |          11.3207 |           5.0616 |
[32m[20230114 18:51:16 @agent_ppo2.py:189][0m |          -0.0133 |          11.1814 |           5.0629 |
[32m[20230114 18:51:16 @agent_ppo2.py:134][0m Policy update time: 0.77 s
[32m[20230114 18:51:16 @agent_ppo2.py:142][0m Average TRAINING episode reward: 262.72
[32m[20230114 18:51:16 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 266.08
[32m[20230114 18:51:16 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 264.02
[32m[20230114 18:51:16 @agent_ppo2.py:147][0m Total time:       9.20 min
[32m[20230114 18:51:16 @agent_ppo2.py:149][0m 776192 total steps have happened
[32m[20230114 18:51:16 @agent_ppo2.py:125][0m #------------------------ Iteration 379 --------------------------#
[32m[20230114 18:51:16 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:51:16 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:51:17 @agent_ppo2.py:189][0m |          -0.0006 |          14.8002 |           4.9968 |
[32m[20230114 18:51:17 @agent_ppo2.py:189][0m |          -0.0038 |          14.0918 |           4.9935 |
[32m[20230114 18:51:17 @agent_ppo2.py:189][0m |          -0.0027 |          13.8443 |           4.9899 |
[32m[20230114 18:51:17 @agent_ppo2.py:189][0m |          -0.0097 |          13.5466 |           4.9906 |
[32m[20230114 18:51:17 @agent_ppo2.py:189][0m |          -0.0094 |          13.3918 |           4.9904 |
[32m[20230114 18:51:17 @agent_ppo2.py:189][0m |          -0.0069 |          13.3197 |           4.9821 |
[32m[20230114 18:51:17 @agent_ppo2.py:189][0m |          -0.0091 |          13.1842 |           4.9884 |
[32m[20230114 18:51:17 @agent_ppo2.py:189][0m |          -0.0100 |          13.0517 |           4.9834 |
[32m[20230114 18:51:17 @agent_ppo2.py:189][0m |          -0.0077 |          13.1091 |           4.9850 |
[32m[20230114 18:51:17 @agent_ppo2.py:189][0m |          -0.0102 |          12.9016 |           4.9805 |
[32m[20230114 18:51:17 @agent_ppo2.py:134][0m Policy update time: 0.80 s
[32m[20230114 18:51:17 @agent_ppo2.py:142][0m Average TRAINING episode reward: 257.98
[32m[20230114 18:51:17 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 259.55
[32m[20230114 18:51:17 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 128.30
[32m[20230114 18:51:17 @agent_ppo2.py:147][0m Total time:       9.22 min
[32m[20230114 18:51:17 @agent_ppo2.py:149][0m 778240 total steps have happened
[32m[20230114 18:51:17 @agent_ppo2.py:125][0m #------------------------ Iteration 380 --------------------------#
[32m[20230114 18:51:18 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:51:18 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:51:18 @agent_ppo2.py:189][0m |           0.0052 |          12.5012 |           5.0231 |
[32m[20230114 18:51:18 @agent_ppo2.py:189][0m |          -0.0031 |           8.3763 |           5.0178 |
[32m[20230114 18:51:18 @agent_ppo2.py:189][0m |          -0.0066 |           6.8126 |           5.0114 |
[32m[20230114 18:51:18 @agent_ppo2.py:189][0m |          -0.0076 |           6.2167 |           5.0064 |
[32m[20230114 18:51:18 @agent_ppo2.py:189][0m |          -0.0083 |           5.8453 |           5.0020 |
[32m[20230114 18:51:18 @agent_ppo2.py:189][0m |          -0.0069 |           5.5194 |           4.9995 |
[32m[20230114 18:51:18 @agent_ppo2.py:189][0m |          -0.0053 |           5.1726 |           4.9927 |
[32m[20230114 18:51:18 @agent_ppo2.py:189][0m |          -0.0091 |           5.1014 |           4.9938 |
[32m[20230114 18:51:18 @agent_ppo2.py:189][0m |          -0.0103 |           4.8366 |           4.9845 |
[32m[20230114 18:51:18 @agent_ppo2.py:189][0m |          -0.0103 |           4.8400 |           4.9886 |
[32m[20230114 18:51:18 @agent_ppo2.py:134][0m Policy update time: 0.79 s
[32m[20230114 18:51:19 @agent_ppo2.py:142][0m Average TRAINING episode reward: 259.56
[32m[20230114 18:51:19 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 261.70
[32m[20230114 18:51:19 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 266.58
[32m[20230114 18:51:19 @agent_ppo2.py:147][0m Total time:       9.24 min
[32m[20230114 18:51:19 @agent_ppo2.py:149][0m 780288 total steps have happened
[32m[20230114 18:51:19 @agent_ppo2.py:125][0m #------------------------ Iteration 381 --------------------------#
[32m[20230114 18:51:19 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:51:19 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:51:19 @agent_ppo2.py:189][0m |          -0.0017 |          13.6425 |           4.9373 |
[32m[20230114 18:51:19 @agent_ppo2.py:189][0m |          -0.0062 |          12.7093 |           4.9188 |
[32m[20230114 18:51:19 @agent_ppo2.py:189][0m |          -0.0081 |          12.2423 |           4.9275 |
[32m[20230114 18:51:19 @agent_ppo2.py:189][0m |           0.0114 |          13.3815 |           4.9322 |
[32m[20230114 18:51:19 @agent_ppo2.py:189][0m |          -0.0106 |          11.6194 |           4.9178 |
[32m[20230114 18:51:19 @agent_ppo2.py:189][0m |          -0.0039 |          11.3043 |           4.9237 |
[32m[20230114 18:51:19 @agent_ppo2.py:189][0m |          -0.0084 |          11.0869 |           4.9236 |
[32m[20230114 18:51:20 @agent_ppo2.py:189][0m |          -0.0112 |          10.8769 |           4.9313 |
[32m[20230114 18:51:20 @agent_ppo2.py:189][0m |          -0.0156 |          10.7356 |           4.9281 |
[32m[20230114 18:51:20 @agent_ppo2.py:189][0m |          -0.0146 |          10.4731 |           4.9300 |
[32m[20230114 18:51:20 @agent_ppo2.py:134][0m Policy update time: 0.79 s
[32m[20230114 18:51:20 @agent_ppo2.py:142][0m Average TRAINING episode reward: 260.47
[32m[20230114 18:51:20 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 264.22
[32m[20230114 18:51:20 @agent_ppo2.py:144][0m Average EVALUATION episode reward: -8.30
[32m[20230114 18:51:20 @agent_ppo2.py:147][0m Total time:       9.26 min
[32m[20230114 18:51:20 @agent_ppo2.py:149][0m 782336 total steps have happened
[32m[20230114 18:51:20 @agent_ppo2.py:125][0m #------------------------ Iteration 382 --------------------------#
[32m[20230114 18:51:20 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:51:20 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:51:20 @agent_ppo2.py:189][0m |           0.0013 |          13.5725 |           4.9633 |
[32m[20230114 18:51:20 @agent_ppo2.py:189][0m |          -0.0041 |          11.8315 |           4.9649 |
[32m[20230114 18:51:20 @agent_ppo2.py:189][0m |          -0.0063 |          11.1299 |           4.9641 |
[32m[20230114 18:51:20 @agent_ppo2.py:189][0m |          -0.0081 |          10.5649 |           4.9612 |
[32m[20230114 18:51:21 @agent_ppo2.py:189][0m |          -0.0079 |          10.1381 |           4.9611 |
[32m[20230114 18:51:21 @agent_ppo2.py:189][0m |          -0.0085 |           9.8024 |           4.9607 |
[32m[20230114 18:51:21 @agent_ppo2.py:189][0m |          -0.0101 |           9.4962 |           4.9609 |
[32m[20230114 18:51:21 @agent_ppo2.py:189][0m |          -0.0099 |           9.2804 |           4.9581 |
[32m[20230114 18:51:21 @agent_ppo2.py:189][0m |          -0.0115 |           9.0416 |           4.9583 |
[32m[20230114 18:51:21 @agent_ppo2.py:189][0m |          -0.0117 |           8.8117 |           4.9594 |
[32m[20230114 18:51:21 @agent_ppo2.py:134][0m Policy update time: 0.80 s
[32m[20230114 18:51:21 @agent_ppo2.py:142][0m Average TRAINING episode reward: 259.96
[32m[20230114 18:51:21 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 263.11
[32m[20230114 18:51:21 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 270.54
[32m[20230114 18:51:21 @agent_ppo2.py:147][0m Total time:       9.28 min
[32m[20230114 18:51:21 @agent_ppo2.py:149][0m 784384 total steps have happened
[32m[20230114 18:51:21 @agent_ppo2.py:125][0m #------------------------ Iteration 383 --------------------------#
[32m[20230114 18:51:21 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:51:21 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:51:21 @agent_ppo2.py:189][0m |          -0.0002 |          26.7212 |           4.9001 |
[32m[20230114 18:51:22 @agent_ppo2.py:189][0m |          -0.0153 |          20.0544 |           4.8925 |
[32m[20230114 18:51:22 @agent_ppo2.py:189][0m |          -0.0115 |          18.2628 |           4.8861 |
[32m[20230114 18:51:22 @agent_ppo2.py:189][0m |          -0.0134 |          18.6276 |           4.8881 |
[32m[20230114 18:51:22 @agent_ppo2.py:189][0m |          -0.0202 |          17.7323 |           4.8884 |
[32m[20230114 18:51:22 @agent_ppo2.py:189][0m |          -0.0156 |          16.8087 |           4.8788 |
[32m[20230114 18:51:22 @agent_ppo2.py:189][0m |          -0.0161 |          16.5176 |           4.8874 |
[32m[20230114 18:51:22 @agent_ppo2.py:189][0m |          -0.0154 |          16.2551 |           4.8848 |
[32m[20230114 18:51:22 @agent_ppo2.py:189][0m |          -0.0099 |          16.2170 |           4.8855 |
[32m[20230114 18:51:22 @agent_ppo2.py:189][0m |          -0.0168 |          15.8627 |           4.8910 |
[32m[20230114 18:51:22 @agent_ppo2.py:134][0m Policy update time: 0.77 s
[32m[20230114 18:51:22 @agent_ppo2.py:142][0m Average TRAINING episode reward: 230.85
[32m[20230114 18:51:22 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 262.95
[32m[20230114 18:51:22 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 269.94
[32m[20230114 18:51:22 @agent_ppo2.py:147][0m Total time:       9.30 min
[32m[20230114 18:51:22 @agent_ppo2.py:149][0m 786432 total steps have happened
[32m[20230114 18:51:22 @agent_ppo2.py:125][0m #------------------------ Iteration 384 --------------------------#
[32m[20230114 18:51:23 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:51:23 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:51:23 @agent_ppo2.py:189][0m |           0.0027 |          13.1662 |           4.9842 |
[32m[20230114 18:51:23 @agent_ppo2.py:189][0m |          -0.0050 |          11.3497 |           4.9763 |
[32m[20230114 18:51:23 @agent_ppo2.py:189][0m |          -0.0095 |          10.8637 |           4.9754 |
[32m[20230114 18:51:23 @agent_ppo2.py:189][0m |          -0.0074 |          10.6958 |           4.9737 |
[32m[20230114 18:51:23 @agent_ppo2.py:189][0m |          -0.0115 |          10.3492 |           4.9766 |
[32m[20230114 18:51:23 @agent_ppo2.py:189][0m |          -0.0079 |          10.3682 |           4.9726 |
[32m[20230114 18:51:23 @agent_ppo2.py:189][0m |          -0.0137 |           9.9422 |           4.9736 |
[32m[20230114 18:51:23 @agent_ppo2.py:189][0m |          -0.0126 |           9.7810 |           4.9757 |
[32m[20230114 18:51:23 @agent_ppo2.py:189][0m |          -0.0143 |           9.5980 |           4.9677 |
[32m[20230114 18:51:23 @agent_ppo2.py:189][0m |          -0.0112 |           9.6275 |           4.9773 |
[32m[20230114 18:51:23 @agent_ppo2.py:134][0m Policy update time: 0.79 s
[32m[20230114 18:51:24 @agent_ppo2.py:142][0m Average TRAINING episode reward: 261.96
[32m[20230114 18:51:24 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 263.26
[32m[20230114 18:51:24 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 268.73
[32m[20230114 18:51:24 @agent_ppo2.py:147][0m Total time:       9.32 min
[32m[20230114 18:51:24 @agent_ppo2.py:149][0m 788480 total steps have happened
[32m[20230114 18:51:24 @agent_ppo2.py:125][0m #------------------------ Iteration 385 --------------------------#
[32m[20230114 18:51:24 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:51:24 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:51:24 @agent_ppo2.py:189][0m |           0.0004 |          16.6139 |           4.8945 |
[32m[20230114 18:51:24 @agent_ppo2.py:189][0m |          -0.0034 |          15.0360 |           4.8885 |
[32m[20230114 18:51:24 @agent_ppo2.py:189][0m |          -0.0042 |          14.8916 |           4.8881 |
[32m[20230114 18:51:24 @agent_ppo2.py:189][0m |          -0.0062 |          14.3444 |           4.8854 |
[32m[20230114 18:51:24 @agent_ppo2.py:189][0m |          -0.0097 |          14.2560 |           4.8765 |
[32m[20230114 18:51:24 @agent_ppo2.py:189][0m |          -0.0074 |          14.0176 |           4.8826 |
[32m[20230114 18:51:24 @agent_ppo2.py:189][0m |          -0.0099 |          13.8128 |           4.8790 |
[32m[20230114 18:51:24 @agent_ppo2.py:189][0m |          -0.0113 |          13.5842 |           4.8731 |
[32m[20230114 18:51:24 @agent_ppo2.py:189][0m |          -0.0128 |          13.4400 |           4.8704 |
[32m[20230114 18:51:25 @agent_ppo2.py:189][0m |          -0.0125 |          13.3049 |           4.8699 |
[32m[20230114 18:51:25 @agent_ppo2.py:134][0m Policy update time: 0.78 s
[32m[20230114 18:51:25 @agent_ppo2.py:142][0m Average TRAINING episode reward: 259.79
[32m[20230114 18:51:25 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 260.80
[32m[20230114 18:51:25 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 272.78
[32m[20230114 18:51:25 @agent_ppo2.py:147][0m Total time:       9.34 min
[32m[20230114 18:51:25 @agent_ppo2.py:149][0m 790528 total steps have happened
[32m[20230114 18:51:25 @agent_ppo2.py:125][0m #------------------------ Iteration 386 --------------------------#
[32m[20230114 18:51:25 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:51:25 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:51:25 @agent_ppo2.py:189][0m |           0.0021 |          14.6159 |           4.9630 |
[32m[20230114 18:51:25 @agent_ppo2.py:189][0m |          -0.0066 |          13.6577 |           4.9597 |
[32m[20230114 18:51:25 @agent_ppo2.py:189][0m |          -0.0068 |          13.3235 |           4.9566 |
[32m[20230114 18:51:25 @agent_ppo2.py:189][0m |          -0.0095 |          13.0722 |           4.9540 |
[32m[20230114 18:51:25 @agent_ppo2.py:189][0m |          -0.0117 |          12.8693 |           4.9530 |
[32m[20230114 18:51:26 @agent_ppo2.py:189][0m |          -0.0102 |          12.7392 |           4.9550 |
[32m[20230114 18:51:26 @agent_ppo2.py:189][0m |          -0.0113 |          12.5718 |           4.9525 |
[32m[20230114 18:51:26 @agent_ppo2.py:189][0m |          -0.0128 |          12.4541 |           4.9593 |
[32m[20230114 18:51:26 @agent_ppo2.py:189][0m |          -0.0122 |          12.3725 |           4.9502 |
[32m[20230114 18:51:26 @agent_ppo2.py:189][0m |          -0.0130 |          12.2488 |           4.9510 |
[32m[20230114 18:51:26 @agent_ppo2.py:134][0m Policy update time: 0.80 s
[32m[20230114 18:51:26 @agent_ppo2.py:142][0m Average TRAINING episode reward: 259.78
[32m[20230114 18:51:26 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 262.52
[32m[20230114 18:51:26 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 271.69
[32m[20230114 18:51:26 @agent_ppo2.py:147][0m Total time:       9.36 min
[32m[20230114 18:51:26 @agent_ppo2.py:149][0m 792576 total steps have happened
[32m[20230114 18:51:26 @agent_ppo2.py:125][0m #------------------------ Iteration 387 --------------------------#
[32m[20230114 18:51:26 @agent_ppo2.py:131][0m Sampling time: 0.30 s by 4 slaves
[32m[20230114 18:51:26 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:51:26 @agent_ppo2.py:189][0m |          -0.0009 |          26.3984 |           4.9198 |
[32m[20230114 18:51:27 @agent_ppo2.py:189][0m |          -0.0065 |          20.0520 |           4.9204 |
[32m[20230114 18:51:27 @agent_ppo2.py:189][0m |          -0.0093 |          18.8816 |           4.9219 |
[32m[20230114 18:51:27 @agent_ppo2.py:189][0m |          -0.0080 |          18.2687 |           4.9239 |
[32m[20230114 18:51:27 @agent_ppo2.py:189][0m |          -0.0104 |          17.6261 |           4.9176 |
[32m[20230114 18:51:27 @agent_ppo2.py:189][0m |          -0.0113 |          17.1975 |           4.9194 |
[32m[20230114 18:51:27 @agent_ppo2.py:189][0m |          -0.0125 |          16.6944 |           4.9173 |
[32m[20230114 18:51:27 @agent_ppo2.py:189][0m |          -0.0121 |          16.3832 |           4.9173 |
[32m[20230114 18:51:27 @agent_ppo2.py:189][0m |          -0.0089 |          16.6587 |           4.9126 |
[32m[20230114 18:51:27 @agent_ppo2.py:189][0m |          -0.0136 |          15.7880 |           4.9135 |
[32m[20230114 18:51:27 @agent_ppo2.py:134][0m Policy update time: 0.83 s
[32m[20230114 18:51:27 @agent_ppo2.py:142][0m Average TRAINING episode reward: 200.47
[32m[20230114 18:51:27 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 265.90
[32m[20230114 18:51:27 @agent_ppo2.py:144][0m Average EVALUATION episode reward: -24.57
[32m[20230114 18:51:27 @agent_ppo2.py:147][0m Total time:       9.39 min
[32m[20230114 18:51:27 @agent_ppo2.py:149][0m 794624 total steps have happened
[32m[20230114 18:51:27 @agent_ppo2.py:125][0m #------------------------ Iteration 388 --------------------------#
[32m[20230114 18:51:28 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:51:28 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:51:28 @agent_ppo2.py:189][0m |           0.0001 |          15.3709 |           5.0148 |
[32m[20230114 18:51:28 @agent_ppo2.py:189][0m |          -0.0052 |          13.7352 |           4.9997 |
[32m[20230114 18:51:28 @agent_ppo2.py:189][0m |          -0.0078 |          13.2940 |           4.9922 |
[32m[20230114 18:51:28 @agent_ppo2.py:189][0m |          -0.0093 |          12.9396 |           4.9884 |
[32m[20230114 18:51:28 @agent_ppo2.py:189][0m |          -0.0115 |          12.6496 |           4.9871 |
[32m[20230114 18:51:28 @agent_ppo2.py:189][0m |          -0.0112 |          12.5245 |           4.9871 |
[32m[20230114 18:51:28 @agent_ppo2.py:189][0m |          -0.0127 |          12.2613 |           4.9856 |
[32m[20230114 18:51:28 @agent_ppo2.py:189][0m |          -0.0117 |          12.1889 |           4.9873 |
[32m[20230114 18:51:28 @agent_ppo2.py:189][0m |          -0.0130 |          12.0123 |           4.9859 |
[32m[20230114 18:51:28 @agent_ppo2.py:189][0m |          -0.0139 |          11.8128 |           4.9863 |
[32m[20230114 18:51:28 @agent_ppo2.py:134][0m Policy update time: 0.77 s
[32m[20230114 18:51:28 @agent_ppo2.py:142][0m Average TRAINING episode reward: 261.81
[32m[20230114 18:51:28 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 263.66
[32m[20230114 18:51:28 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 38.02
[32m[20230114 18:51:28 @agent_ppo2.py:147][0m Total time:       9.41 min
[32m[20230114 18:51:28 @agent_ppo2.py:149][0m 796672 total steps have happened
[32m[20230114 18:51:28 @agent_ppo2.py:125][0m #------------------------ Iteration 389 --------------------------#
[32m[20230114 18:51:29 @agent_ppo2.py:131][0m Sampling time: 0.31 s by 4 slaves
[32m[20230114 18:51:29 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:51:29 @agent_ppo2.py:189][0m |           0.0009 |          55.5987 |           4.9879 |
[32m[20230114 18:51:29 @agent_ppo2.py:189][0m |           0.0010 |          39.4427 |           4.9798 |
[32m[20230114 18:51:29 @agent_ppo2.py:189][0m |          -0.0076 |          31.1141 |           4.9800 |
[32m[20230114 18:51:29 @agent_ppo2.py:189][0m |          -0.0017 |          23.1547 |           4.9792 |
[32m[20230114 18:51:29 @agent_ppo2.py:189][0m |          -0.0022 |          21.0624 |           4.9729 |
[32m[20230114 18:51:29 @agent_ppo2.py:189][0m |           0.0027 |          19.2465 |           4.9713 |
[32m[20230114 18:51:30 @agent_ppo2.py:189][0m |          -0.0156 |          18.4023 |           4.9622 |
[32m[20230114 18:51:30 @agent_ppo2.py:189][0m |          -0.0117 |          17.5092 |           4.9580 |
[32m[20230114 18:51:30 @agent_ppo2.py:189][0m |          -0.0115 |          16.8887 |           4.9604 |
[32m[20230114 18:51:30 @agent_ppo2.py:189][0m |          -0.0144 |          16.3288 |           4.9595 |
[32m[20230114 18:51:30 @agent_ppo2.py:134][0m Policy update time: 1.03 s
[32m[20230114 18:51:30 @agent_ppo2.py:142][0m Average TRAINING episode reward: 141.65
[32m[20230114 18:51:30 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 262.50
[32m[20230114 18:51:30 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 272.21
[32m[20230114 18:51:30 @agent_ppo2.py:147][0m Total time:       9.43 min
[32m[20230114 18:51:30 @agent_ppo2.py:149][0m 798720 total steps have happened
[32m[20230114 18:51:30 @agent_ppo2.py:125][0m #------------------------ Iteration 390 --------------------------#
[32m[20230114 18:51:30 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:51:30 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:51:30 @agent_ppo2.py:189][0m |          -0.0019 |          15.8184 |           4.9325 |
[32m[20230114 18:51:31 @agent_ppo2.py:189][0m |          -0.0056 |          14.6373 |           4.9196 |
[32m[20230114 18:51:31 @agent_ppo2.py:189][0m |          -0.0056 |          14.2552 |           4.9158 |
[32m[20230114 18:51:31 @agent_ppo2.py:189][0m |          -0.0086 |          13.8896 |           4.9157 |
[32m[20230114 18:51:31 @agent_ppo2.py:189][0m |          -0.0092 |          13.6115 |           4.9091 |
[32m[20230114 18:51:31 @agent_ppo2.py:189][0m |          -0.0102 |          13.3972 |           4.9063 |
[32m[20230114 18:51:31 @agent_ppo2.py:189][0m |          -0.0078 |          13.4365 |           4.9088 |
[32m[20230114 18:51:31 @agent_ppo2.py:189][0m |          -0.0088 |          13.2269 |           4.9041 |
[32m[20230114 18:51:31 @agent_ppo2.py:189][0m |          -0.0127 |          12.7905 |           4.9030 |
[32m[20230114 18:51:31 @agent_ppo2.py:189][0m |          -0.0128 |          12.6311 |           4.9012 |
[32m[20230114 18:51:31 @agent_ppo2.py:134][0m Policy update time: 0.79 s
[32m[20230114 18:51:31 @agent_ppo2.py:142][0m Average TRAINING episode reward: 258.78
[32m[20230114 18:51:31 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 261.38
[32m[20230114 18:51:31 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 269.98
[32m[20230114 18:51:31 @agent_ppo2.py:147][0m Total time:       9.45 min
[32m[20230114 18:51:31 @agent_ppo2.py:149][0m 800768 total steps have happened
[32m[20230114 18:51:31 @agent_ppo2.py:125][0m #------------------------ Iteration 391 --------------------------#
[32m[20230114 18:51:32 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:51:32 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:51:32 @agent_ppo2.py:189][0m |           0.0003 |          15.9364 |           4.8780 |
[32m[20230114 18:51:32 @agent_ppo2.py:189][0m |          -0.0050 |          13.9605 |           4.8714 |
[32m[20230114 18:51:32 @agent_ppo2.py:189][0m |          -0.0042 |          13.5856 |           4.8604 |
[32m[20230114 18:51:32 @agent_ppo2.py:189][0m |          -0.0060 |          13.2472 |           4.8571 |
[32m[20230114 18:51:32 @agent_ppo2.py:189][0m |          -0.0091 |          12.8116 |           4.8489 |
[32m[20230114 18:51:32 @agent_ppo2.py:189][0m |          -0.0067 |          12.6770 |           4.8526 |
[32m[20230114 18:51:32 @agent_ppo2.py:189][0m |          -0.0094 |          12.4452 |           4.8546 |
[32m[20230114 18:51:32 @agent_ppo2.py:189][0m |          -0.0110 |          12.2911 |           4.8521 |
[32m[20230114 18:51:32 @agent_ppo2.py:189][0m |          -0.0101 |          12.1562 |           4.8493 |
[32m[20230114 18:51:32 @agent_ppo2.py:189][0m |          -0.0115 |          12.0052 |           4.8470 |
[32m[20230114 18:51:32 @agent_ppo2.py:134][0m Policy update time: 0.78 s
[32m[20230114 18:51:32 @agent_ppo2.py:142][0m Average TRAINING episode reward: 261.64
[32m[20230114 18:51:33 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 264.38
[32m[20230114 18:51:33 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 273.04
[32m[20230114 18:51:33 @agent_ppo2.py:147][0m Total time:       9.47 min
[32m[20230114 18:51:33 @agent_ppo2.py:149][0m 802816 total steps have happened
[32m[20230114 18:51:33 @agent_ppo2.py:125][0m #------------------------ Iteration 392 --------------------------#
[32m[20230114 18:51:33 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:51:33 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:51:33 @agent_ppo2.py:189][0m |           0.0001 |          14.0447 |           4.8734 |
[32m[20230114 18:51:33 @agent_ppo2.py:189][0m |          -0.0045 |          10.2336 |           4.8580 |
[32m[20230114 18:51:33 @agent_ppo2.py:189][0m |          -0.0059 |           8.2004 |           4.8580 |
[32m[20230114 18:51:33 @agent_ppo2.py:189][0m |          -0.0073 |           6.7614 |           4.8574 |
[32m[20230114 18:51:33 @agent_ppo2.py:189][0m |          -0.0075 |           5.5633 |           4.8568 |
[32m[20230114 18:51:33 @agent_ppo2.py:189][0m |          -0.0079 |           4.6759 |           4.8506 |
[32m[20230114 18:51:33 @agent_ppo2.py:189][0m |          -0.0086 |           3.9559 |           4.8549 |
[32m[20230114 18:51:33 @agent_ppo2.py:189][0m |          -0.0092 |           3.4459 |           4.8511 |
[32m[20230114 18:51:33 @agent_ppo2.py:189][0m |          -0.0093 |           3.0660 |           4.8539 |
[32m[20230114 18:51:33 @agent_ppo2.py:189][0m |          -0.0104 |           2.8197 |           4.8492 |
[32m[20230114 18:51:33 @agent_ppo2.py:134][0m Policy update time: 0.77 s
[32m[20230114 18:51:34 @agent_ppo2.py:142][0m Average TRAINING episode reward: 260.76
[32m[20230114 18:51:34 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 264.56
[32m[20230114 18:51:34 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 274.33
[32m[20230114 18:51:34 @agent_ppo2.py:147][0m Total time:       9.49 min
[32m[20230114 18:51:34 @agent_ppo2.py:149][0m 804864 total steps have happened
[32m[20230114 18:51:34 @agent_ppo2.py:125][0m #------------------------ Iteration 393 --------------------------#
[32m[20230114 18:51:34 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:51:34 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:51:34 @agent_ppo2.py:189][0m |           0.0025 |          16.2823 |           4.7033 |
[32m[20230114 18:51:34 @agent_ppo2.py:189][0m |          -0.0124 |          11.4938 |           4.6861 |
[32m[20230114 18:51:34 @agent_ppo2.py:189][0m |          -0.0133 |          10.7467 |           4.6771 |
[32m[20230114 18:51:34 @agent_ppo2.py:189][0m |          -0.0160 |          10.2136 |           4.6743 |
[32m[20230114 18:51:34 @agent_ppo2.py:189][0m |          -0.0140 |           9.7578 |           4.6772 |
[32m[20230114 18:51:34 @agent_ppo2.py:189][0m |          -0.0201 |           9.5022 |           4.6717 |
[32m[20230114 18:51:35 @agent_ppo2.py:189][0m |          -0.0101 |           9.1894 |           4.6744 |
[32m[20230114 18:51:35 @agent_ppo2.py:189][0m |          -0.0124 |           8.9994 |           4.6709 |
[32m[20230114 18:51:35 @agent_ppo2.py:189][0m |          -0.0175 |           8.8065 |           4.6695 |
[32m[20230114 18:51:35 @agent_ppo2.py:189][0m |          -0.0165 |           8.6115 |           4.6707 |
[32m[20230114 18:51:35 @agent_ppo2.py:134][0m Policy update time: 0.79 s
[32m[20230114 18:51:35 @agent_ppo2.py:142][0m Average TRAINING episode reward: 233.38
[32m[20230114 18:51:35 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 263.55
[32m[20230114 18:51:35 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 265.71
[32m[20230114 18:51:35 @agent_ppo2.py:147][0m Total time:       9.51 min
[32m[20230114 18:51:35 @agent_ppo2.py:149][0m 806912 total steps have happened
[32m[20230114 18:51:35 @agent_ppo2.py:125][0m #------------------------ Iteration 394 --------------------------#
[32m[20230114 18:51:35 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:51:35 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:51:35 @agent_ppo2.py:189][0m |           0.0009 |          20.9509 |           4.7657 |
[32m[20230114 18:51:35 @agent_ppo2.py:189][0m |          -0.0041 |          18.7564 |           4.7630 |
[32m[20230114 18:51:35 @agent_ppo2.py:189][0m |          -0.0057 |          17.8270 |           4.7551 |
[32m[20230114 18:51:36 @agent_ppo2.py:189][0m |          -0.0063 |          16.9941 |           4.7535 |
[32m[20230114 18:51:36 @agent_ppo2.py:189][0m |          -0.0084 |          16.1153 |           4.7520 |
[32m[20230114 18:51:36 @agent_ppo2.py:189][0m |          -0.0092 |          15.4645 |           4.7484 |
[32m[20230114 18:51:36 @agent_ppo2.py:189][0m |          -0.0085 |          15.0605 |           4.7537 |
[32m[20230114 18:51:36 @agent_ppo2.py:189][0m |          -0.0107 |          14.6450 |           4.7579 |
[32m[20230114 18:51:36 @agent_ppo2.py:189][0m |          -0.0125 |          14.3277 |           4.7522 |
[32m[20230114 18:51:36 @agent_ppo2.py:189][0m |          -0.0125 |          14.1553 |           4.7566 |
[32m[20230114 18:51:36 @agent_ppo2.py:134][0m Policy update time: 0.77 s
[32m[20230114 18:51:36 @agent_ppo2.py:142][0m Average TRAINING episode reward: 261.99
[32m[20230114 18:51:36 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 264.09
[32m[20230114 18:51:36 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 117.61
[32m[20230114 18:51:36 @agent_ppo2.py:147][0m Total time:       9.53 min
[32m[20230114 18:51:36 @agent_ppo2.py:149][0m 808960 total steps have happened
[32m[20230114 18:51:36 @agent_ppo2.py:125][0m #------------------------ Iteration 395 --------------------------#
[32m[20230114 18:51:36 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:51:36 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:51:37 @agent_ppo2.py:189][0m |           0.0015 |          15.9866 |           4.8605 |
[32m[20230114 18:51:37 @agent_ppo2.py:189][0m |          -0.0048 |          13.6624 |           4.8481 |
[32m[20230114 18:51:37 @agent_ppo2.py:189][0m |          -0.0070 |          12.9660 |           4.8514 |
[32m[20230114 18:51:37 @agent_ppo2.py:189][0m |          -0.0077 |          12.6873 |           4.8401 |
[32m[20230114 18:51:37 @agent_ppo2.py:189][0m |          -0.0098 |          12.3768 |           4.8451 |
[32m[20230114 18:51:37 @agent_ppo2.py:189][0m |          -0.0104 |          12.2068 |           4.8478 |
[32m[20230114 18:51:37 @agent_ppo2.py:189][0m |          -0.0112 |          12.0502 |           4.8409 |
[32m[20230114 18:51:37 @agent_ppo2.py:189][0m |          -0.0115 |          11.9537 |           4.8499 |
[32m[20230114 18:51:37 @agent_ppo2.py:189][0m |          -0.0128 |          11.7337 |           4.8462 |
[32m[20230114 18:51:37 @agent_ppo2.py:189][0m |          -0.0134 |          11.5554 |           4.8450 |
[32m[20230114 18:51:37 @agent_ppo2.py:134][0m Policy update time: 0.78 s
[32m[20230114 18:51:37 @agent_ppo2.py:142][0m Average TRAINING episode reward: 259.75
[32m[20230114 18:51:37 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 261.33
[32m[20230114 18:51:37 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 273.57
[32m[20230114 18:51:37 @agent_ppo2.py:147][0m Total time:       9.55 min
[32m[20230114 18:51:37 @agent_ppo2.py:149][0m 811008 total steps have happened
[32m[20230114 18:51:37 @agent_ppo2.py:125][0m #------------------------ Iteration 396 --------------------------#
[32m[20230114 18:51:38 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:51:38 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:51:38 @agent_ppo2.py:189][0m |           0.0049 |          14.6113 |           4.8295 |
[32m[20230114 18:51:38 @agent_ppo2.py:189][0m |          -0.0042 |          12.5625 |           4.8141 |
[32m[20230114 18:51:38 @agent_ppo2.py:189][0m |          -0.0044 |          12.0057 |           4.8214 |
[32m[20230114 18:51:38 @agent_ppo2.py:189][0m |          -0.0066 |          11.5848 |           4.8230 |
[32m[20230114 18:51:38 @agent_ppo2.py:189][0m |          -0.0075 |          11.3044 |           4.8163 |
[32m[20230114 18:51:38 @agent_ppo2.py:189][0m |          -0.0090 |          11.0489 |           4.8177 |
[32m[20230114 18:51:38 @agent_ppo2.py:189][0m |          -0.0092 |          10.9271 |           4.8203 |
[32m[20230114 18:51:38 @agent_ppo2.py:189][0m |          -0.0080 |          10.8108 |           4.8175 |
[32m[20230114 18:51:38 @agent_ppo2.py:189][0m |          -0.0097 |          10.8373 |           4.8193 |
[32m[20230114 18:51:38 @agent_ppo2.py:189][0m |          -0.0101 |          10.5982 |           4.8168 |
[32m[20230114 18:51:38 @agent_ppo2.py:134][0m Policy update time: 0.77 s
[32m[20230114 18:51:39 @agent_ppo2.py:142][0m Average TRAINING episode reward: 260.70
[32m[20230114 18:51:39 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 263.50
[32m[20230114 18:51:39 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 112.91
[32m[20230114 18:51:39 @agent_ppo2.py:147][0m Total time:       9.57 min
[32m[20230114 18:51:39 @agent_ppo2.py:149][0m 813056 total steps have happened
[32m[20230114 18:51:39 @agent_ppo2.py:125][0m #------------------------ Iteration 397 --------------------------#
[32m[20230114 18:51:39 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:51:39 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:51:39 @agent_ppo2.py:189][0m |           0.0012 |          14.0176 |           4.8275 |
[32m[20230114 18:51:39 @agent_ppo2.py:189][0m |          -0.0036 |          13.1443 |           4.8217 |
[32m[20230114 18:51:39 @agent_ppo2.py:189][0m |          -0.0042 |          12.8559 |           4.8272 |
[32m[20230114 18:51:39 @agent_ppo2.py:189][0m |          -0.0074 |          12.5350 |           4.8241 |
[32m[20230114 18:51:39 @agent_ppo2.py:189][0m |          -0.0079 |          12.3042 |           4.8264 |
[32m[20230114 18:51:39 @agent_ppo2.py:189][0m |          -0.0083 |          12.1314 |           4.8244 |
[32m[20230114 18:51:39 @agent_ppo2.py:189][0m |          -0.0101 |          11.9794 |           4.8298 |
[32m[20230114 18:51:39 @agent_ppo2.py:189][0m |          -0.0107 |          11.8069 |           4.8295 |
[32m[20230114 18:51:40 @agent_ppo2.py:189][0m |          -0.0103 |          11.7138 |           4.8324 |
[32m[20230114 18:51:40 @agent_ppo2.py:189][0m |          -0.0124 |          11.5144 |           4.8294 |
[32m[20230114 18:51:40 @agent_ppo2.py:134][0m Policy update time: 0.79 s
[32m[20230114 18:51:40 @agent_ppo2.py:142][0m Average TRAINING episode reward: 260.75
[32m[20230114 18:51:40 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 262.27
[32m[20230114 18:51:40 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 273.20
[32m[20230114 18:51:40 @agent_ppo2.py:147][0m Total time:       9.59 min
[32m[20230114 18:51:40 @agent_ppo2.py:149][0m 815104 total steps have happened
[32m[20230114 18:51:40 @agent_ppo2.py:125][0m #------------------------ Iteration 398 --------------------------#
[32m[20230114 18:51:40 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:51:40 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:51:40 @agent_ppo2.py:189][0m |           0.0018 |          24.7732 |           4.8736 |
[32m[20230114 18:51:40 @agent_ppo2.py:189][0m |          -0.0035 |          16.6062 |           4.8678 |
[32m[20230114 18:51:40 @agent_ppo2.py:189][0m |          -0.0060 |          14.9295 |           4.8636 |
[32m[20230114 18:51:40 @agent_ppo2.py:189][0m |          -0.0083 |          13.5053 |           4.8620 |
[32m[20230114 18:51:40 @agent_ppo2.py:189][0m |          -0.0106 |          12.3121 |           4.8631 |
[32m[20230114 18:51:40 @agent_ppo2.py:189][0m |          -0.0119 |          11.5073 |           4.8648 |
[32m[20230114 18:51:41 @agent_ppo2.py:189][0m |          -0.0114 |          10.5236 |           4.8692 |
[32m[20230114 18:51:41 @agent_ppo2.py:189][0m |          -0.0123 |          10.0645 |           4.8693 |
[32m[20230114 18:51:41 @agent_ppo2.py:189][0m |          -0.0147 |           9.4459 |           4.8646 |
[32m[20230114 18:51:41 @agent_ppo2.py:189][0m |          -0.0129 |           9.1432 |           4.8683 |
[32m[20230114 18:51:41 @agent_ppo2.py:134][0m Policy update time: 0.70 s
[32m[20230114 18:51:41 @agent_ppo2.py:142][0m Average TRAINING episode reward: 213.33
[32m[20230114 18:51:41 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 264.13
[32m[20230114 18:51:41 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 274.10
[32m[20230114 18:51:41 @agent_ppo2.py:147][0m Total time:       9.61 min
[32m[20230114 18:51:41 @agent_ppo2.py:149][0m 817152 total steps have happened
[32m[20230114 18:51:41 @agent_ppo2.py:125][0m #------------------------ Iteration 399 --------------------------#
[32m[20230114 18:51:41 @agent_ppo2.py:131][0m Sampling time: 0.26 s by 4 slaves
[32m[20230114 18:51:41 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:51:41 @agent_ppo2.py:189][0m |          -0.0002 |          50.1775 |           4.9667 |
[32m[20230114 18:51:41 @agent_ppo2.py:189][0m |          -0.0064 |          29.4270 |           4.9643 |
[32m[20230114 18:51:41 @agent_ppo2.py:189][0m |          -0.0099 |          24.8489 |           4.9575 |
[32m[20230114 18:51:42 @agent_ppo2.py:189][0m |          -0.0122 |          22.5829 |           4.9565 |
[32m[20230114 18:51:42 @agent_ppo2.py:189][0m |          -0.0137 |          20.8892 |           4.9557 |
[32m[20230114 18:51:42 @agent_ppo2.py:189][0m |          -0.0141 |          19.5190 |           4.9551 |
[32m[20230114 18:51:42 @agent_ppo2.py:189][0m |          -0.0151 |          18.5787 |           4.9536 |
[32m[20230114 18:51:42 @agent_ppo2.py:189][0m |          -0.0159 |          17.7214 |           4.9556 |
[32m[20230114 18:51:42 @agent_ppo2.py:189][0m |          -0.0151 |          16.8960 |           4.9526 |
[32m[20230114 18:51:42 @agent_ppo2.py:189][0m |          -0.0164 |          16.2735 |           4.9543 |
[32m[20230114 18:51:42 @agent_ppo2.py:134][0m Policy update time: 0.82 s
[32m[20230114 18:51:42 @agent_ppo2.py:142][0m Average TRAINING episode reward: 166.95
[32m[20230114 18:51:42 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 263.72
[32m[20230114 18:51:42 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 114.08
[32m[20230114 18:51:42 @agent_ppo2.py:147][0m Total time:       9.63 min
[32m[20230114 18:51:42 @agent_ppo2.py:149][0m 819200 total steps have happened
[32m[20230114 18:51:42 @agent_ppo2.py:125][0m #------------------------ Iteration 400 --------------------------#
[32m[20230114 18:51:42 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:51:42 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:51:43 @agent_ppo2.py:189][0m |          -0.0039 |          18.2133 |           4.9099 |
[32m[20230114 18:51:43 @agent_ppo2.py:189][0m |          -0.0077 |          16.0859 |           4.9004 |
[32m[20230114 18:51:43 @agent_ppo2.py:189][0m |          -0.0093 |          15.3318 |           4.8967 |
[32m[20230114 18:51:43 @agent_ppo2.py:189][0m |          -0.0130 |          14.8831 |           4.8915 |
[32m[20230114 18:51:43 @agent_ppo2.py:189][0m |          -0.0113 |          14.4684 |           4.8852 |
[32m[20230114 18:51:43 @agent_ppo2.py:189][0m |          -0.0131 |          14.2771 |           4.8846 |
[32m[20230114 18:51:43 @agent_ppo2.py:189][0m |          -0.0135 |          13.8811 |           4.8877 |
[32m[20230114 18:51:43 @agent_ppo2.py:189][0m |          -0.0141 |          13.6717 |           4.8830 |
[32m[20230114 18:51:43 @agent_ppo2.py:189][0m |          -0.0115 |          13.4938 |           4.8782 |
[32m[20230114 18:51:43 @agent_ppo2.py:189][0m |          -0.0158 |          13.3250 |           4.8753 |
[32m[20230114 18:51:43 @agent_ppo2.py:134][0m Policy update time: 0.77 s
[32m[20230114 18:51:43 @agent_ppo2.py:142][0m Average TRAINING episode reward: 261.44
[32m[20230114 18:51:43 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 262.74
[32m[20230114 18:51:43 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 273.92
[32m[20230114 18:51:43 @agent_ppo2.py:147][0m Total time:       9.65 min
[32m[20230114 18:51:43 @agent_ppo2.py:149][0m 821248 total steps have happened
[32m[20230114 18:51:43 @agent_ppo2.py:125][0m #------------------------ Iteration 401 --------------------------#
[32m[20230114 18:51:44 @agent_ppo2.py:131][0m Sampling time: 0.25 s by 4 slaves
[32m[20230114 18:51:44 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:51:44 @agent_ppo2.py:189][0m |           0.0024 |          15.2457 |           4.8455 |
[32m[20230114 18:51:44 @agent_ppo2.py:189][0m |          -0.0043 |          12.2209 |           4.8466 |
[32m[20230114 18:51:44 @agent_ppo2.py:189][0m |          -0.0065 |          11.5285 |           4.8401 |
[32m[20230114 18:51:44 @agent_ppo2.py:189][0m |          -0.0079 |          11.0694 |           4.8425 |
[32m[20230114 18:51:44 @agent_ppo2.py:189][0m |          -0.0083 |          10.7126 |           4.8409 |
[32m[20230114 18:51:44 @agent_ppo2.py:189][0m |          -0.0102 |          10.4054 |           4.8418 |
[32m[20230114 18:51:44 @agent_ppo2.py:189][0m |          -0.0101 |          10.1663 |           4.8380 |
[32m[20230114 18:51:44 @agent_ppo2.py:189][0m |          -0.0116 |           9.9507 |           4.8399 |
[32m[20230114 18:51:44 @agent_ppo2.py:189][0m |          -0.0118 |           9.7470 |           4.8424 |
[32m[20230114 18:51:44 @agent_ppo2.py:189][0m |          -0.0120 |           9.5748 |           4.8406 |
[32m[20230114 18:51:44 @agent_ppo2.py:134][0m Policy update time: 0.80 s
[32m[20230114 18:51:45 @agent_ppo2.py:142][0m Average TRAINING episode reward: 258.68
[32m[20230114 18:51:45 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 261.44
[32m[20230114 18:51:45 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 256.55
[32m[20230114 18:51:45 @agent_ppo2.py:147][0m Total time:       9.67 min
[32m[20230114 18:51:45 @agent_ppo2.py:149][0m 823296 total steps have happened
[32m[20230114 18:51:45 @agent_ppo2.py:125][0m #------------------------ Iteration 402 --------------------------#
[32m[20230114 18:51:45 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:51:45 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:51:45 @agent_ppo2.py:189][0m |          -0.0002 |          15.6075 |           4.8286 |
[32m[20230114 18:51:45 @agent_ppo2.py:189][0m |          -0.0046 |          13.5059 |           4.8247 |
[32m[20230114 18:51:45 @agent_ppo2.py:189][0m |          -0.0065 |          12.2728 |           4.8342 |
[32m[20230114 18:51:45 @agent_ppo2.py:189][0m |          -0.0084 |          11.5567 |           4.8298 |
[32m[20230114 18:51:45 @agent_ppo2.py:189][0m |          -0.0097 |          11.0821 |           4.8320 |
[32m[20230114 18:51:45 @agent_ppo2.py:189][0m |          -0.0104 |          10.6718 |           4.8372 |
[32m[20230114 18:51:45 @agent_ppo2.py:189][0m |          -0.0114 |          10.2223 |           4.8343 |
[32m[20230114 18:51:45 @agent_ppo2.py:189][0m |          -0.0117 |           9.7804 |           4.8394 |
[32m[20230114 18:51:46 @agent_ppo2.py:189][0m |          -0.0119 |           9.3285 |           4.8385 |
[32m[20230114 18:51:46 @agent_ppo2.py:189][0m |          -0.0128 |           8.9577 |           4.8385 |
[32m[20230114 18:51:46 @agent_ppo2.py:134][0m Policy update time: 0.78 s
[32m[20230114 18:51:46 @agent_ppo2.py:142][0m Average TRAINING episode reward: 260.90
[32m[20230114 18:51:46 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 262.02
[32m[20230114 18:51:46 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 266.56
[32m[20230114 18:51:46 @agent_ppo2.py:147][0m Total time:       9.70 min
[32m[20230114 18:51:46 @agent_ppo2.py:149][0m 825344 total steps have happened
[32m[20230114 18:51:46 @agent_ppo2.py:125][0m #------------------------ Iteration 403 --------------------------#
[32m[20230114 18:51:46 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:51:46 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:51:46 @agent_ppo2.py:189][0m |           0.0005 |          18.6396 |           4.9186 |
[32m[20230114 18:51:46 @agent_ppo2.py:189][0m |          -0.0045 |          15.9905 |           4.9120 |
[32m[20230114 18:51:46 @agent_ppo2.py:189][0m |          -0.0068 |          14.8812 |           4.9093 |
[32m[20230114 18:51:46 @agent_ppo2.py:189][0m |          -0.0074 |          14.2718 |           4.9059 |
[32m[20230114 18:51:47 @agent_ppo2.py:189][0m |          -0.0089 |          13.7745 |           4.9032 |
[32m[20230114 18:51:47 @agent_ppo2.py:189][0m |          -0.0084 |          13.3910 |           4.8955 |
[32m[20230114 18:51:47 @agent_ppo2.py:189][0m |          -0.0113 |          13.0575 |           4.9036 |
[32m[20230114 18:51:47 @agent_ppo2.py:189][0m |          -0.0106 |          12.8492 |           4.8963 |
[32m[20230114 18:51:47 @agent_ppo2.py:189][0m |          -0.0122 |          12.5030 |           4.8972 |
[32m[20230114 18:51:47 @agent_ppo2.py:189][0m |          -0.0107 |          12.4300 |           4.8915 |
[32m[20230114 18:51:47 @agent_ppo2.py:134][0m Policy update time: 0.79 s
[32m[20230114 18:51:47 @agent_ppo2.py:142][0m Average TRAINING episode reward: 260.04
[32m[20230114 18:51:47 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 261.44
[32m[20230114 18:51:47 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 273.27
[32m[20230114 18:51:47 @agent_ppo2.py:147][0m Total time:       9.72 min
[32m[20230114 18:51:47 @agent_ppo2.py:149][0m 827392 total steps have happened
[32m[20230114 18:51:47 @agent_ppo2.py:125][0m #------------------------ Iteration 404 --------------------------#
[32m[20230114 18:51:47 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:51:47 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:51:47 @agent_ppo2.py:189][0m |           0.0006 |          15.7500 |           4.8928 |
[32m[20230114 18:51:48 @agent_ppo2.py:189][0m |          -0.0054 |          14.2883 |           4.8890 |
[32m[20230114 18:51:48 @agent_ppo2.py:189][0m |          -0.0077 |          13.8195 |           4.8856 |
[32m[20230114 18:51:48 @agent_ppo2.py:189][0m |          -0.0070 |          13.6370 |           4.8840 |
[32m[20230114 18:51:48 @agent_ppo2.py:189][0m |          -0.0074 |          13.3173 |           4.8852 |
[32m[20230114 18:51:48 @agent_ppo2.py:189][0m |          -0.0092 |          13.1706 |           4.8774 |
[32m[20230114 18:51:48 @agent_ppo2.py:189][0m |          -0.0093 |          13.0295 |           4.8799 |
[32m[20230114 18:51:48 @agent_ppo2.py:189][0m |          -0.0102 |          12.8137 |           4.8830 |
[32m[20230114 18:51:48 @agent_ppo2.py:189][0m |          -0.0096 |          12.7937 |           4.8789 |
[32m[20230114 18:51:48 @agent_ppo2.py:189][0m |          -0.0121 |          12.5549 |           4.8805 |
[32m[20230114 18:51:48 @agent_ppo2.py:134][0m Policy update time: 0.77 s
[32m[20230114 18:51:48 @agent_ppo2.py:142][0m Average TRAINING episode reward: 260.97
[32m[20230114 18:51:48 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 264.97
[32m[20230114 18:51:48 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 273.09
[32m[20230114 18:51:48 @agent_ppo2.py:147][0m Total time:       9.74 min
[32m[20230114 18:51:48 @agent_ppo2.py:149][0m 829440 total steps have happened
[32m[20230114 18:51:48 @agent_ppo2.py:125][0m #------------------------ Iteration 405 --------------------------#
[32m[20230114 18:51:48 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:51:49 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:51:49 @agent_ppo2.py:189][0m |           0.0001 |          13.0965 |           4.7898 |
[32m[20230114 18:51:49 @agent_ppo2.py:189][0m |          -0.0029 |          12.4386 |           4.7852 |
[32m[20230114 18:51:49 @agent_ppo2.py:189][0m |          -0.0077 |          11.7908 |           4.7763 |
[32m[20230114 18:51:49 @agent_ppo2.py:189][0m |          -0.0122 |          11.5577 |           4.7770 |
[32m[20230114 18:51:49 @agent_ppo2.py:189][0m |          -0.0107 |          11.3824 |           4.7820 |
[32m[20230114 18:51:49 @agent_ppo2.py:189][0m |          -0.0130 |          11.2191 |           4.7745 |
[32m[20230114 18:51:49 @agent_ppo2.py:189][0m |          -0.0088 |          11.2802 |           4.7741 |
[32m[20230114 18:51:49 @agent_ppo2.py:189][0m |          -0.0104 |          11.5977 |           4.7746 |
[32m[20230114 18:51:49 @agent_ppo2.py:189][0m |          -0.0132 |          10.7915 |           4.7762 |
[32m[20230114 18:51:49 @agent_ppo2.py:189][0m |          -0.0127 |          10.7243 |           4.7704 |
[32m[20230114 18:51:49 @agent_ppo2.py:134][0m Policy update time: 0.78 s
[32m[20230114 18:51:49 @agent_ppo2.py:142][0m Average TRAINING episode reward: 260.50
[32m[20230114 18:51:49 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 261.29
[32m[20230114 18:51:49 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 274.96
[32m[20230114 18:51:49 @agent_ppo2.py:147][0m Total time:       9.76 min
[32m[20230114 18:51:49 @agent_ppo2.py:149][0m 831488 total steps have happened
[32m[20230114 18:51:49 @agent_ppo2.py:125][0m #------------------------ Iteration 406 --------------------------#
[32m[20230114 18:51:50 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:51:50 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:51:50 @agent_ppo2.py:189][0m |          -0.0016 |          13.5915 |           4.9610 |
[32m[20230114 18:51:50 @agent_ppo2.py:189][0m |           0.0002 |          11.1242 |           4.9495 |
[32m[20230114 18:51:50 @agent_ppo2.py:189][0m |          -0.0037 |          10.4849 |           4.9466 |
[32m[20230114 18:51:50 @agent_ppo2.py:189][0m |          -0.0089 |           9.7454 |           4.9434 |
[32m[20230114 18:51:50 @agent_ppo2.py:189][0m |          -0.0120 |           9.1939 |           4.9394 |
[32m[20230114 18:51:50 @agent_ppo2.py:189][0m |          -0.0111 |           8.8076 |           4.9449 |
[32m[20230114 18:51:50 @agent_ppo2.py:189][0m |          -0.0048 |           9.0242 |           4.9419 |
[32m[20230114 18:51:50 @agent_ppo2.py:189][0m |          -0.0120 |           8.2921 |           4.9373 |
[32m[20230114 18:51:50 @agent_ppo2.py:189][0m |          -0.0114 |           8.1344 |           4.9425 |
[32m[20230114 18:51:51 @agent_ppo2.py:189][0m |          -0.0126 |           7.7801 |           4.9338 |
[32m[20230114 18:51:51 @agent_ppo2.py:134][0m Policy update time: 0.80 s
[32m[20230114 18:51:51 @agent_ppo2.py:142][0m Average TRAINING episode reward: 258.80
[32m[20230114 18:51:51 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 263.62
[32m[20230114 18:51:51 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 272.11
[32m[20230114 18:51:51 @agent_ppo2.py:147][0m Total time:       9.78 min
[32m[20230114 18:51:51 @agent_ppo2.py:149][0m 833536 total steps have happened
[32m[20230114 18:51:51 @agent_ppo2.py:125][0m #------------------------ Iteration 407 --------------------------#
[32m[20230114 18:51:51 @agent_ppo2.py:131][0m Sampling time: 0.30 s by 4 slaves
[32m[20230114 18:51:51 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:51:51 @agent_ppo2.py:189][0m |          -0.0012 |          21.8471 |           4.9589 |
[32m[20230114 18:51:51 @agent_ppo2.py:189][0m |          -0.0044 |          16.2998 |           4.9466 |
[32m[20230114 18:51:51 @agent_ppo2.py:189][0m |          -0.0061 |          15.3293 |           4.9484 |
[32m[20230114 18:51:51 @agent_ppo2.py:189][0m |          -0.0071 |          14.7544 |           4.9525 |
[32m[20230114 18:51:51 @agent_ppo2.py:189][0m |          -0.0087 |          14.2642 |           4.9530 |
[32m[20230114 18:51:52 @agent_ppo2.py:189][0m |          -0.0102 |          13.9921 |           4.9512 |
[32m[20230114 18:51:52 @agent_ppo2.py:189][0m |          -0.0110 |          13.7288 |           4.9531 |
[32m[20230114 18:51:52 @agent_ppo2.py:189][0m |          -0.0113 |          13.5455 |           4.9540 |
[32m[20230114 18:51:52 @agent_ppo2.py:189][0m |          -0.0122 |          13.3117 |           4.9539 |
[32m[20230114 18:51:52 @agent_ppo2.py:189][0m |          -0.0115 |          13.1316 |           4.9510 |
[32m[20230114 18:51:52 @agent_ppo2.py:134][0m Policy update time: 0.84 s
[32m[20230114 18:51:52 @agent_ppo2.py:142][0m Average TRAINING episode reward: 197.71
[32m[20230114 18:51:52 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 262.26
[32m[20230114 18:51:52 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 273.44
[32m[20230114 18:51:52 @agent_ppo2.py:147][0m Total time:       9.80 min
[32m[20230114 18:51:52 @agent_ppo2.py:149][0m 835584 total steps have happened
[32m[20230114 18:51:52 @agent_ppo2.py:125][0m #------------------------ Iteration 408 --------------------------#
[32m[20230114 18:51:52 @agent_ppo2.py:131][0m Sampling time: 0.21 s by 4 slaves
[32m[20230114 18:51:52 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:51:52 @agent_ppo2.py:189][0m |           0.0004 |          14.0437 |           4.9116 |
[32m[20230114 18:51:52 @agent_ppo2.py:189][0m |          -0.0033 |          13.0459 |           4.8983 |
[32m[20230114 18:51:53 @agent_ppo2.py:189][0m |          -0.0052 |          12.5072 |           4.8889 |
[32m[20230114 18:51:53 @agent_ppo2.py:189][0m |          -0.0050 |          12.0533 |           4.8937 |
[32m[20230114 18:51:53 @agent_ppo2.py:189][0m |          -0.0060 |          11.7739 |           4.8891 |
[32m[20230114 18:51:53 @agent_ppo2.py:189][0m |          -0.0079 |          11.4943 |           4.8893 |
[32m[20230114 18:51:53 @agent_ppo2.py:189][0m |          -0.0091 |          11.2049 |           4.8855 |
[32m[20230114 18:51:53 @agent_ppo2.py:189][0m |          -0.0094 |          10.9973 |           4.8833 |
[32m[20230114 18:51:53 @agent_ppo2.py:189][0m |          -0.0093 |          10.8717 |           4.8802 |
[32m[20230114 18:51:53 @agent_ppo2.py:189][0m |          -0.0106 |          10.7113 |           4.8832 |
[32m[20230114 18:51:53 @agent_ppo2.py:134][0m Policy update time: 0.76 s
[32m[20230114 18:51:53 @agent_ppo2.py:142][0m Average TRAINING episode reward: 263.60
[32m[20230114 18:51:53 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 267.58
[32m[20230114 18:51:53 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 273.06
[32m[20230114 18:51:53 @agent_ppo2.py:147][0m Total time:       9.82 min
[32m[20230114 18:51:53 @agent_ppo2.py:149][0m 837632 total steps have happened
[32m[20230114 18:51:53 @agent_ppo2.py:125][0m #------------------------ Iteration 409 --------------------------#
[32m[20230114 18:51:53 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:51:54 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:51:54 @agent_ppo2.py:189][0m |           0.0003 |          31.6761 |           4.8611 |
[32m[20230114 18:51:54 @agent_ppo2.py:189][0m |          -0.0050 |          23.4749 |           4.8541 |
[32m[20230114 18:51:54 @agent_ppo2.py:189][0m |          -0.0080 |          20.8958 |           4.8497 |
[32m[20230114 18:51:54 @agent_ppo2.py:189][0m |          -0.0106 |          19.4183 |           4.8398 |
[32m[20230114 18:51:54 @agent_ppo2.py:189][0m |          -0.0117 |          18.3016 |           4.8391 |
[32m[20230114 18:51:54 @agent_ppo2.py:189][0m |          -0.0124 |          17.3376 |           4.8391 |
[32m[20230114 18:51:54 @agent_ppo2.py:189][0m |          -0.0125 |          16.9452 |           4.8397 |
[32m[20230114 18:51:54 @agent_ppo2.py:189][0m |          -0.0138 |          16.1402 |           4.8359 |
[32m[20230114 18:51:54 @agent_ppo2.py:189][0m |          -0.0144 |          15.4404 |           4.8361 |
[32m[20230114 18:51:54 @agent_ppo2.py:189][0m |          -0.0142 |          15.2328 |           4.8351 |
[32m[20230114 18:51:54 @agent_ppo2.py:134][0m Policy update time: 0.71 s
[32m[20230114 18:51:54 @agent_ppo2.py:142][0m Average TRAINING episode reward: 211.10
[32m[20230114 18:51:54 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 265.52
[32m[20230114 18:51:54 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 273.02
[32m[20230114 18:51:54 @agent_ppo2.py:147][0m Total time:       9.84 min
[32m[20230114 18:51:54 @agent_ppo2.py:149][0m 839680 total steps have happened
[32m[20230114 18:51:54 @agent_ppo2.py:125][0m #------------------------ Iteration 410 --------------------------#
[32m[20230114 18:51:55 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:51:55 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:51:55 @agent_ppo2.py:189][0m |          -0.0024 |          18.6918 |           4.9480 |
[32m[20230114 18:51:55 @agent_ppo2.py:189][0m |          -0.0083 |          15.8744 |           4.9450 |
[32m[20230114 18:51:55 @agent_ppo2.py:189][0m |          -0.0109 |          15.5431 |           4.9430 |
[32m[20230114 18:51:55 @agent_ppo2.py:189][0m |          -0.0110 |          15.2988 |           4.9380 |
[32m[20230114 18:51:55 @agent_ppo2.py:189][0m |          -0.0132 |          15.0444 |           4.9366 |
[32m[20230114 18:51:55 @agent_ppo2.py:189][0m |          -0.0133 |          14.8745 |           4.9378 |
[32m[20230114 18:51:55 @agent_ppo2.py:189][0m |          -0.0146 |          14.7105 |           4.9357 |
[32m[20230114 18:51:55 @agent_ppo2.py:189][0m |          -0.0138 |          14.6425 |           4.9325 |
[32m[20230114 18:51:55 @agent_ppo2.py:189][0m |          -0.0151 |          14.3979 |           4.9360 |
[32m[20230114 18:51:55 @agent_ppo2.py:189][0m |          -0.0153 |          14.2729 |           4.9353 |
[32m[20230114 18:51:55 @agent_ppo2.py:134][0m Policy update time: 0.78 s
[32m[20230114 18:51:56 @agent_ppo2.py:142][0m Average TRAINING episode reward: 260.31
[32m[20230114 18:51:56 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 260.61
[32m[20230114 18:51:56 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 271.16
[32m[20230114 18:51:56 @agent_ppo2.py:147][0m Total time:       9.86 min
[32m[20230114 18:51:56 @agent_ppo2.py:149][0m 841728 total steps have happened
[32m[20230114 18:51:56 @agent_ppo2.py:125][0m #------------------------ Iteration 411 --------------------------#
[32m[20230114 18:51:56 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:51:56 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:51:56 @agent_ppo2.py:189][0m |           0.0097 |          13.3126 |           4.7539 |
[32m[20230114 18:51:56 @agent_ppo2.py:189][0m |          -0.0099 |          11.3694 |           4.7406 |
[32m[20230114 18:51:56 @agent_ppo2.py:189][0m |          -0.0123 |          10.8651 |           4.7318 |
[32m[20230114 18:51:56 @agent_ppo2.py:189][0m |          -0.0120 |          10.3407 |           4.7294 |
[32m[20230114 18:51:56 @agent_ppo2.py:189][0m |          -0.0124 |          10.0217 |           4.7308 |
[32m[20230114 18:51:56 @agent_ppo2.py:189][0m |          -0.0124 |           9.5655 |           4.7300 |
[32m[20230114 18:51:56 @agent_ppo2.py:189][0m |          -0.0138 |           9.2839 |           4.7244 |
[32m[20230114 18:51:56 @agent_ppo2.py:189][0m |          -0.0106 |           8.9795 |           4.7214 |
[32m[20230114 18:51:57 @agent_ppo2.py:189][0m |          -0.0133 |           8.7032 |           4.7224 |
[32m[20230114 18:51:57 @agent_ppo2.py:189][0m |          -0.0145 |           8.4819 |           4.7235 |
[32m[20230114 18:51:57 @agent_ppo2.py:134][0m Policy update time: 0.77 s
[32m[20230114 18:51:57 @agent_ppo2.py:142][0m Average TRAINING episode reward: 262.44
[32m[20230114 18:51:57 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 263.63
[32m[20230114 18:51:57 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 65.79
[32m[20230114 18:51:57 @agent_ppo2.py:147][0m Total time:       9.88 min
[32m[20230114 18:51:57 @agent_ppo2.py:149][0m 843776 total steps have happened
[32m[20230114 18:51:57 @agent_ppo2.py:125][0m #------------------------ Iteration 412 --------------------------#
[32m[20230114 18:51:57 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:51:57 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:51:57 @agent_ppo2.py:189][0m |           0.0013 |          18.3754 |           4.7987 |
[32m[20230114 18:51:57 @agent_ppo2.py:189][0m |          -0.0047 |          12.3640 |           4.7897 |
[32m[20230114 18:51:57 @agent_ppo2.py:189][0m |          -0.0058 |          11.2506 |           4.7886 |
[32m[20230114 18:51:57 @agent_ppo2.py:189][0m |          -0.0075 |          10.4123 |           4.7861 |
[32m[20230114 18:51:57 @agent_ppo2.py:189][0m |          -0.0082 |          10.0024 |           4.7822 |
[32m[20230114 18:51:57 @agent_ppo2.py:189][0m |          -0.0113 |           9.5452 |           4.7826 |
[32m[20230114 18:51:58 @agent_ppo2.py:189][0m |          -0.0117 |           9.1376 |           4.7787 |
[32m[20230114 18:51:58 @agent_ppo2.py:189][0m |          -0.0109 |           8.9866 |           4.7780 |
[32m[20230114 18:51:58 @agent_ppo2.py:189][0m |          -0.0128 |           8.6678 |           4.7739 |
[32m[20230114 18:51:58 @agent_ppo2.py:189][0m |          -0.0127 |           8.4165 |           4.7727 |
[32m[20230114 18:51:58 @agent_ppo2.py:134][0m Policy update time: 0.75 s
[32m[20230114 18:51:58 @agent_ppo2.py:142][0m Average TRAINING episode reward: 225.74
[32m[20230114 18:51:58 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 264.52
[32m[20230114 18:51:58 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 271.47
[32m[20230114 18:51:58 @agent_ppo2.py:147][0m Total time:       9.90 min
[32m[20230114 18:51:58 @agent_ppo2.py:149][0m 845824 total steps have happened
[32m[20230114 18:51:58 @agent_ppo2.py:125][0m #------------------------ Iteration 413 --------------------------#
[32m[20230114 18:51:58 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:51:58 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:51:58 @agent_ppo2.py:189][0m |           0.0003 |          16.7684 |           4.8619 |
[32m[20230114 18:51:58 @agent_ppo2.py:189][0m |          -0.0101 |          14.9156 |           4.8585 |
[32m[20230114 18:51:58 @agent_ppo2.py:189][0m |          -0.0151 |          14.4102 |           4.8558 |
[32m[20230114 18:51:59 @agent_ppo2.py:189][0m |          -0.0096 |          13.9364 |           4.8492 |
[32m[20230114 18:51:59 @agent_ppo2.py:189][0m |          -0.0067 |          13.5867 |           4.8430 |
[32m[20230114 18:51:59 @agent_ppo2.py:189][0m |          -0.0109 |          13.3252 |           4.8431 |
[32m[20230114 18:51:59 @agent_ppo2.py:189][0m |          -0.0172 |          13.0989 |           4.8418 |
[32m[20230114 18:51:59 @agent_ppo2.py:189][0m |          -0.0112 |          12.9137 |           4.8387 |
[32m[20230114 18:51:59 @agent_ppo2.py:189][0m |          -0.0110 |          12.8054 |           4.8311 |
[32m[20230114 18:51:59 @agent_ppo2.py:189][0m |          -0.0150 |          12.6128 |           4.8299 |
[32m[20230114 18:51:59 @agent_ppo2.py:134][0m Policy update time: 0.78 s
[32m[20230114 18:51:59 @agent_ppo2.py:142][0m Average TRAINING episode reward: 262.33
[32m[20230114 18:51:59 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 266.66
[32m[20230114 18:51:59 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 272.54
[32m[20230114 18:51:59 @agent_ppo2.py:147][0m Total time:       9.92 min
[32m[20230114 18:51:59 @agent_ppo2.py:149][0m 847872 total steps have happened
[32m[20230114 18:51:59 @agent_ppo2.py:125][0m #------------------------ Iteration 414 --------------------------#
[32m[20230114 18:51:59 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:51:59 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:52:00 @agent_ppo2.py:189][0m |           0.0016 |          15.4804 |           4.6565 |
[32m[20230114 18:52:00 @agent_ppo2.py:189][0m |          -0.0077 |          13.3798 |           4.6497 |
[32m[20230114 18:52:00 @agent_ppo2.py:189][0m |          -0.0067 |          12.5370 |           4.6540 |
[32m[20230114 18:52:00 @agent_ppo2.py:189][0m |          -0.0084 |          11.9504 |           4.6440 |
[32m[20230114 18:52:00 @agent_ppo2.py:189][0m |          -0.0059 |          11.5667 |           4.6517 |
[32m[20230114 18:52:00 @agent_ppo2.py:189][0m |          -0.0137 |          10.9993 |           4.6513 |
[32m[20230114 18:52:00 @agent_ppo2.py:189][0m |          -0.0123 |          10.6826 |           4.6562 |
[32m[20230114 18:52:00 @agent_ppo2.py:189][0m |          -0.0154 |          10.2302 |           4.6500 |
[32m[20230114 18:52:00 @agent_ppo2.py:189][0m |          -0.0157 |           9.9358 |           4.6554 |
[32m[20230114 18:52:00 @agent_ppo2.py:189][0m |          -0.0118 |           9.6960 |           4.6546 |
[32m[20230114 18:52:00 @agent_ppo2.py:134][0m Policy update time: 0.77 s
[32m[20230114 18:52:00 @agent_ppo2.py:142][0m Average TRAINING episode reward: 262.79
[32m[20230114 18:52:00 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 264.84
[32m[20230114 18:52:00 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 276.42
[32m[20230114 18:52:00 @agent_ppo2.py:147][0m Total time:       9.94 min
[32m[20230114 18:52:00 @agent_ppo2.py:149][0m 849920 total steps have happened
[32m[20230114 18:52:00 @agent_ppo2.py:125][0m #------------------------ Iteration 415 --------------------------#
[32m[20230114 18:52:01 @agent_ppo2.py:131][0m Sampling time: 0.32 s by 4 slaves
[32m[20230114 18:52:01 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:52:01 @agent_ppo2.py:189][0m |           0.0013 |          36.6153 |           4.8134 |
[32m[20230114 18:52:01 @agent_ppo2.py:189][0m |          -0.0040 |          31.8385 |           4.8132 |
[32m[20230114 18:52:01 @agent_ppo2.py:189][0m |          -0.0060 |          29.7185 |           4.8096 |
[32m[20230114 18:52:01 @agent_ppo2.py:189][0m |          -0.0081 |          27.4990 |           4.8088 |
[32m[20230114 18:52:01 @agent_ppo2.py:189][0m |          -0.0082 |          26.6496 |           4.8114 |
[32m[20230114 18:52:01 @agent_ppo2.py:189][0m |          -0.0088 |          25.6956 |           4.8069 |
[32m[20230114 18:52:01 @agent_ppo2.py:189][0m |          -0.0102 |          24.8807 |           4.8070 |
[32m[20230114 18:52:01 @agent_ppo2.py:189][0m |          -0.0117 |          24.2589 |           4.8074 |
[32m[20230114 18:52:01 @agent_ppo2.py:189][0m |          -0.0116 |          23.8593 |           4.8039 |
[32m[20230114 18:52:02 @agent_ppo2.py:189][0m |          -0.0130 |          23.1966 |           4.8037 |
[32m[20230114 18:52:02 @agent_ppo2.py:134][0m Policy update time: 0.88 s
[32m[20230114 18:52:02 @agent_ppo2.py:142][0m Average TRAINING episode reward: 203.47
[32m[20230114 18:52:02 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 262.09
[32m[20230114 18:52:02 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 273.28
[32m[20230114 18:52:02 @agent_ppo2.py:147][0m Total time:       9.96 min
[32m[20230114 18:52:02 @agent_ppo2.py:149][0m 851968 total steps have happened
[32m[20230114 18:52:02 @agent_ppo2.py:125][0m #------------------------ Iteration 416 --------------------------#
[32m[20230114 18:52:02 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:52:02 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:52:02 @agent_ppo2.py:189][0m |          -0.0002 |          15.0650 |           4.8737 |
[32m[20230114 18:52:02 @agent_ppo2.py:189][0m |          -0.0056 |          14.0945 |           4.8653 |
[32m[20230114 18:52:02 @agent_ppo2.py:189][0m |          -0.0071 |          13.6551 |           4.8629 |
[32m[20230114 18:52:02 @agent_ppo2.py:189][0m |          -0.0085 |          13.4336 |           4.8656 |
[32m[20230114 18:52:02 @agent_ppo2.py:189][0m |          -0.0093 |          13.2493 |           4.8658 |
[32m[20230114 18:52:02 @agent_ppo2.py:189][0m |          -0.0102 |          13.0443 |           4.8652 |
[32m[20230114 18:52:03 @agent_ppo2.py:189][0m |          -0.0111 |          12.9156 |           4.8644 |
[32m[20230114 18:52:03 @agent_ppo2.py:189][0m |          -0.0120 |          12.8030 |           4.8659 |
[32m[20230114 18:52:03 @agent_ppo2.py:189][0m |          -0.0125 |          12.6831 |           4.8628 |
[32m[20230114 18:52:03 @agent_ppo2.py:189][0m |          -0.0128 |          12.5285 |           4.8686 |
[32m[20230114 18:52:03 @agent_ppo2.py:134][0m Policy update time: 0.76 s
[32m[20230114 18:52:03 @agent_ppo2.py:142][0m Average TRAINING episode reward: 262.82
[32m[20230114 18:52:03 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 265.61
[32m[20230114 18:52:03 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 274.06
[32m[20230114 18:52:03 @agent_ppo2.py:147][0m Total time:       9.98 min
[32m[20230114 18:52:03 @agent_ppo2.py:149][0m 854016 total steps have happened
[32m[20230114 18:52:03 @agent_ppo2.py:125][0m #------------------------ Iteration 417 --------------------------#
[32m[20230114 18:52:03 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:52:03 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:52:03 @agent_ppo2.py:189][0m |           0.0076 |          26.9782 |           4.8027 |
[32m[20230114 18:52:03 @agent_ppo2.py:189][0m |           0.0013 |          14.8483 |           4.7932 |
[32m[20230114 18:52:03 @agent_ppo2.py:189][0m |          -0.0019 |          13.3442 |           4.7925 |
[32m[20230114 18:52:03 @agent_ppo2.py:189][0m |           0.0053 |          12.5377 |           4.7843 |
[32m[20230114 18:52:04 @agent_ppo2.py:189][0m |          -0.0077 |          11.9148 |           4.7858 |
[32m[20230114 18:52:04 @agent_ppo2.py:189][0m |          -0.0084 |          11.3899 |           4.7814 |
[32m[20230114 18:52:04 @agent_ppo2.py:189][0m |          -0.0107 |          10.9144 |           4.7775 |
[32m[20230114 18:52:04 @agent_ppo2.py:189][0m |          -0.0117 |          10.6504 |           4.7722 |
[32m[20230114 18:52:04 @agent_ppo2.py:189][0m |          -0.0136 |          10.4208 |           4.7686 |
[32m[20230114 18:52:04 @agent_ppo2.py:189][0m |          -0.0138 |          10.2164 |           4.7708 |
[32m[20230114 18:52:04 @agent_ppo2.py:134][0m Policy update time: 0.70 s
[32m[20230114 18:52:04 @agent_ppo2.py:142][0m Average TRAINING episode reward: 200.68
[32m[20230114 18:52:04 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 263.13
[32m[20230114 18:52:04 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 273.31
[32m[20230114 18:52:04 @agent_ppo2.py:147][0m Total time:      10.00 min
[32m[20230114 18:52:04 @agent_ppo2.py:149][0m 856064 total steps have happened
[32m[20230114 18:52:04 @agent_ppo2.py:125][0m #------------------------ Iteration 418 --------------------------#
[32m[20230114 18:52:04 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:52:04 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:52:04 @agent_ppo2.py:189][0m |           0.0019 |          26.1856 |           4.6698 |
[32m[20230114 18:52:04 @agent_ppo2.py:189][0m |          -0.0050 |          15.4552 |           4.6786 |
[32m[20230114 18:52:05 @agent_ppo2.py:189][0m |          -0.0077 |          14.2729 |           4.6727 |
[32m[20230114 18:52:05 @agent_ppo2.py:189][0m |          -0.0071 |          13.2977 |           4.6761 |
[32m[20230114 18:52:05 @agent_ppo2.py:189][0m |          -0.0114 |          12.7048 |           4.6762 |
[32m[20230114 18:52:05 @agent_ppo2.py:189][0m |          -0.0110 |          12.1206 |           4.6788 |
[32m[20230114 18:52:05 @agent_ppo2.py:189][0m |          -0.0121 |          11.8144 |           4.6794 |
[32m[20230114 18:52:05 @agent_ppo2.py:189][0m |          -0.0132 |          11.5750 |           4.6812 |
[32m[20230114 18:52:05 @agent_ppo2.py:189][0m |          -0.0129 |          11.1670 |           4.6837 |
[32m[20230114 18:52:05 @agent_ppo2.py:189][0m |          -0.0136 |          11.1064 |           4.6811 |
[32m[20230114 18:52:05 @agent_ppo2.py:134][0m Policy update time: 0.69 s
[32m[20230114 18:52:05 @agent_ppo2.py:142][0m Average TRAINING episode reward: 203.03
[32m[20230114 18:52:05 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 264.47
[32m[20230114 18:52:05 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 279.17
[32m[20230114 18:52:05 @agent_ppo2.py:147][0m Total time:      10.02 min
[32m[20230114 18:52:05 @agent_ppo2.py:149][0m 858112 total steps have happened
[32m[20230114 18:52:05 @agent_ppo2.py:125][0m #------------------------ Iteration 419 --------------------------#
[32m[20230114 18:52:05 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:52:05 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:52:06 @agent_ppo2.py:189][0m |           0.0002 |          18.1062 |           4.8431 |
[32m[20230114 18:52:06 @agent_ppo2.py:189][0m |          -0.0029 |          15.2571 |           4.8338 |
[32m[20230114 18:52:06 @agent_ppo2.py:189][0m |          -0.0064 |          14.1345 |           4.8320 |
[32m[20230114 18:52:06 @agent_ppo2.py:189][0m |          -0.0082 |          13.5456 |           4.8285 |
[32m[20230114 18:52:06 @agent_ppo2.py:189][0m |          -0.0091 |          13.0347 |           4.8315 |
[32m[20230114 18:52:06 @agent_ppo2.py:189][0m |          -0.0101 |          12.8050 |           4.8221 |
[32m[20230114 18:52:06 @agent_ppo2.py:189][0m |          -0.0092 |          12.3494 |           4.8259 |
[32m[20230114 18:52:06 @agent_ppo2.py:189][0m |          -0.0104 |          12.0916 |           4.8242 |
[32m[20230114 18:52:06 @agent_ppo2.py:189][0m |          -0.0111 |          11.8162 |           4.8212 |
[32m[20230114 18:52:06 @agent_ppo2.py:189][0m |          -0.0124 |          11.5999 |           4.8225 |
[32m[20230114 18:52:06 @agent_ppo2.py:134][0m Policy update time: 0.75 s
[32m[20230114 18:52:06 @agent_ppo2.py:142][0m Average TRAINING episode reward: 216.65
[32m[20230114 18:52:06 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 262.09
[32m[20230114 18:52:06 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 277.61
[32m[20230114 18:52:06 @agent_ppo2.py:147][0m Total time:      10.04 min
[32m[20230114 18:52:06 @agent_ppo2.py:149][0m 860160 total steps have happened
[32m[20230114 18:52:06 @agent_ppo2.py:125][0m #------------------------ Iteration 420 --------------------------#
[32m[20230114 18:52:07 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:52:07 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:52:07 @agent_ppo2.py:189][0m |           0.0019 |          16.0906 |           4.7907 |
[32m[20230114 18:52:07 @agent_ppo2.py:189][0m |          -0.0044 |          14.3787 |           4.7856 |
[32m[20230114 18:52:07 @agent_ppo2.py:189][0m |          -0.0041 |          13.8691 |           4.7869 |
[32m[20230114 18:52:07 @agent_ppo2.py:189][0m |          -0.0056 |          13.5526 |           4.7864 |
[32m[20230114 18:52:07 @agent_ppo2.py:189][0m |          -0.0080 |          13.1424 |           4.7901 |
[32m[20230114 18:52:07 @agent_ppo2.py:189][0m |          -0.0089 |          12.9009 |           4.7884 |
[32m[20230114 18:52:07 @agent_ppo2.py:189][0m |          -0.0092 |          12.7189 |           4.7900 |
[32m[20230114 18:52:07 @agent_ppo2.py:189][0m |          -0.0101 |          12.5231 |           4.7896 |
[32m[20230114 18:52:07 @agent_ppo2.py:189][0m |          -0.0102 |          12.3955 |           4.7911 |
[32m[20230114 18:52:07 @agent_ppo2.py:189][0m |          -0.0101 |          12.2981 |           4.7884 |
[32m[20230114 18:52:07 @agent_ppo2.py:134][0m Policy update time: 0.79 s
[32m[20230114 18:52:08 @agent_ppo2.py:142][0m Average TRAINING episode reward: 260.59
[32m[20230114 18:52:08 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 262.77
[32m[20230114 18:52:08 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 277.91
[32m[20230114 18:52:08 @agent_ppo2.py:147][0m Total time:      10.06 min
[32m[20230114 18:52:08 @agent_ppo2.py:149][0m 862208 total steps have happened
[32m[20230114 18:52:08 @agent_ppo2.py:125][0m #------------------------ Iteration 421 --------------------------#
[32m[20230114 18:52:08 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:52:08 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:52:08 @agent_ppo2.py:189][0m |           0.0031 |          14.4233 |           4.8119 |
[32m[20230114 18:52:08 @agent_ppo2.py:189][0m |          -0.0119 |          13.0574 |           4.8050 |
[32m[20230114 18:52:08 @agent_ppo2.py:189][0m |          -0.0114 |          12.2708 |           4.8010 |
[32m[20230114 18:52:08 @agent_ppo2.py:189][0m |          -0.0053 |          11.7130 |           4.8034 |
[32m[20230114 18:52:08 @agent_ppo2.py:189][0m |          -0.0094 |          11.3088 |           4.8049 |
[32m[20230114 18:52:08 @agent_ppo2.py:189][0m |          -0.0096 |          11.0856 |           4.7973 |
[32m[20230114 18:52:08 @agent_ppo2.py:189][0m |           0.0113 |          12.2489 |           4.7962 |
[32m[20230114 18:52:08 @agent_ppo2.py:189][0m |          -0.0120 |          10.7486 |           4.7949 |
[32m[20230114 18:52:08 @agent_ppo2.py:189][0m |          -0.0149 |          10.6272 |           4.7957 |
[32m[20230114 18:52:09 @agent_ppo2.py:189][0m |          -0.0143 |          10.4885 |           4.7946 |
[32m[20230114 18:52:09 @agent_ppo2.py:134][0m Policy update time: 0.78 s
[32m[20230114 18:52:09 @agent_ppo2.py:142][0m Average TRAINING episode reward: 262.70
[32m[20230114 18:52:09 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 263.80
[32m[20230114 18:52:09 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 274.80
[32m[20230114 18:52:09 @agent_ppo2.py:147][0m Total time:      10.08 min
[32m[20230114 18:52:09 @agent_ppo2.py:149][0m 864256 total steps have happened
[32m[20230114 18:52:09 @agent_ppo2.py:125][0m #------------------------ Iteration 422 --------------------------#
[32m[20230114 18:52:09 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:52:09 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:52:09 @agent_ppo2.py:189][0m |          -0.0006 |          16.9227 |           4.8179 |
[32m[20230114 18:52:09 @agent_ppo2.py:189][0m |          -0.0054 |          12.8731 |           4.8136 |
[32m[20230114 18:52:09 @agent_ppo2.py:189][0m |          -0.0055 |          12.0881 |           4.8111 |
[32m[20230114 18:52:09 @agent_ppo2.py:189][0m |          -0.0074 |          11.7095 |           4.8103 |
[32m[20230114 18:52:09 @agent_ppo2.py:189][0m |          -0.0089 |          11.2065 |           4.8087 |
[32m[20230114 18:52:09 @agent_ppo2.py:189][0m |          -0.0095 |          10.9167 |           4.8050 |
[32m[20230114 18:52:10 @agent_ppo2.py:189][0m |          -0.0098 |          10.8263 |           4.8061 |
[32m[20230114 18:52:10 @agent_ppo2.py:189][0m |          -0.0114 |          10.3327 |           4.8049 |
[32m[20230114 18:52:10 @agent_ppo2.py:189][0m |          -0.0115 |          10.0626 |           4.8023 |
[32m[20230114 18:52:10 @agent_ppo2.py:189][0m |          -0.0125 |           9.8800 |           4.8039 |
[32m[20230114 18:52:10 @agent_ppo2.py:134][0m Policy update time: 0.73 s
[32m[20230114 18:52:10 @agent_ppo2.py:142][0m Average TRAINING episode reward: 218.79
[32m[20230114 18:52:10 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 263.01
[32m[20230114 18:52:10 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 90.53
[32m[20230114 18:52:10 @agent_ppo2.py:147][0m Total time:      10.10 min
[32m[20230114 18:52:10 @agent_ppo2.py:149][0m 866304 total steps have happened
[32m[20230114 18:52:10 @agent_ppo2.py:125][0m #------------------------ Iteration 423 --------------------------#
[32m[20230114 18:52:10 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:52:10 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:52:10 @agent_ppo2.py:189][0m |           0.0005 |          16.0862 |           4.7290 |
[32m[20230114 18:52:10 @agent_ppo2.py:189][0m |          -0.0058 |          14.7451 |           4.7235 |
[32m[20230114 18:52:10 @agent_ppo2.py:189][0m |          -0.0058 |          14.3652 |           4.7245 |
[32m[20230114 18:52:11 @agent_ppo2.py:189][0m |          -0.0090 |          13.7828 |           4.7177 |
[32m[20230114 18:52:11 @agent_ppo2.py:189][0m |          -0.0090 |          13.5319 |           4.7190 |
[32m[20230114 18:52:11 @agent_ppo2.py:189][0m |          -0.0093 |          13.3165 |           4.7191 |
[32m[20230114 18:52:11 @agent_ppo2.py:189][0m |          -0.0120 |          13.1654 |           4.7217 |
[32m[20230114 18:52:11 @agent_ppo2.py:189][0m |          -0.0121 |          13.0273 |           4.7188 |
[32m[20230114 18:52:11 @agent_ppo2.py:189][0m |          -0.0117 |          12.9357 |           4.7194 |
[32m[20230114 18:52:11 @agent_ppo2.py:189][0m |          -0.0135 |          12.8008 |           4.7209 |
[32m[20230114 18:52:11 @agent_ppo2.py:134][0m Policy update time: 0.77 s
[32m[20230114 18:52:11 @agent_ppo2.py:142][0m Average TRAINING episode reward: 262.53
[32m[20230114 18:52:11 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 263.20
[32m[20230114 18:52:11 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 276.22
[32m[20230114 18:52:11 @agent_ppo2.py:147][0m Total time:      10.12 min
[32m[20230114 18:52:11 @agent_ppo2.py:149][0m 868352 total steps have happened
[32m[20230114 18:52:11 @agent_ppo2.py:125][0m #------------------------ Iteration 424 --------------------------#
[32m[20230114 18:52:11 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:52:11 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:52:11 @agent_ppo2.py:189][0m |           0.0007 |          21.6059 |           4.7103 |
[32m[20230114 18:52:12 @agent_ppo2.py:189][0m |          -0.0040 |          13.4091 |           4.7055 |
[32m[20230114 18:52:12 @agent_ppo2.py:189][0m |          -0.0032 |          11.9888 |           4.6959 |
[32m[20230114 18:52:12 @agent_ppo2.py:189][0m |          -0.0094 |          11.4208 |           4.6987 |
[32m[20230114 18:52:12 @agent_ppo2.py:189][0m |          -0.0107 |          10.9510 |           4.6948 |
[32m[20230114 18:52:12 @agent_ppo2.py:189][0m |          -0.0094 |          10.6674 |           4.6939 |
[32m[20230114 18:52:12 @agent_ppo2.py:189][0m |          -0.0076 |          10.3630 |           4.6942 |
[32m[20230114 18:52:12 @agent_ppo2.py:189][0m |          -0.0128 |          10.1652 |           4.6971 |
[32m[20230114 18:52:12 @agent_ppo2.py:189][0m |          -0.0112 |           9.9927 |           4.6942 |
[32m[20230114 18:52:12 @agent_ppo2.py:189][0m |          -0.0106 |          10.0799 |           4.6915 |
[32m[20230114 18:52:12 @agent_ppo2.py:134][0m Policy update time: 0.71 s
[32m[20230114 18:52:12 @agent_ppo2.py:142][0m Average TRAINING episode reward: 205.93
[32m[20230114 18:52:12 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 267.38
[32m[20230114 18:52:12 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 274.67
[32m[20230114 18:52:12 @agent_ppo2.py:147][0m Total time:      10.14 min
[32m[20230114 18:52:12 @agent_ppo2.py:149][0m 870400 total steps have happened
[32m[20230114 18:52:12 @agent_ppo2.py:125][0m #------------------------ Iteration 425 --------------------------#
[32m[20230114 18:52:12 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:52:13 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:52:13 @agent_ppo2.py:189][0m |          -0.0024 |          15.9825 |           4.6748 |
[32m[20230114 18:52:13 @agent_ppo2.py:189][0m |          -0.0023 |          14.4900 |           4.6658 |
[32m[20230114 18:52:13 @agent_ppo2.py:189][0m |          -0.0068 |          13.6398 |           4.6608 |
[32m[20230114 18:52:13 @agent_ppo2.py:189][0m |          -0.0084 |          12.6611 |           4.6593 |
[32m[20230114 18:52:13 @agent_ppo2.py:189][0m |          -0.0101 |          11.8628 |           4.6613 |
[32m[20230114 18:52:13 @agent_ppo2.py:189][0m |          -0.0125 |          11.4471 |           4.6567 |
[32m[20230114 18:52:13 @agent_ppo2.py:189][0m |          -0.0139 |          11.0856 |           4.6567 |
[32m[20230114 18:52:13 @agent_ppo2.py:189][0m |          -0.0161 |          10.7816 |           4.6522 |
[32m[20230114 18:52:13 @agent_ppo2.py:189][0m |          -0.0127 |          10.5574 |           4.6605 |
[32m[20230114 18:52:13 @agent_ppo2.py:189][0m |          -0.0133 |          10.3863 |           4.6539 |
[32m[20230114 18:52:13 @agent_ppo2.py:134][0m Policy update time: 0.77 s
[32m[20230114 18:52:13 @agent_ppo2.py:142][0m Average TRAINING episode reward: 264.99
[32m[20230114 18:52:13 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 269.08
[32m[20230114 18:52:13 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 277.38
[32m[20230114 18:52:13 @agent_ppo2.py:147][0m Total time:      10.15 min
[32m[20230114 18:52:13 @agent_ppo2.py:149][0m 872448 total steps have happened
[32m[20230114 18:52:13 @agent_ppo2.py:125][0m #------------------------ Iteration 426 --------------------------#
[32m[20230114 18:52:14 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:52:14 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:52:14 @agent_ppo2.py:189][0m |           0.0006 |          16.3922 |           4.7313 |
[32m[20230114 18:52:14 @agent_ppo2.py:189][0m |          -0.0058 |          14.7834 |           4.7378 |
[32m[20230114 18:52:14 @agent_ppo2.py:189][0m |          -0.0032 |          14.1851 |           4.7244 |
[32m[20230114 18:52:14 @agent_ppo2.py:189][0m |          -0.0026 |          13.8296 |           4.7222 |
[32m[20230114 18:52:14 @agent_ppo2.py:189][0m |          -0.0064 |          13.4985 |           4.7165 |
[32m[20230114 18:52:14 @agent_ppo2.py:189][0m |          -0.0035 |          13.3353 |           4.7204 |
[32m[20230114 18:52:14 @agent_ppo2.py:189][0m |          -0.0094 |          13.0163 |           4.7092 |
[32m[20230114 18:52:14 @agent_ppo2.py:189][0m |          -0.0084 |          12.9082 |           4.7072 |
[32m[20230114 18:52:14 @agent_ppo2.py:189][0m |          -0.0159 |          12.6818 |           4.7069 |
[32m[20230114 18:52:14 @agent_ppo2.py:189][0m |          -0.0014 |          12.9515 |           4.7049 |
[32m[20230114 18:52:14 @agent_ppo2.py:134][0m Policy update time: 0.79 s
[32m[20230114 18:52:15 @agent_ppo2.py:142][0m Average TRAINING episode reward: 262.82
[32m[20230114 18:52:15 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 263.90
[32m[20230114 18:52:15 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 273.52
[32m[20230114 18:52:15 @agent_ppo2.py:147][0m Total time:      10.18 min
[32m[20230114 18:52:15 @agent_ppo2.py:149][0m 874496 total steps have happened
[32m[20230114 18:52:15 @agent_ppo2.py:125][0m #------------------------ Iteration 427 --------------------------#
[32m[20230114 18:52:15 @agent_ppo2.py:131][0m Sampling time: 0.28 s by 4 slaves
[32m[20230114 18:52:15 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:52:15 @agent_ppo2.py:189][0m |          -0.0005 |          34.1769 |           4.7383 |
[32m[20230114 18:52:15 @agent_ppo2.py:189][0m |          -0.0041 |          26.1509 |           4.7301 |
[32m[20230114 18:52:15 @agent_ppo2.py:189][0m |          -0.0063 |          24.0134 |           4.7256 |
[32m[20230114 18:52:15 @agent_ppo2.py:189][0m |          -0.0081 |          23.0441 |           4.7310 |
[32m[20230114 18:52:15 @agent_ppo2.py:189][0m |          -0.0102 |          21.9204 |           4.7270 |
[32m[20230114 18:52:15 @agent_ppo2.py:189][0m |          -0.0100 |          21.3434 |           4.7218 |
[32m[20230114 18:52:16 @agent_ppo2.py:189][0m |          -0.0118 |          20.8162 |           4.7233 |
[32m[20230114 18:52:16 @agent_ppo2.py:189][0m |          -0.0128 |          20.4017 |           4.7228 |
[32m[20230114 18:52:16 @agent_ppo2.py:189][0m |          -0.0135 |          19.9406 |           4.7238 |
[32m[20230114 18:52:16 @agent_ppo2.py:189][0m |          -0.0128 |          19.6346 |           4.7203 |
[32m[20230114 18:52:16 @agent_ppo2.py:134][0m Policy update time: 0.84 s
[32m[20230114 18:52:16 @agent_ppo2.py:142][0m Average TRAINING episode reward: 154.74
[32m[20230114 18:52:16 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 265.06
[32m[20230114 18:52:16 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 87.02
[32m[20230114 18:52:16 @agent_ppo2.py:147][0m Total time:      10.20 min
[32m[20230114 18:52:16 @agent_ppo2.py:149][0m 876544 total steps have happened
[32m[20230114 18:52:16 @agent_ppo2.py:125][0m #------------------------ Iteration 428 --------------------------#
[32m[20230114 18:52:16 @agent_ppo2.py:131][0m Sampling time: 0.29 s by 4 slaves
[32m[20230114 18:52:16 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:52:16 @agent_ppo2.py:189][0m |           0.0007 |          75.7603 |           4.7624 |
[32m[20230114 18:52:16 @agent_ppo2.py:189][0m |          -0.0046 |          62.1227 |           4.7511 |
[32m[20230114 18:52:16 @agent_ppo2.py:189][0m |          -0.0059 |          56.6501 |           4.7486 |
[32m[20230114 18:52:17 @agent_ppo2.py:189][0m |          -0.0079 |          51.4975 |           4.7447 |
[32m[20230114 18:52:17 @agent_ppo2.py:189][0m |          -0.0091 |          49.2111 |           4.7394 |
[32m[20230114 18:52:17 @agent_ppo2.py:189][0m |          -0.0087 |          46.8917 |           4.7373 |
[32m[20230114 18:52:17 @agent_ppo2.py:189][0m |          -0.0100 |          44.7051 |           4.7373 |
[32m[20230114 18:52:17 @agent_ppo2.py:189][0m |          -0.0108 |          42.7883 |           4.7379 |
[32m[20230114 18:52:17 @agent_ppo2.py:189][0m |          -0.0112 |          41.6976 |           4.7335 |
[32m[20230114 18:52:17 @agent_ppo2.py:189][0m |          -0.0113 |          40.6178 |           4.7322 |
[32m[20230114 18:52:17 @agent_ppo2.py:134][0m Policy update time: 0.72 s
[32m[20230114 18:52:17 @agent_ppo2.py:142][0m Average TRAINING episode reward: 129.36
[32m[20230114 18:52:17 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 262.30
[32m[20230114 18:52:17 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 276.14
[32m[20230114 18:52:17 @agent_ppo2.py:147][0m Total time:      10.22 min
[32m[20230114 18:52:17 @agent_ppo2.py:149][0m 878592 total steps have happened
[32m[20230114 18:52:17 @agent_ppo2.py:125][0m #------------------------ Iteration 429 --------------------------#
[32m[20230114 18:52:17 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:52:17 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:52:17 @agent_ppo2.py:189][0m |           0.0011 |          45.3839 |           4.7162 |
[32m[20230114 18:52:18 @agent_ppo2.py:189][0m |          -0.0049 |          36.6718 |           4.7132 |
[32m[20230114 18:52:18 @agent_ppo2.py:189][0m |          -0.0064 |          33.8230 |           4.7122 |
[32m[20230114 18:52:18 @agent_ppo2.py:189][0m |          -0.0095 |          32.0749 |           4.7093 |
[32m[20230114 18:52:18 @agent_ppo2.py:189][0m |          -0.0110 |          30.6267 |           4.7055 |
[32m[20230114 18:52:18 @agent_ppo2.py:189][0m |          -0.0091 |          30.3696 |           4.7043 |
[32m[20230114 18:52:18 @agent_ppo2.py:189][0m |          -0.0127 |          28.9945 |           4.7078 |
[32m[20230114 18:52:18 @agent_ppo2.py:189][0m |          -0.0113 |          28.6064 |           4.7049 |
[32m[20230114 18:52:18 @agent_ppo2.py:189][0m |          -0.0125 |          27.6168 |           4.7002 |
[32m[20230114 18:52:18 @agent_ppo2.py:189][0m |          -0.0134 |          27.7608 |           4.6962 |
[32m[20230114 18:52:18 @agent_ppo2.py:134][0m Policy update time: 0.71 s
[32m[20230114 18:52:18 @agent_ppo2.py:142][0m Average TRAINING episode reward: 210.66
[32m[20230114 18:52:18 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 262.31
[32m[20230114 18:52:18 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 114.10
[32m[20230114 18:52:18 @agent_ppo2.py:147][0m Total time:      10.24 min
[32m[20230114 18:52:18 @agent_ppo2.py:149][0m 880640 total steps have happened
[32m[20230114 18:52:18 @agent_ppo2.py:125][0m #------------------------ Iteration 430 --------------------------#
[32m[20230114 18:52:19 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:52:19 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:52:19 @agent_ppo2.py:189][0m |          -0.0006 |          34.2623 |           4.7261 |
[32m[20230114 18:52:19 @agent_ppo2.py:189][0m |          -0.0053 |          26.5147 |           4.7227 |
[32m[20230114 18:52:19 @agent_ppo2.py:189][0m |          -0.0088 |          24.3960 |           4.7204 |
[32m[20230114 18:52:19 @agent_ppo2.py:189][0m |          -0.0095 |          23.3867 |           4.7244 |
[32m[20230114 18:52:19 @agent_ppo2.py:189][0m |          -0.0124 |          22.0079 |           4.7265 |
[32m[20230114 18:52:19 @agent_ppo2.py:189][0m |          -0.0139 |          21.1933 |           4.7228 |
[32m[20230114 18:52:19 @agent_ppo2.py:189][0m |          -0.0140 |          20.5826 |           4.7250 |
[32m[20230114 18:52:19 @agent_ppo2.py:189][0m |          -0.0147 |          19.9477 |           4.7249 |
[32m[20230114 18:52:19 @agent_ppo2.py:189][0m |          -0.0150 |          19.4737 |           4.7251 |
[32m[20230114 18:52:19 @agent_ppo2.py:189][0m |          -0.0158 |          19.0445 |           4.7217 |
[32m[20230114 18:52:19 @agent_ppo2.py:134][0m Policy update time: 0.68 s
[32m[20230114 18:52:19 @agent_ppo2.py:142][0m Average TRAINING episode reward: 207.23
[32m[20230114 18:52:19 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 264.03
[32m[20230114 18:52:19 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 8.10
[32m[20230114 18:52:19 @agent_ppo2.py:147][0m Total time:      10.26 min
[32m[20230114 18:52:19 @agent_ppo2.py:149][0m 882688 total steps have happened
[32m[20230114 18:52:19 @agent_ppo2.py:125][0m #------------------------ Iteration 431 --------------------------#
[32m[20230114 18:52:20 @agent_ppo2.py:131][0m Sampling time: 0.21 s by 4 slaves
[32m[20230114 18:52:20 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:52:20 @agent_ppo2.py:189][0m |           0.0008 |          55.1639 |           4.7230 |
[32m[20230114 18:52:20 @agent_ppo2.py:189][0m |          -0.0045 |          43.6551 |           4.7215 |
[32m[20230114 18:52:20 @agent_ppo2.py:189][0m |          -0.0072 |          43.2297 |           4.7171 |
[32m[20230114 18:52:20 @agent_ppo2.py:189][0m |          -0.0077 |          40.2754 |           4.7143 |
[32m[20230114 18:52:20 @agent_ppo2.py:189][0m |          -0.0100 |          38.2129 |           4.7167 |
[32m[20230114 18:52:20 @agent_ppo2.py:189][0m |          -0.0081 |          37.4495 |           4.7143 |
[32m[20230114 18:52:20 @agent_ppo2.py:189][0m |          -0.0129 |          35.0589 |           4.7153 |
[32m[20230114 18:52:20 @agent_ppo2.py:189][0m |          -0.0116 |          34.2738 |           4.7115 |
[32m[20230114 18:52:20 @agent_ppo2.py:189][0m |          -0.0129 |          33.5042 |           4.7128 |
[32m[20230114 18:52:20 @agent_ppo2.py:189][0m |          -0.0131 |          33.0071 |           4.7110 |
[32m[20230114 18:52:20 @agent_ppo2.py:134][0m Policy update time: 0.72 s
[32m[20230114 18:52:21 @agent_ppo2.py:142][0m Average TRAINING episode reward: 187.16
[32m[20230114 18:52:21 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 261.92
[32m[20230114 18:52:21 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 26.25
[32m[20230114 18:52:21 @agent_ppo2.py:147][0m Total time:      10.27 min
[32m[20230114 18:52:21 @agent_ppo2.py:149][0m 884736 total steps have happened
[32m[20230114 18:52:21 @agent_ppo2.py:125][0m #------------------------ Iteration 432 --------------------------#
[32m[20230114 18:52:21 @agent_ppo2.py:131][0m Sampling time: 0.21 s by 4 slaves
[32m[20230114 18:52:21 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:52:21 @agent_ppo2.py:189][0m |           0.0002 |          45.9101 |           4.6501 |
[32m[20230114 18:52:21 @agent_ppo2.py:189][0m |          -0.0051 |          41.7625 |           4.6450 |
[32m[20230114 18:52:21 @agent_ppo2.py:189][0m |          -0.0059 |          39.5050 |           4.6443 |
[32m[20230114 18:52:21 @agent_ppo2.py:189][0m |          -0.0077 |          37.9680 |           4.6423 |
[32m[20230114 18:52:21 @agent_ppo2.py:189][0m |          -0.0081 |          36.4352 |           4.6447 |
[32m[20230114 18:52:21 @agent_ppo2.py:189][0m |          -0.0086 |          35.4777 |           4.6381 |
[32m[20230114 18:52:21 @agent_ppo2.py:189][0m |          -0.0094 |          34.6221 |           4.6408 |
[32m[20230114 18:52:21 @agent_ppo2.py:189][0m |          -0.0105 |          33.3391 |           4.6468 |
[32m[20230114 18:52:21 @agent_ppo2.py:189][0m |          -0.0114 |          32.6773 |           4.6457 |
[32m[20230114 18:52:21 @agent_ppo2.py:189][0m |          -0.0123 |          32.2860 |           4.6433 |
[32m[20230114 18:52:21 @agent_ppo2.py:134][0m Policy update time: 0.70 s
[32m[20230114 18:52:22 @agent_ppo2.py:142][0m Average TRAINING episode reward: 213.48
[32m[20230114 18:52:22 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 263.44
[32m[20230114 18:52:22 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 88.58
[32m[20230114 18:52:22 @agent_ppo2.py:147][0m Total time:      10.29 min
[32m[20230114 18:52:22 @agent_ppo2.py:149][0m 886784 total steps have happened
[32m[20230114 18:52:22 @agent_ppo2.py:125][0m #------------------------ Iteration 433 --------------------------#
[32m[20230114 18:52:22 @agent_ppo2.py:131][0m Sampling time: 0.29 s by 4 slaves
[32m[20230114 18:52:22 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:52:22 @agent_ppo2.py:189][0m |           0.0016 |          30.8637 |           4.7247 |
[32m[20230114 18:52:22 @agent_ppo2.py:189][0m |          -0.0037 |          26.3016 |           4.7189 |
[32m[20230114 18:52:22 @agent_ppo2.py:189][0m |          -0.0002 |          25.6138 |           4.7201 |
[32m[20230114 18:52:22 @agent_ppo2.py:189][0m |          -0.0095 |          24.0697 |           4.7142 |
[32m[20230114 18:52:22 @agent_ppo2.py:189][0m |           0.0028 |          24.1702 |           4.7182 |
[32m[20230114 18:52:22 @agent_ppo2.py:189][0m |          -0.0111 |          23.0517 |           4.7129 |
[32m[20230114 18:52:22 @agent_ppo2.py:189][0m |          -0.0048 |          24.0380 |           4.7144 |
[32m[20230114 18:52:23 @agent_ppo2.py:189][0m |          -0.0088 |          23.0628 |           4.7140 |
[32m[20230114 18:52:23 @agent_ppo2.py:189][0m |          -0.0107 |          22.3976 |           4.7132 |
[32m[20230114 18:52:23 @agent_ppo2.py:189][0m |          -0.0130 |          21.8366 |           4.7068 |
[32m[20230114 18:52:23 @agent_ppo2.py:134][0m Policy update time: 0.83 s
[32m[20230114 18:52:23 @agent_ppo2.py:142][0m Average TRAINING episode reward: 212.08
[32m[20230114 18:52:23 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 266.86
[32m[20230114 18:52:23 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 273.56
[32m[20230114 18:52:23 @agent_ppo2.py:147][0m Total time:      10.31 min
[32m[20230114 18:52:23 @agent_ppo2.py:149][0m 888832 total steps have happened
[32m[20230114 18:52:23 @agent_ppo2.py:125][0m #------------------------ Iteration 434 --------------------------#
[32m[20230114 18:52:23 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:52:23 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:52:23 @agent_ppo2.py:189][0m |           0.0023 |          64.5584 |           4.7069 |
[32m[20230114 18:52:23 @agent_ppo2.py:189][0m |          -0.0034 |          44.2734 |           4.7074 |
[32m[20230114 18:52:23 @agent_ppo2.py:189][0m |          -0.0085 |          36.9678 |           4.7037 |
[32m[20230114 18:52:23 @agent_ppo2.py:189][0m |          -0.0080 |          32.1603 |           4.7006 |
[32m[20230114 18:52:24 @agent_ppo2.py:189][0m |          -0.0122 |          28.6014 |           4.7008 |
[32m[20230114 18:52:24 @agent_ppo2.py:189][0m |          -0.0130 |          26.1523 |           4.6972 |
[32m[20230114 18:52:24 @agent_ppo2.py:189][0m |          -0.0143 |          24.3738 |           4.6976 |
[32m[20230114 18:52:24 @agent_ppo2.py:189][0m |          -0.0158 |          22.8583 |           4.6977 |
[32m[20230114 18:52:24 @agent_ppo2.py:189][0m |          -0.0147 |          22.0129 |           4.6984 |
[32m[20230114 18:52:24 @agent_ppo2.py:189][0m |          -0.0179 |          20.7324 |           4.6949 |
[32m[20230114 18:52:24 @agent_ppo2.py:134][0m Policy update time: 0.77 s
[32m[20230114 18:52:24 @agent_ppo2.py:142][0m Average TRAINING episode reward: 185.57
[32m[20230114 18:52:24 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 261.07
[32m[20230114 18:52:24 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 120.75
[32m[20230114 18:52:24 @agent_ppo2.py:147][0m Total time:      10.33 min
[32m[20230114 18:52:24 @agent_ppo2.py:149][0m 890880 total steps have happened
[32m[20230114 18:52:24 @agent_ppo2.py:125][0m #------------------------ Iteration 435 --------------------------#
[32m[20230114 18:52:24 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:52:24 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:52:25 @agent_ppo2.py:189][0m |          -0.0012 |          14.5417 |           4.7366 |
[32m[20230114 18:52:25 @agent_ppo2.py:189][0m |          -0.0042 |          13.6569 |           4.7297 |
[32m[20230114 18:52:25 @agent_ppo2.py:189][0m |          -0.0016 |          13.2830 |           4.7238 |
[32m[20230114 18:52:25 @agent_ppo2.py:189][0m |          -0.0093 |          13.0134 |           4.7251 |
[32m[20230114 18:52:25 @agent_ppo2.py:189][0m |          -0.0080 |          12.8367 |           4.7230 |
[32m[20230114 18:52:25 @agent_ppo2.py:189][0m |          -0.0018 |          13.3979 |           4.7227 |
[32m[20230114 18:52:25 @agent_ppo2.py:189][0m |          -0.0112 |          12.5403 |           4.7158 |
[32m[20230114 18:52:25 @agent_ppo2.py:189][0m |          -0.0104 |          12.4222 |           4.7212 |
[32m[20230114 18:52:25 @agent_ppo2.py:189][0m |          -0.0097 |          12.3300 |           4.7134 |
[32m[20230114 18:52:25 @agent_ppo2.py:189][0m |          -0.0141 |          12.2394 |           4.7152 |
[32m[20230114 18:52:25 @agent_ppo2.py:134][0m Policy update time: 0.78 s
[32m[20230114 18:52:25 @agent_ppo2.py:142][0m Average TRAINING episode reward: 260.50
[32m[20230114 18:52:25 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 261.96
[32m[20230114 18:52:25 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 273.72
[32m[20230114 18:52:25 @agent_ppo2.py:147][0m Total time:      10.35 min
[32m[20230114 18:52:25 @agent_ppo2.py:149][0m 892928 total steps have happened
[32m[20230114 18:52:25 @agent_ppo2.py:125][0m #------------------------ Iteration 436 --------------------------#
[32m[20230114 18:52:26 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:52:26 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:52:26 @agent_ppo2.py:189][0m |          -0.0027 |          37.3525 |           4.6745 |
[32m[20230114 18:52:26 @agent_ppo2.py:189][0m |          -0.0079 |          33.1770 |           4.6629 |
[32m[20230114 18:52:26 @agent_ppo2.py:189][0m |          -0.0155 |          29.7643 |           4.6654 |
[32m[20230114 18:52:26 @agent_ppo2.py:189][0m |          -0.0161 |          27.2086 |           4.6634 |
[32m[20230114 18:52:26 @agent_ppo2.py:189][0m |          -0.0136 |          25.2888 |           4.6643 |
[32m[20230114 18:52:26 @agent_ppo2.py:189][0m |          -0.0188 |          23.3514 |           4.6631 |
[32m[20230114 18:52:26 @agent_ppo2.py:189][0m |          -0.0185 |          21.9209 |           4.6565 |
[32m[20230114 18:52:26 @agent_ppo2.py:189][0m |          -0.0169 |          20.8517 |           4.6600 |
[32m[20230114 18:52:26 @agent_ppo2.py:189][0m |          -0.0213 |          19.8358 |           4.6575 |
[32m[20230114 18:52:26 @agent_ppo2.py:189][0m |          -0.0196 |          18.9716 |           4.6572 |
[32m[20230114 18:52:26 @agent_ppo2.py:134][0m Policy update time: 0.71 s
[32m[20230114 18:52:27 @agent_ppo2.py:142][0m Average TRAINING episode reward: 207.46
[32m[20230114 18:52:27 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 262.55
[32m[20230114 18:52:27 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 275.07
[32m[20230114 18:52:27 @agent_ppo2.py:147][0m Total time:      10.37 min
[32m[20230114 18:52:27 @agent_ppo2.py:149][0m 894976 total steps have happened
[32m[20230114 18:52:27 @agent_ppo2.py:125][0m #------------------------ Iteration 437 --------------------------#
[32m[20230114 18:52:27 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:52:27 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:52:27 @agent_ppo2.py:189][0m |           0.0019 |          39.5489 |           4.5951 |
[32m[20230114 18:52:27 @agent_ppo2.py:189][0m |          -0.0116 |          27.1518 |           4.5913 |
[32m[20230114 18:52:27 @agent_ppo2.py:189][0m |           0.0089 |          24.9183 |           4.5928 |
[32m[20230114 18:52:27 @agent_ppo2.py:189][0m |          -0.0102 |          20.2750 |           4.5855 |
[32m[20230114 18:52:27 @agent_ppo2.py:189][0m |          -0.0090 |          18.5851 |           4.5938 |
[32m[20230114 18:52:27 @agent_ppo2.py:189][0m |          -0.0053 |          17.4066 |           4.5943 |
[32m[20230114 18:52:27 @agent_ppo2.py:189][0m |           0.0120 |          19.0014 |           4.5906 |
[32m[20230114 18:52:27 @agent_ppo2.py:189][0m |          -0.0109 |          15.6022 |           4.5833 |
[32m[20230114 18:52:27 @agent_ppo2.py:189][0m |          -0.0123 |          14.7439 |           4.5936 |
[32m[20230114 18:52:27 @agent_ppo2.py:189][0m |          -0.0156 |          14.0924 |           4.5935 |
[32m[20230114 18:52:27 @agent_ppo2.py:134][0m Policy update time: 0.70 s
[32m[20230114 18:52:28 @agent_ppo2.py:142][0m Average TRAINING episode reward: 202.96
[32m[20230114 18:52:28 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 264.65
[32m[20230114 18:52:28 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 278.43
[32m[20230114 18:52:28 @agent_ppo2.py:147][0m Total time:      10.39 min
[32m[20230114 18:52:28 @agent_ppo2.py:149][0m 897024 total steps have happened
[32m[20230114 18:52:28 @agent_ppo2.py:125][0m #------------------------ Iteration 438 --------------------------#
[32m[20230114 18:52:28 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:52:28 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:52:28 @agent_ppo2.py:189][0m |           0.0017 |          15.1883 |           4.7537 |
[32m[20230114 18:52:28 @agent_ppo2.py:189][0m |          -0.0041 |          13.7297 |           4.7445 |
[32m[20230114 18:52:28 @agent_ppo2.py:189][0m |          -0.0051 |          13.0425 |           4.7529 |
[32m[20230114 18:52:28 @agent_ppo2.py:189][0m |          -0.0056 |          12.5323 |           4.7495 |
[32m[20230114 18:52:28 @agent_ppo2.py:189][0m |          -0.0070 |          12.0729 |           4.7454 |
[32m[20230114 18:52:28 @agent_ppo2.py:189][0m |          -0.0091 |          11.7100 |           4.7526 |
[32m[20230114 18:52:28 @agent_ppo2.py:189][0m |          -0.0091 |          11.3343 |           4.7569 |
[32m[20230114 18:52:29 @agent_ppo2.py:189][0m |          -0.0092 |          11.1027 |           4.7492 |
[32m[20230114 18:52:29 @agent_ppo2.py:189][0m |          -0.0104 |          10.7449 |           4.7514 |
[32m[20230114 18:52:29 @agent_ppo2.py:189][0m |          -0.0107 |          10.5219 |           4.7510 |
[32m[20230114 18:52:29 @agent_ppo2.py:134][0m Policy update time: 0.76 s
[32m[20230114 18:52:29 @agent_ppo2.py:142][0m Average TRAINING episode reward: 263.25
[32m[20230114 18:52:29 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 265.61
[32m[20230114 18:52:29 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 276.77
[32m[20230114 18:52:29 @agent_ppo2.py:147][0m Total time:      10.41 min
[32m[20230114 18:52:29 @agent_ppo2.py:149][0m 899072 total steps have happened
[32m[20230114 18:52:29 @agent_ppo2.py:125][0m #------------------------ Iteration 439 --------------------------#
[32m[20230114 18:52:29 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:52:29 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:52:29 @agent_ppo2.py:189][0m |           0.0012 |          44.1936 |           4.7240 |
[32m[20230114 18:52:29 @agent_ppo2.py:189][0m |          -0.0047 |          36.2908 |           4.7200 |
[32m[20230114 18:52:29 @agent_ppo2.py:189][0m |          -0.0059 |          34.3327 |           4.7207 |
[32m[20230114 18:52:29 @agent_ppo2.py:189][0m |          -0.0083 |          33.0403 |           4.7198 |
[32m[20230114 18:52:29 @agent_ppo2.py:189][0m |          -0.0092 |          30.5056 |           4.7150 |
[32m[20230114 18:52:30 @agent_ppo2.py:189][0m |          -0.0092 |          29.8020 |           4.7157 |
[32m[20230114 18:52:30 @agent_ppo2.py:189][0m |          -0.0103 |          29.3550 |           4.7177 |
[32m[20230114 18:52:30 @agent_ppo2.py:189][0m |          -0.0116 |          28.9812 |           4.7132 |
[32m[20230114 18:52:30 @agent_ppo2.py:189][0m |          -0.0129 |          28.1138 |           4.7175 |
[32m[20230114 18:52:30 @agent_ppo2.py:189][0m |          -0.0122 |          27.7831 |           4.7168 |
[32m[20230114 18:52:30 @agent_ppo2.py:134][0m Policy update time: 0.71 s
[32m[20230114 18:52:30 @agent_ppo2.py:142][0m Average TRAINING episode reward: 189.04
[32m[20230114 18:52:30 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 263.08
[32m[20230114 18:52:30 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 273.55
[32m[20230114 18:52:30 @agent_ppo2.py:147][0m Total time:      10.43 min
[32m[20230114 18:52:30 @agent_ppo2.py:149][0m 901120 total steps have happened
[32m[20230114 18:52:30 @agent_ppo2.py:125][0m #------------------------ Iteration 440 --------------------------#
[32m[20230114 18:52:30 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:52:30 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:52:30 @agent_ppo2.py:189][0m |           0.0021 |          38.9992 |           4.7658 |
[32m[20230114 18:52:30 @agent_ppo2.py:189][0m |          -0.0023 |          24.0342 |           4.7620 |
[32m[20230114 18:52:31 @agent_ppo2.py:189][0m |          -0.0030 |          18.4502 |           4.7588 |
[32m[20230114 18:52:31 @agent_ppo2.py:189][0m |          -0.0047 |          16.2113 |           4.7550 |
[32m[20230114 18:52:31 @agent_ppo2.py:189][0m |          -0.0059 |          14.8974 |           4.7568 |
[32m[20230114 18:52:31 @agent_ppo2.py:189][0m |          -0.0071 |          14.3454 |           4.7515 |
[32m[20230114 18:52:31 @agent_ppo2.py:189][0m |          -0.0070 |          13.9388 |           4.7594 |
[32m[20230114 18:52:31 @agent_ppo2.py:189][0m |          -0.0080 |          13.6405 |           4.7528 |
[32m[20230114 18:52:31 @agent_ppo2.py:189][0m |          -0.0084 |          13.4355 |           4.7557 |
[32m[20230114 18:52:31 @agent_ppo2.py:189][0m |          -0.0080 |          13.2675 |           4.7578 |
[32m[20230114 18:52:31 @agent_ppo2.py:134][0m Policy update time: 0.79 s
[32m[20230114 18:52:31 @agent_ppo2.py:142][0m Average TRAINING episode reward: 187.98
[32m[20230114 18:52:31 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 262.78
[32m[20230114 18:52:31 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 274.92
[32m[20230114 18:52:31 @agent_ppo2.py:147][0m Total time:      10.45 min
[32m[20230114 18:52:31 @agent_ppo2.py:149][0m 903168 total steps have happened
[32m[20230114 18:52:31 @agent_ppo2.py:125][0m #------------------------ Iteration 441 --------------------------#
[32m[20230114 18:52:31 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:52:32 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:52:32 @agent_ppo2.py:189][0m |          -0.0009 |          15.2996 |           4.7007 |
[32m[20230114 18:52:32 @agent_ppo2.py:189][0m |          -0.0056 |          13.9622 |           4.6921 |
[32m[20230114 18:52:32 @agent_ppo2.py:189][0m |          -0.0071 |          13.4635 |           4.6932 |
[32m[20230114 18:52:32 @agent_ppo2.py:189][0m |          -0.0092 |          13.1911 |           4.6865 |
[32m[20230114 18:52:32 @agent_ppo2.py:189][0m |          -0.0113 |          12.9881 |           4.6878 |
[32m[20230114 18:52:32 @agent_ppo2.py:189][0m |          -0.0126 |          12.7992 |           4.6834 |
[32m[20230114 18:52:32 @agent_ppo2.py:189][0m |          -0.0137 |          12.6677 |           4.6871 |
[32m[20230114 18:52:32 @agent_ppo2.py:189][0m |          -0.0123 |          12.5318 |           4.6873 |
[32m[20230114 18:52:32 @agent_ppo2.py:189][0m |          -0.0130 |          12.4791 |           4.6853 |
[32m[20230114 18:52:32 @agent_ppo2.py:189][0m |          -0.0142 |          12.3474 |           4.6875 |
[32m[20230114 18:52:32 @agent_ppo2.py:134][0m Policy update time: 0.76 s
[32m[20230114 18:52:32 @agent_ppo2.py:142][0m Average TRAINING episode reward: 264.42
[32m[20230114 18:52:32 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 265.81
[32m[20230114 18:52:32 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 137.99
[32m[20230114 18:52:32 @agent_ppo2.py:147][0m Total time:      10.47 min
[32m[20230114 18:52:32 @agent_ppo2.py:149][0m 905216 total steps have happened
[32m[20230114 18:52:32 @agent_ppo2.py:125][0m #------------------------ Iteration 442 --------------------------#
[32m[20230114 18:52:33 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:52:33 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:52:33 @agent_ppo2.py:189][0m |           0.0002 |          14.3929 |           4.7955 |
[32m[20230114 18:52:33 @agent_ppo2.py:189][0m |          -0.0071 |          13.3797 |           4.7846 |
[32m[20230114 18:52:33 @agent_ppo2.py:189][0m |          -0.0089 |          12.5606 |           4.7807 |
[32m[20230114 18:52:33 @agent_ppo2.py:189][0m |          -0.0068 |          11.8169 |           4.7793 |
[32m[20230114 18:52:33 @agent_ppo2.py:189][0m |          -0.0107 |          11.3200 |           4.7749 |
[32m[20230114 18:52:33 @agent_ppo2.py:189][0m |           0.0014 |          11.9006 |           4.7770 |
[32m[20230114 18:52:33 @agent_ppo2.py:189][0m |          -0.0064 |          10.6214 |           4.7669 |
[32m[20230114 18:52:33 @agent_ppo2.py:189][0m |          -0.0121 |          10.3112 |           4.7670 |
[32m[20230114 18:52:33 @agent_ppo2.py:189][0m |          -0.0106 |          10.1118 |           4.7658 |
[32m[20230114 18:52:33 @agent_ppo2.py:189][0m |          -0.0142 |           9.8911 |           4.7648 |
[32m[20230114 18:52:33 @agent_ppo2.py:134][0m Policy update time: 0.75 s
[32m[20230114 18:52:34 @agent_ppo2.py:142][0m Average TRAINING episode reward: 264.23
[32m[20230114 18:52:34 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 268.61
[32m[20230114 18:52:34 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 57.09
[32m[20230114 18:52:34 @agent_ppo2.py:147][0m Total time:      10.49 min
[32m[20230114 18:52:34 @agent_ppo2.py:149][0m 907264 total steps have happened
[32m[20230114 18:52:34 @agent_ppo2.py:125][0m #------------------------ Iteration 443 --------------------------#
[32m[20230114 18:52:34 @agent_ppo2.py:131][0m Sampling time: 0.21 s by 4 slaves
[32m[20230114 18:52:34 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:52:34 @agent_ppo2.py:189][0m |          -0.0015 |          15.5803 |           4.7939 |
[32m[20230114 18:52:34 @agent_ppo2.py:189][0m |          -0.0060 |          13.4096 |           4.7881 |
[32m[20230114 18:52:34 @agent_ppo2.py:189][0m |          -0.0088 |          12.5130 |           4.7930 |
[32m[20230114 18:52:34 @agent_ppo2.py:189][0m |          -0.0100 |          11.8946 |           4.7904 |
[32m[20230114 18:52:34 @agent_ppo2.py:189][0m |          -0.0098 |          11.4707 |           4.7930 |
[32m[20230114 18:52:34 @agent_ppo2.py:189][0m |          -0.0120 |          11.0295 |           4.7931 |
[32m[20230114 18:52:34 @agent_ppo2.py:189][0m |          -0.0082 |          10.9064 |           4.7920 |
[32m[20230114 18:52:34 @agent_ppo2.py:189][0m |          -0.0135 |          10.3131 |           4.7906 |
[32m[20230114 18:52:35 @agent_ppo2.py:189][0m |          -0.0111 |          10.0941 |           4.7910 |
[32m[20230114 18:52:35 @agent_ppo2.py:189][0m |          -0.0130 |           9.7224 |           4.7923 |
[32m[20230114 18:52:35 @agent_ppo2.py:134][0m Policy update time: 0.76 s
[32m[20230114 18:52:35 @agent_ppo2.py:142][0m Average TRAINING episode reward: 263.34
[32m[20230114 18:52:35 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 266.05
[32m[20230114 18:52:35 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 275.37
[32m[20230114 18:52:35 @agent_ppo2.py:147][0m Total time:      10.51 min
[32m[20230114 18:52:35 @agent_ppo2.py:149][0m 909312 total steps have happened
[32m[20230114 18:52:35 @agent_ppo2.py:125][0m #------------------------ Iteration 444 --------------------------#
[32m[20230114 18:52:35 @agent_ppo2.py:131][0m Sampling time: 0.21 s by 4 slaves
[32m[20230114 18:52:35 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:52:35 @agent_ppo2.py:189][0m |           0.0008 |          13.2563 |           4.7821 |
[32m[20230114 18:52:35 @agent_ppo2.py:189][0m |          -0.0048 |          10.6938 |           4.7745 |
[32m[20230114 18:52:35 @agent_ppo2.py:189][0m |          -0.0087 |           8.9239 |           4.7699 |
[32m[20230114 18:52:35 @agent_ppo2.py:189][0m |          -0.0096 |           7.7378 |           4.7694 |
[32m[20230114 18:52:35 @agent_ppo2.py:189][0m |          -0.0097 |           6.9764 |           4.7672 |
[32m[20230114 18:52:35 @agent_ppo2.py:189][0m |          -0.0119 |           6.2380 |           4.7707 |
[32m[20230114 18:52:36 @agent_ppo2.py:189][0m |          -0.0119 |           5.5866 |           4.7747 |
[32m[20230114 18:52:36 @agent_ppo2.py:189][0m |          -0.0111 |           5.0831 |           4.7714 |
[32m[20230114 18:52:36 @agent_ppo2.py:189][0m |          -0.0144 |           4.4824 |           4.7739 |
[32m[20230114 18:52:36 @agent_ppo2.py:189][0m |          -0.0142 |           4.1116 |           4.7739 |
[32m[20230114 18:52:36 @agent_ppo2.py:134][0m Policy update time: 0.75 s
[32m[20230114 18:52:36 @agent_ppo2.py:142][0m Average TRAINING episode reward: 264.90
[32m[20230114 18:52:36 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 266.01
[32m[20230114 18:52:36 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 53.81
[32m[20230114 18:52:36 @agent_ppo2.py:147][0m Total time:      10.53 min
[32m[20230114 18:52:36 @agent_ppo2.py:149][0m 911360 total steps have happened
[32m[20230114 18:52:36 @agent_ppo2.py:125][0m #------------------------ Iteration 445 --------------------------#
[32m[20230114 18:52:36 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:52:36 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:52:36 @agent_ppo2.py:189][0m |          -0.0054 |          19.4404 |           4.8235 |
[32m[20230114 18:52:36 @agent_ppo2.py:189][0m |          -0.0051 |          15.8246 |           4.8219 |
[32m[20230114 18:52:37 @agent_ppo2.py:189][0m |          -0.0078 |          14.9746 |           4.8251 |
[32m[20230114 18:52:37 @agent_ppo2.py:189][0m |          -0.0054 |          14.2746 |           4.8253 |
[32m[20230114 18:52:37 @agent_ppo2.py:189][0m |          -0.0096 |          13.7916 |           4.8277 |
[32m[20230114 18:52:37 @agent_ppo2.py:189][0m |          -0.0068 |          13.6254 |           4.8245 |
[32m[20230114 18:52:37 @agent_ppo2.py:189][0m |          -0.0097 |          12.9877 |           4.8248 |
[32m[20230114 18:52:37 @agent_ppo2.py:189][0m |          -0.0009 |          13.9217 |           4.8245 |
[32m[20230114 18:52:37 @agent_ppo2.py:189][0m |          -0.0091 |          12.3151 |           4.8196 |
[32m[20230114 18:52:37 @agent_ppo2.py:189][0m |          -0.0123 |          12.0265 |           4.8272 |
[32m[20230114 18:52:37 @agent_ppo2.py:134][0m Policy update time: 0.77 s
[32m[20230114 18:52:37 @agent_ppo2.py:142][0m Average TRAINING episode reward: 262.49
[32m[20230114 18:52:37 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 264.05
[32m[20230114 18:52:37 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 273.32
[32m[20230114 18:52:37 @agent_ppo2.py:147][0m Total time:      10.55 min
[32m[20230114 18:52:37 @agent_ppo2.py:149][0m 913408 total steps have happened
[32m[20230114 18:52:37 @agent_ppo2.py:125][0m #------------------------ Iteration 446 --------------------------#
[32m[20230114 18:52:38 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:52:38 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:52:38 @agent_ppo2.py:189][0m |          -0.0041 |          19.1725 |           4.8132 |
[32m[20230114 18:52:38 @agent_ppo2.py:189][0m |          -0.0099 |          16.0314 |           4.8032 |
[32m[20230114 18:52:38 @agent_ppo2.py:189][0m |          -0.0106 |          15.2140 |           4.7997 |
[32m[20230114 18:52:38 @agent_ppo2.py:189][0m |          -0.0094 |          14.8554 |           4.7984 |
[32m[20230114 18:52:38 @agent_ppo2.py:189][0m |          -0.0120 |          14.2693 |           4.8002 |
[32m[20230114 18:52:38 @agent_ppo2.py:189][0m |          -0.0122 |          14.0565 |           4.8025 |
[32m[20230114 18:52:38 @agent_ppo2.py:189][0m |          -0.0139 |          13.7291 |           4.8002 |
[32m[20230114 18:52:38 @agent_ppo2.py:189][0m |          -0.0160 |          13.5479 |           4.7993 |
[32m[20230114 18:52:38 @agent_ppo2.py:189][0m |          -0.0147 |          13.3715 |           4.8046 |
[32m[20230114 18:52:38 @agent_ppo2.py:189][0m |          -0.0176 |          13.1342 |           4.8063 |
[32m[20230114 18:52:38 @agent_ppo2.py:134][0m Policy update time: 0.79 s
[32m[20230114 18:52:39 @agent_ppo2.py:142][0m Average TRAINING episode reward: 258.92
[32m[20230114 18:52:39 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 261.19
[32m[20230114 18:52:39 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 116.62
[32m[20230114 18:52:39 @agent_ppo2.py:147][0m Total time:      10.57 min
[32m[20230114 18:52:39 @agent_ppo2.py:149][0m 915456 total steps have happened
[32m[20230114 18:52:39 @agent_ppo2.py:125][0m #------------------------ Iteration 447 --------------------------#
[32m[20230114 18:52:39 @agent_ppo2.py:131][0m Sampling time: 0.21 s by 4 slaves
[32m[20230114 18:52:39 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:52:39 @agent_ppo2.py:189][0m |           0.0018 |          14.7570 |           4.9680 |
[32m[20230114 18:52:39 @agent_ppo2.py:189][0m |          -0.0041 |          13.9594 |           4.9609 |
[32m[20230114 18:52:39 @agent_ppo2.py:189][0m |          -0.0071 |          13.3970 |           4.9556 |
[32m[20230114 18:52:39 @agent_ppo2.py:189][0m |          -0.0078 |          12.7932 |           4.9543 |
[32m[20230114 18:52:39 @agent_ppo2.py:189][0m |          -0.0087 |          12.2321 |           4.9499 |
[32m[20230114 18:52:39 @agent_ppo2.py:189][0m |          -0.0108 |          11.7613 |           4.9458 |
[32m[20230114 18:52:39 @agent_ppo2.py:189][0m |          -0.0111 |          11.4033 |           4.9550 |
[32m[20230114 18:52:39 @agent_ppo2.py:189][0m |          -0.0117 |          11.0966 |           4.9508 |
[32m[20230114 18:52:39 @agent_ppo2.py:189][0m |          -0.0127 |          10.8561 |           4.9492 |
[32m[20230114 18:52:40 @agent_ppo2.py:189][0m |          -0.0131 |          10.6660 |           4.9458 |
[32m[20230114 18:52:40 @agent_ppo2.py:134][0m Policy update time: 0.74 s
[32m[20230114 18:52:40 @agent_ppo2.py:142][0m Average TRAINING episode reward: 266.10
[32m[20230114 18:52:40 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 267.78
[32m[20230114 18:52:40 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 276.22
[32m[20230114 18:52:40 @agent_ppo2.py:147][0m Total time:      10.59 min
[32m[20230114 18:52:40 @agent_ppo2.py:149][0m 917504 total steps have happened
[32m[20230114 18:52:40 @agent_ppo2.py:125][0m #------------------------ Iteration 448 --------------------------#
[32m[20230114 18:52:40 @agent_ppo2.py:131][0m Sampling time: 0.30 s by 4 slaves
[32m[20230114 18:52:40 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:52:40 @agent_ppo2.py:189][0m |           0.0004 |          18.7865 |           4.8767 |
[32m[20230114 18:52:40 @agent_ppo2.py:189][0m |          -0.0053 |          14.7269 |           4.8683 |
[32m[20230114 18:52:40 @agent_ppo2.py:189][0m |           0.0042 |          14.2220 |           4.8579 |
[32m[20230114 18:52:40 @agent_ppo2.py:189][0m |          -0.0105 |          13.8975 |           4.8618 |
[32m[20230114 18:52:40 @agent_ppo2.py:189][0m |          -0.0108 |          13.6135 |           4.8580 |
[32m[20230114 18:52:41 @agent_ppo2.py:189][0m |          -0.0120 |          13.3758 |           4.8544 |
[32m[20230114 18:52:41 @agent_ppo2.py:189][0m |          -0.0116 |          13.2112 |           4.8512 |
[32m[20230114 18:52:41 @agent_ppo2.py:189][0m |          -0.0131 |          13.1339 |           4.8520 |
[32m[20230114 18:52:41 @agent_ppo2.py:189][0m |          -0.0161 |          12.9743 |           4.8462 |
[32m[20230114 18:52:41 @agent_ppo2.py:189][0m |          -0.0091 |          12.8481 |           4.8469 |
[32m[20230114 18:52:41 @agent_ppo2.py:134][0m Policy update time: 0.85 s
[32m[20230114 18:52:41 @agent_ppo2.py:142][0m Average TRAINING episode reward: 197.07
[32m[20230114 18:52:41 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 267.92
[32m[20230114 18:52:41 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 31.81
[32m[20230114 18:52:41 @agent_ppo2.py:147][0m Total time:      10.62 min
[32m[20230114 18:52:41 @agent_ppo2.py:149][0m 919552 total steps have happened
[32m[20230114 18:52:41 @agent_ppo2.py:125][0m #------------------------ Iteration 449 --------------------------#
[32m[20230114 18:52:41 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:52:41 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:52:42 @agent_ppo2.py:189][0m |           0.0013 |          16.2133 |           4.8636 |
[32m[20230114 18:52:42 @agent_ppo2.py:189][0m |          -0.0037 |          14.9950 |           4.8575 |
[32m[20230114 18:52:42 @agent_ppo2.py:189][0m |          -0.0053 |          14.6527 |           4.8569 |
[32m[20230114 18:52:42 @agent_ppo2.py:189][0m |          -0.0084 |          14.2721 |           4.8527 |
[32m[20230114 18:52:42 @agent_ppo2.py:189][0m |          -0.0098 |          13.8880 |           4.8526 |
[32m[20230114 18:52:42 @agent_ppo2.py:189][0m |          -0.0087 |          13.7889 |           4.8543 |
[32m[20230114 18:52:42 @agent_ppo2.py:189][0m |          -0.0114 |          13.4761 |           4.8548 |
[32m[20230114 18:52:42 @agent_ppo2.py:189][0m |          -0.0120 |          13.3188 |           4.8484 |
[32m[20230114 18:52:42 @agent_ppo2.py:189][0m |          -0.0113 |          13.2858 |           4.8527 |
[32m[20230114 18:52:42 @agent_ppo2.py:189][0m |          -0.0122 |          13.0873 |           4.8478 |
[32m[20230114 18:52:42 @agent_ppo2.py:134][0m Policy update time: 0.90 s
[32m[20230114 18:52:42 @agent_ppo2.py:142][0m Average TRAINING episode reward: 262.70
[32m[20230114 18:52:42 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 265.16
[32m[20230114 18:52:42 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 273.89
[32m[20230114 18:52:42 @agent_ppo2.py:147][0m Total time:      10.64 min
[32m[20230114 18:52:42 @agent_ppo2.py:149][0m 921600 total steps have happened
[32m[20230114 18:52:42 @agent_ppo2.py:125][0m #------------------------ Iteration 450 --------------------------#
[32m[20230114 18:52:43 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:52:43 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:52:43 @agent_ppo2.py:189][0m |           0.0013 |          10.2524 |           4.8058 |
[32m[20230114 18:52:43 @agent_ppo2.py:189][0m |           0.0031 |           5.8100 |           4.8083 |
[32m[20230114 18:52:43 @agent_ppo2.py:189][0m |          -0.0046 |           4.6030 |           4.7987 |
[32m[20230114 18:52:43 @agent_ppo2.py:189][0m |          -0.0075 |           4.1868 |           4.7996 |
[32m[20230114 18:52:43 @agent_ppo2.py:189][0m |          -0.0109 |           3.9394 |           4.8006 |
[32m[20230114 18:52:43 @agent_ppo2.py:189][0m |          -0.0105 |           3.7449 |           4.7983 |
[32m[20230114 18:52:43 @agent_ppo2.py:189][0m |          -0.0103 |           3.6543 |           4.7972 |
[32m[20230114 18:52:43 @agent_ppo2.py:189][0m |          -0.0117 |           3.4377 |           4.7977 |
[32m[20230114 18:52:43 @agent_ppo2.py:189][0m |          -0.0113 |           3.3164 |           4.7944 |
[32m[20230114 18:52:43 @agent_ppo2.py:189][0m |          -0.0009 |           3.2195 |           4.7954 |
[32m[20230114 18:52:43 @agent_ppo2.py:134][0m Policy update time: 0.78 s
[32m[20230114 18:52:44 @agent_ppo2.py:142][0m Average TRAINING episode reward: 261.92
[32m[20230114 18:52:44 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 264.68
[32m[20230114 18:52:44 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 274.63
[32m[20230114 18:52:44 @agent_ppo2.py:147][0m Total time:      10.66 min
[32m[20230114 18:52:44 @agent_ppo2.py:149][0m 923648 total steps have happened
[32m[20230114 18:52:44 @agent_ppo2.py:125][0m #------------------------ Iteration 451 --------------------------#
[32m[20230114 18:52:44 @agent_ppo2.py:131][0m Sampling time: 0.21 s by 4 slaves
[32m[20230114 18:52:44 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:52:44 @agent_ppo2.py:189][0m |           0.0026 |          13.7279 |           4.9537 |
[32m[20230114 18:52:44 @agent_ppo2.py:189][0m |          -0.0026 |          12.5634 |           4.9546 |
[32m[20230114 18:52:44 @agent_ppo2.py:189][0m |          -0.0054 |          12.0838 |           4.9520 |
[32m[20230114 18:52:44 @agent_ppo2.py:189][0m |          -0.0076 |          11.4871 |           4.9515 |
[32m[20230114 18:52:44 @agent_ppo2.py:189][0m |          -0.0071 |          11.1291 |           4.9536 |
[32m[20230114 18:52:44 @agent_ppo2.py:189][0m |          -0.0083 |          10.7033 |           4.9466 |
[32m[20230114 18:52:44 @agent_ppo2.py:189][0m |          -0.0084 |          10.5303 |           4.9534 |
[32m[20230114 18:52:45 @agent_ppo2.py:189][0m |          -0.0103 |          10.1997 |           4.9497 |
[32m[20230114 18:52:45 @agent_ppo2.py:189][0m |          -0.0103 |          10.0151 |           4.9474 |
[32m[20230114 18:52:45 @agent_ppo2.py:189][0m |          -0.0101 |           9.8130 |           4.9538 |
[32m[20230114 18:52:45 @agent_ppo2.py:134][0m Policy update time: 0.78 s
[32m[20230114 18:52:45 @agent_ppo2.py:142][0m Average TRAINING episode reward: 265.08
[32m[20230114 18:52:45 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 266.33
[32m[20230114 18:52:45 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 108.33
[32m[20230114 18:52:45 @agent_ppo2.py:147][0m Total time:      10.68 min
[32m[20230114 18:52:45 @agent_ppo2.py:149][0m 925696 total steps have happened
[32m[20230114 18:52:45 @agent_ppo2.py:125][0m #------------------------ Iteration 452 --------------------------#
[32m[20230114 18:52:45 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:52:45 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:52:45 @agent_ppo2.py:189][0m |           0.0013 |          16.8586 |           4.8648 |
[32m[20230114 18:52:45 @agent_ppo2.py:189][0m |          -0.0068 |          15.4760 |           4.8600 |
[32m[20230114 18:52:46 @agent_ppo2.py:189][0m |          -0.0076 |          15.2555 |           4.8600 |
[32m[20230114 18:52:46 @agent_ppo2.py:189][0m |          -0.0086 |          15.0934 |           4.8565 |
[32m[20230114 18:52:46 @agent_ppo2.py:189][0m |          -0.0088 |          14.9877 |           4.8526 |
[32m[20230114 18:52:46 @agent_ppo2.py:189][0m |          -0.0105 |          14.8792 |           4.8500 |
[32m[20230114 18:52:46 @agent_ppo2.py:189][0m |          -0.0113 |          14.8137 |           4.8532 |
[32m[20230114 18:52:46 @agent_ppo2.py:189][0m |          -0.0116 |          14.7104 |           4.8520 |
[32m[20230114 18:52:46 @agent_ppo2.py:189][0m |          -0.0132 |          14.6688 |           4.8526 |
[32m[20230114 18:52:46 @agent_ppo2.py:189][0m |          -0.0129 |          14.6073 |           4.8517 |
[32m[20230114 18:52:46 @agent_ppo2.py:134][0m Policy update time: 0.83 s
[32m[20230114 18:52:46 @agent_ppo2.py:142][0m Average TRAINING episode reward: 265.51
[32m[20230114 18:52:46 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 267.51
[32m[20230114 18:52:46 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 276.15
[32m[20230114 18:52:46 @agent_ppo2.py:147][0m Total time:      10.70 min
[32m[20230114 18:52:46 @agent_ppo2.py:149][0m 927744 total steps have happened
[32m[20230114 18:52:46 @agent_ppo2.py:125][0m #------------------------ Iteration 453 --------------------------#
[32m[20230114 18:52:46 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:52:47 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:52:47 @agent_ppo2.py:189][0m |           0.0006 |          16.3556 |           4.8041 |
[32m[20230114 18:52:47 @agent_ppo2.py:189][0m |          -0.0070 |          14.9680 |           4.7949 |
[32m[20230114 18:52:47 @agent_ppo2.py:189][0m |          -0.0071 |          14.3126 |           4.7915 |
[32m[20230114 18:52:47 @agent_ppo2.py:189][0m |          -0.0009 |          14.5611 |           4.7968 |
[32m[20230114 18:52:47 @agent_ppo2.py:189][0m |          -0.0044 |          14.0945 |           4.7983 |
[32m[20230114 18:52:47 @agent_ppo2.py:189][0m |          -0.0088 |          13.6243 |           4.7949 |
[32m[20230114 18:52:47 @agent_ppo2.py:189][0m |          -0.0101 |          13.4072 |           4.7979 |
[32m[20230114 18:52:47 @agent_ppo2.py:189][0m |          -0.0116 |          13.1697 |           4.7946 |
[32m[20230114 18:52:47 @agent_ppo2.py:189][0m |          -0.0140 |          12.9510 |           4.7920 |
[32m[20230114 18:52:47 @agent_ppo2.py:189][0m |          -0.0161 |          12.7304 |           4.7955 |
[32m[20230114 18:52:47 @agent_ppo2.py:134][0m Policy update time: 0.87 s
[32m[20230114 18:52:48 @agent_ppo2.py:142][0m Average TRAINING episode reward: 263.15
[32m[20230114 18:52:48 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 264.70
[32m[20230114 18:52:48 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 275.92
[32m[20230114 18:52:48 @agent_ppo2.py:147][0m Total time:      10.72 min
[32m[20230114 18:52:48 @agent_ppo2.py:149][0m 929792 total steps have happened
[32m[20230114 18:52:48 @agent_ppo2.py:125][0m #------------------------ Iteration 454 --------------------------#
[32m[20230114 18:52:48 @agent_ppo2.py:131][0m Sampling time: 0.25 s by 4 slaves
[32m[20230114 18:52:48 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:52:48 @agent_ppo2.py:189][0m |          -0.0045 |          21.5009 |           4.8153 |
[32m[20230114 18:52:48 @agent_ppo2.py:189][0m |          -0.0080 |          13.6713 |           4.7998 |
[32m[20230114 18:52:48 @agent_ppo2.py:189][0m |          -0.0107 |          12.7307 |           4.7982 |
[32m[20230114 18:52:48 @agent_ppo2.py:189][0m |           0.0061 |          12.0922 |           4.7947 |
[32m[20230114 18:52:48 @agent_ppo2.py:189][0m |          -0.0132 |          11.6625 |           4.7863 |
[32m[20230114 18:52:48 @agent_ppo2.py:189][0m |          -0.0157 |          11.3881 |           4.7882 |
[32m[20230114 18:52:48 @agent_ppo2.py:189][0m |          -0.0048 |          11.9462 |           4.7847 |
[32m[20230114 18:52:48 @agent_ppo2.py:189][0m |           0.0051 |          10.9841 |           4.7768 |
[32m[20230114 18:52:49 @agent_ppo2.py:189][0m |          -0.0181 |          10.7826 |           4.7772 |
[32m[20230114 18:52:49 @agent_ppo2.py:189][0m |          -0.0049 |          12.4125 |           4.7770 |
[32m[20230114 18:52:49 @agent_ppo2.py:134][0m Policy update time: 0.77 s
[32m[20230114 18:52:49 @agent_ppo2.py:142][0m Average TRAINING episode reward: 231.38
[32m[20230114 18:52:49 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 265.59
[32m[20230114 18:52:49 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 272.96
[32m[20230114 18:52:49 @agent_ppo2.py:147][0m Total time:      10.74 min
[32m[20230114 18:52:49 @agent_ppo2.py:149][0m 931840 total steps have happened
[32m[20230114 18:52:49 @agent_ppo2.py:125][0m #------------------------ Iteration 455 --------------------------#
[32m[20230114 18:52:49 @agent_ppo2.py:131][0m Sampling time: 0.25 s by 4 slaves
[32m[20230114 18:52:49 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:52:49 @agent_ppo2.py:189][0m |          -0.0004 |          24.2411 |           4.7575 |
[32m[20230114 18:52:49 @agent_ppo2.py:189][0m |          -0.0083 |          15.0582 |           4.7507 |
[32m[20230114 18:52:49 @agent_ppo2.py:189][0m |          -0.0097 |          14.0462 |           4.7495 |
[32m[20230114 18:52:49 @agent_ppo2.py:189][0m |          -0.0101 |          13.3145 |           4.7464 |
[32m[20230114 18:52:49 @agent_ppo2.py:189][0m |          -0.0113 |          12.7588 |           4.7460 |
[32m[20230114 18:52:50 @agent_ppo2.py:189][0m |          -0.0118 |          12.3556 |           4.7436 |
[32m[20230114 18:52:50 @agent_ppo2.py:189][0m |          -0.0131 |          12.0639 |           4.7453 |
[32m[20230114 18:52:50 @agent_ppo2.py:189][0m |          -0.0143 |          11.7552 |           4.7420 |
[32m[20230114 18:52:50 @agent_ppo2.py:189][0m |          -0.0129 |          11.5746 |           4.7433 |
[32m[20230114 18:52:50 @agent_ppo2.py:189][0m |          -0.0143 |          11.3892 |           4.7367 |
[32m[20230114 18:52:50 @agent_ppo2.py:134][0m Policy update time: 0.86 s
[32m[20230114 18:52:50 @agent_ppo2.py:142][0m Average TRAINING episode reward: 216.08
[32m[20230114 18:52:50 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 263.36
[32m[20230114 18:52:50 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 281.66
[32m[20230114 18:52:50 @agent_ppo2.py:147][0m Total time:      10.77 min
[32m[20230114 18:52:50 @agent_ppo2.py:149][0m 933888 total steps have happened
[32m[20230114 18:52:50 @agent_ppo2.py:125][0m #------------------------ Iteration 456 --------------------------#
[32m[20230114 18:52:50 @agent_ppo2.py:131][0m Sampling time: 0.25 s by 4 slaves
[32m[20230114 18:52:50 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:52:51 @agent_ppo2.py:189][0m |           0.0107 |          15.4938 |           4.7540 |
[32m[20230114 18:52:51 @agent_ppo2.py:189][0m |          -0.0041 |          12.8211 |           4.7573 |
[32m[20230114 18:52:51 @agent_ppo2.py:189][0m |          -0.0054 |          12.2256 |           4.7604 |
[32m[20230114 18:52:51 @agent_ppo2.py:189][0m |          -0.0066 |          11.8620 |           4.7611 |
[32m[20230114 18:52:51 @agent_ppo2.py:189][0m |          -0.0069 |          11.5073 |           4.7629 |
[32m[20230114 18:52:51 @agent_ppo2.py:189][0m |          -0.0112 |          11.3069 |           4.7611 |
[32m[20230114 18:52:51 @agent_ppo2.py:189][0m |          -0.0120 |          11.0245 |           4.7599 |
[32m[20230114 18:52:51 @agent_ppo2.py:189][0m |          -0.0108 |          10.8283 |           4.7652 |
[32m[20230114 18:52:51 @agent_ppo2.py:189][0m |          -0.0120 |          10.6618 |           4.7638 |
[32m[20230114 18:52:51 @agent_ppo2.py:189][0m |          -0.0056 |          11.1540 |           4.7611 |
[32m[20230114 18:52:51 @agent_ppo2.py:134][0m Policy update time: 0.91 s
[32m[20230114 18:52:52 @agent_ppo2.py:142][0m Average TRAINING episode reward: 262.28
[32m[20230114 18:52:52 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 264.48
[32m[20230114 18:52:52 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 277.56
[32m[20230114 18:52:52 @agent_ppo2.py:147][0m Total time:      10.79 min
[32m[20230114 18:52:52 @agent_ppo2.py:149][0m 935936 total steps have happened
[32m[20230114 18:52:52 @agent_ppo2.py:125][0m #------------------------ Iteration 457 --------------------------#
[32m[20230114 18:52:52 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:52:52 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:52:52 @agent_ppo2.py:189][0m |           0.0002 |          14.2031 |           4.7779 |
[32m[20230114 18:52:52 @agent_ppo2.py:189][0m |          -0.0082 |           9.6654 |           4.7724 |
[32m[20230114 18:52:52 @agent_ppo2.py:189][0m |          -0.0091 |           8.0026 |           4.7663 |
[32m[20230114 18:52:52 @agent_ppo2.py:189][0m |          -0.0077 |           6.7742 |           4.7711 |
[32m[20230114 18:52:52 @agent_ppo2.py:189][0m |          -0.0102 |           5.5536 |           4.7626 |
[32m[20230114 18:52:52 @agent_ppo2.py:189][0m |          -0.0128 |           4.7231 |           4.7628 |
[32m[20230114 18:52:52 @agent_ppo2.py:189][0m |          -0.0132 |           4.1584 |           4.7644 |
[32m[20230114 18:52:52 @agent_ppo2.py:189][0m |          -0.0117 |           3.8364 |           4.7614 |
[32m[20230114 18:52:52 @agent_ppo2.py:189][0m |          -0.0134 |           3.6068 |           4.7598 |
[32m[20230114 18:52:53 @agent_ppo2.py:189][0m |          -0.0136 |           3.4769 |           4.7593 |
[32m[20230114 18:52:53 @agent_ppo2.py:134][0m Policy update time: 0.78 s
[32m[20230114 18:52:53 @agent_ppo2.py:142][0m Average TRAINING episode reward: 263.89
[32m[20230114 18:52:53 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 264.70
[32m[20230114 18:52:53 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 276.70
[32m[20230114 18:52:53 @agent_ppo2.py:147][0m Total time:      10.81 min
[32m[20230114 18:52:53 @agent_ppo2.py:149][0m 937984 total steps have happened
[32m[20230114 18:52:53 @agent_ppo2.py:125][0m #------------------------ Iteration 458 --------------------------#
[32m[20230114 18:52:53 @agent_ppo2.py:131][0m Sampling time: 0.21 s by 4 slaves
[32m[20230114 18:52:53 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:52:53 @agent_ppo2.py:189][0m |           0.0019 |          21.7802 |           4.7928 |
[32m[20230114 18:52:53 @agent_ppo2.py:189][0m |          -0.0018 |          14.1262 |           4.7821 |
[32m[20230114 18:52:53 @agent_ppo2.py:189][0m |          -0.0043 |          12.6411 |           4.7709 |
[32m[20230114 18:52:53 @agent_ppo2.py:189][0m |          -0.0098 |          11.9525 |           4.7895 |
[32m[20230114 18:52:53 @agent_ppo2.py:189][0m |          -0.0131 |          11.2608 |           4.7842 |
[32m[20230114 18:52:53 @agent_ppo2.py:189][0m |          -0.0128 |          10.6490 |           4.7819 |
[32m[20230114 18:52:54 @agent_ppo2.py:189][0m |          -0.0148 |          10.3917 |           4.7785 |
[32m[20230114 18:52:54 @agent_ppo2.py:189][0m |          -0.0153 |          10.0920 |           4.7827 |
[32m[20230114 18:52:54 @agent_ppo2.py:189][0m |          -0.0125 |           9.9615 |           4.7849 |
[32m[20230114 18:52:54 @agent_ppo2.py:189][0m |          -0.0151 |           9.6534 |           4.7792 |
[32m[20230114 18:52:54 @agent_ppo2.py:134][0m Policy update time: 0.70 s
[32m[20230114 18:52:54 @agent_ppo2.py:142][0m Average TRAINING episode reward: 206.09
[32m[20230114 18:52:54 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 264.95
[32m[20230114 18:52:54 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 273.35
[32m[20230114 18:52:54 @agent_ppo2.py:147][0m Total time:      10.83 min
[32m[20230114 18:52:54 @agent_ppo2.py:149][0m 940032 total steps have happened
[32m[20230114 18:52:54 @agent_ppo2.py:125][0m #------------------------ Iteration 459 --------------------------#
[32m[20230114 18:52:54 @agent_ppo2.py:131][0m Sampling time: 0.26 s by 4 slaves
[32m[20230114 18:52:54 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:52:54 @agent_ppo2.py:189][0m |          -0.0012 |          45.1518 |           4.7414 |
[32m[20230114 18:52:54 @agent_ppo2.py:189][0m |          -0.0079 |          31.6339 |           4.7294 |
[32m[20230114 18:52:54 @agent_ppo2.py:189][0m |          -0.0097 |          28.0226 |           4.7268 |
[32m[20230114 18:52:55 @agent_ppo2.py:189][0m |          -0.0115 |          25.7631 |           4.7212 |
[32m[20230114 18:52:55 @agent_ppo2.py:189][0m |          -0.0124 |          24.2766 |           4.7213 |
[32m[20230114 18:52:55 @agent_ppo2.py:189][0m |          -0.0127 |          23.0602 |           4.7167 |
[32m[20230114 18:52:55 @agent_ppo2.py:189][0m |          -0.0138 |          22.4342 |           4.7181 |
[32m[20230114 18:52:55 @agent_ppo2.py:189][0m |          -0.0145 |          21.4594 |           4.7139 |
[32m[20230114 18:52:55 @agent_ppo2.py:189][0m |          -0.0154 |          20.9577 |           4.7123 |
[32m[20230114 18:52:55 @agent_ppo2.py:189][0m |          -0.0159 |          20.0934 |           4.7157 |
[32m[20230114 18:52:55 @agent_ppo2.py:134][0m Policy update time: 0.93 s
[32m[20230114 18:52:55 @agent_ppo2.py:142][0m Average TRAINING episode reward: 161.29
[32m[20230114 18:52:55 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 261.79
[32m[20230114 18:52:55 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 276.87
[32m[20230114 18:52:55 @agent_ppo2.py:147][0m Total time:      10.85 min
[32m[20230114 18:52:55 @agent_ppo2.py:149][0m 942080 total steps have happened
[32m[20230114 18:52:55 @agent_ppo2.py:125][0m #------------------------ Iteration 460 --------------------------#
[32m[20230114 18:52:56 @agent_ppo2.py:131][0m Sampling time: 0.21 s by 4 slaves
[32m[20230114 18:52:56 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:52:56 @agent_ppo2.py:189][0m |          -0.0010 |          39.3897 |           4.7049 |
[32m[20230114 18:52:56 @agent_ppo2.py:189][0m |          -0.0074 |          21.6365 |           4.6981 |
[32m[20230114 18:52:56 @agent_ppo2.py:189][0m |          -0.0094 |          17.2158 |           4.6947 |
[32m[20230114 18:52:56 @agent_ppo2.py:189][0m |          -0.0117 |          15.3870 |           4.6915 |
[32m[20230114 18:52:56 @agent_ppo2.py:189][0m |          -0.0126 |          14.3182 |           4.6867 |
[32m[20230114 18:52:56 @agent_ppo2.py:189][0m |          -0.0141 |          13.6585 |           4.6908 |
[32m[20230114 18:52:56 @agent_ppo2.py:189][0m |          -0.0146 |          13.1123 |           4.6865 |
[32m[20230114 18:52:56 @agent_ppo2.py:189][0m |          -0.0150 |          12.7568 |           4.6880 |
[32m[20230114 18:52:56 @agent_ppo2.py:189][0m |          -0.0163 |          12.4947 |           4.6842 |
[32m[20230114 18:52:56 @agent_ppo2.py:189][0m |          -0.0161 |          12.3362 |           4.6881 |
[32m[20230114 18:52:56 @agent_ppo2.py:134][0m Policy update time: 0.71 s
[32m[20230114 18:52:56 @agent_ppo2.py:142][0m Average TRAINING episode reward: 206.20
[32m[20230114 18:52:56 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 265.60
[32m[20230114 18:52:56 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 275.75
[32m[20230114 18:52:56 @agent_ppo2.py:147][0m Total time:      10.87 min
[32m[20230114 18:52:56 @agent_ppo2.py:149][0m 944128 total steps have happened
[32m[20230114 18:52:56 @agent_ppo2.py:125][0m #------------------------ Iteration 461 --------------------------#
[32m[20230114 18:52:57 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:52:57 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:52:57 @agent_ppo2.py:189][0m |           0.0018 |          30.1216 |           4.7749 |
[32m[20230114 18:52:57 @agent_ppo2.py:189][0m |          -0.0009 |          25.3818 |           4.7728 |
[32m[20230114 18:52:57 @agent_ppo2.py:189][0m |          -0.0060 |          23.2168 |           4.7669 |
[32m[20230114 18:52:57 @agent_ppo2.py:189][0m |          -0.0074 |          21.9556 |           4.7675 |
[32m[20230114 18:52:57 @agent_ppo2.py:189][0m |          -0.0078 |          21.2120 |           4.7663 |
[32m[20230114 18:52:57 @agent_ppo2.py:189][0m |          -0.0083 |          20.5007 |           4.7660 |
[32m[20230114 18:52:57 @agent_ppo2.py:189][0m |          -0.0066 |          19.8247 |           4.7624 |
[32m[20230114 18:52:57 @agent_ppo2.py:189][0m |          -0.0108 |          18.8666 |           4.7623 |
[32m[20230114 18:52:57 @agent_ppo2.py:189][0m |          -0.0101 |          18.1899 |           4.7632 |
[32m[20230114 18:52:58 @agent_ppo2.py:189][0m |          -0.0123 |          17.6592 |           4.7658 |
[32m[20230114 18:52:58 @agent_ppo2.py:134][0m Policy update time: 0.89 s
[32m[20230114 18:52:58 @agent_ppo2.py:142][0m Average TRAINING episode reward: 228.89
[32m[20230114 18:52:58 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 265.49
[32m[20230114 18:52:58 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 274.01
[32m[20230114 18:52:58 @agent_ppo2.py:147][0m Total time:      10.89 min
[32m[20230114 18:52:58 @agent_ppo2.py:149][0m 946176 total steps have happened
[32m[20230114 18:52:58 @agent_ppo2.py:125][0m #------------------------ Iteration 462 --------------------------#
[32m[20230114 18:52:58 @agent_ppo2.py:131][0m Sampling time: 0.21 s by 4 slaves
[32m[20230114 18:52:58 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:52:58 @agent_ppo2.py:189][0m |           0.0014 |          79.3967 |           4.7356 |
[32m[20230114 18:52:58 @agent_ppo2.py:189][0m |          -0.0077 |          52.0825 |           4.7228 |
[32m[20230114 18:52:58 @agent_ppo2.py:189][0m |          -0.0109 |          45.3966 |           4.7232 |
[32m[20230114 18:52:58 @agent_ppo2.py:189][0m |          -0.0125 |          42.2323 |           4.7244 |
[32m[20230114 18:52:58 @agent_ppo2.py:189][0m |          -0.0140 |          39.4374 |           4.7222 |
[32m[20230114 18:52:58 @agent_ppo2.py:189][0m |          -0.0149 |          37.7472 |           4.7209 |
[32m[20230114 18:52:58 @agent_ppo2.py:189][0m |          -0.0160 |          35.8956 |           4.7192 |
[32m[20230114 18:52:58 @agent_ppo2.py:189][0m |          -0.0180 |          34.5957 |           4.7207 |
[32m[20230114 18:52:58 @agent_ppo2.py:189][0m |          -0.0178 |          33.5779 |           4.7232 |
[32m[20230114 18:52:59 @agent_ppo2.py:189][0m |          -0.0192 |          32.6669 |           4.7199 |
[32m[20230114 18:52:59 @agent_ppo2.py:134][0m Policy update time: 0.54 s
[32m[20230114 18:52:59 @agent_ppo2.py:142][0m Average TRAINING episode reward: 74.72
[32m[20230114 18:52:59 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 261.43
[32m[20230114 18:52:59 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 81.58
[32m[20230114 18:52:59 @agent_ppo2.py:147][0m Total time:      10.91 min
[32m[20230114 18:52:59 @agent_ppo2.py:149][0m 948224 total steps have happened
[32m[20230114 18:52:59 @agent_ppo2.py:125][0m #------------------------ Iteration 463 --------------------------#
[32m[20230114 18:52:59 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:52:59 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:52:59 @agent_ppo2.py:189][0m |           0.0023 |          50.8403 |           4.7492 |
[32m[20230114 18:52:59 @agent_ppo2.py:189][0m |          -0.0029 |          31.6518 |           4.7417 |
[32m[20230114 18:52:59 @agent_ppo2.py:189][0m |          -0.0060 |          28.1633 |           4.7415 |
[32m[20230114 18:52:59 @agent_ppo2.py:189][0m |          -0.0040 |          26.2649 |           4.7337 |
[32m[20230114 18:52:59 @agent_ppo2.py:189][0m |          -0.0074 |          24.5755 |           4.7316 |
[32m[20230114 18:52:59 @agent_ppo2.py:189][0m |          -0.0106 |          23.5593 |           4.7280 |
[32m[20230114 18:53:00 @agent_ppo2.py:189][0m |          -0.0100 |          23.1512 |           4.7335 |
[32m[20230114 18:53:00 @agent_ppo2.py:189][0m |          -0.0116 |          21.8709 |           4.7319 |
[32m[20230114 18:53:00 @agent_ppo2.py:189][0m |          -0.0194 |          21.0276 |           4.7338 |
[32m[20230114 18:53:00 @agent_ppo2.py:189][0m |          -0.0170 |          20.7175 |           4.7338 |
[32m[20230114 18:53:00 @agent_ppo2.py:134][0m Policy update time: 0.81 s
[32m[20230114 18:53:00 @agent_ppo2.py:142][0m Average TRAINING episode reward: 191.27
[32m[20230114 18:53:00 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 262.45
[32m[20230114 18:53:00 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 273.36
[32m[20230114 18:53:00 @agent_ppo2.py:147][0m Total time:      10.93 min
[32m[20230114 18:53:00 @agent_ppo2.py:149][0m 950272 total steps have happened
[32m[20230114 18:53:00 @agent_ppo2.py:125][0m #------------------------ Iteration 464 --------------------------#
[32m[20230114 18:53:00 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:53:00 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:53:00 @agent_ppo2.py:189][0m |          -0.0016 |          17.2504 |           4.6969 |
[32m[20230114 18:53:00 @agent_ppo2.py:189][0m |          -0.0027 |          14.8888 |           4.6810 |
[32m[20230114 18:53:00 @agent_ppo2.py:189][0m |          -0.0059 |          13.7543 |           4.6883 |
[32m[20230114 18:53:01 @agent_ppo2.py:189][0m |          -0.0049 |          13.0709 |           4.6823 |
[32m[20230114 18:53:01 @agent_ppo2.py:189][0m |          -0.0072 |          12.4729 |           4.6831 |
[32m[20230114 18:53:01 @agent_ppo2.py:189][0m |          -0.0059 |          12.0731 |           4.6790 |
[32m[20230114 18:53:01 @agent_ppo2.py:189][0m |          -0.0069 |          11.6244 |           4.6782 |
[32m[20230114 18:53:01 @agent_ppo2.py:189][0m |          -0.0065 |          11.2161 |           4.6756 |
[32m[20230114 18:53:01 @agent_ppo2.py:189][0m |          -0.0077 |          11.0330 |           4.6778 |
[32m[20230114 18:53:01 @agent_ppo2.py:189][0m |          -0.0091 |          10.6115 |           4.6696 |
[32m[20230114 18:53:01 @agent_ppo2.py:134][0m Policy update time: 0.82 s
[32m[20230114 18:53:01 @agent_ppo2.py:142][0m Average TRAINING episode reward: 262.88
[32m[20230114 18:53:01 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 264.08
[32m[20230114 18:53:01 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 274.36
[32m[20230114 18:53:01 @agent_ppo2.py:147][0m Total time:      10.95 min
[32m[20230114 18:53:01 @agent_ppo2.py:149][0m 952320 total steps have happened
[32m[20230114 18:53:01 @agent_ppo2.py:125][0m #------------------------ Iteration 465 --------------------------#
[32m[20230114 18:53:01 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:53:02 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:53:02 @agent_ppo2.py:189][0m |          -0.0014 |          15.2493 |           4.6432 |
[32m[20230114 18:53:02 @agent_ppo2.py:189][0m |          -0.0046 |          12.9664 |           4.6292 |
[32m[20230114 18:53:02 @agent_ppo2.py:189][0m |          -0.0081 |          12.0813 |           4.6243 |
[32m[20230114 18:53:02 @agent_ppo2.py:189][0m |          -0.0074 |          11.4750 |           4.6239 |
[32m[20230114 18:53:02 @agent_ppo2.py:189][0m |          -0.0081 |          11.0961 |           4.6243 |
[32m[20230114 18:53:02 @agent_ppo2.py:189][0m |          -0.0074 |          10.7319 |           4.6157 |
[32m[20230114 18:53:02 @agent_ppo2.py:189][0m |          -0.0103 |          10.5113 |           4.6159 |
[32m[20230114 18:53:02 @agent_ppo2.py:189][0m |          -0.0119 |          10.2243 |           4.6169 |
[32m[20230114 18:53:02 @agent_ppo2.py:189][0m |          -0.0123 |           9.9779 |           4.6144 |
[32m[20230114 18:53:02 @agent_ppo2.py:189][0m |          -0.0163 |           9.8508 |           4.6158 |
[32m[20230114 18:53:02 @agent_ppo2.py:134][0m Policy update time: 0.82 s
[32m[20230114 18:53:02 @agent_ppo2.py:142][0m Average TRAINING episode reward: 260.79
[32m[20230114 18:53:02 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 263.31
[32m[20230114 18:53:02 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 271.25
[32m[20230114 18:53:02 @agent_ppo2.py:147][0m Total time:      10.97 min
[32m[20230114 18:53:02 @agent_ppo2.py:149][0m 954368 total steps have happened
[32m[20230114 18:53:02 @agent_ppo2.py:125][0m #------------------------ Iteration 466 --------------------------#
[32m[20230114 18:53:03 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:53:03 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:53:03 @agent_ppo2.py:189][0m |          -0.0001 |          15.8905 |           4.6690 |
[32m[20230114 18:53:03 @agent_ppo2.py:189][0m |          -0.0050 |          14.8324 |           4.6612 |
[32m[20230114 18:53:03 @agent_ppo2.py:189][0m |          -0.0063 |          14.3902 |           4.6570 |
[32m[20230114 18:53:03 @agent_ppo2.py:189][0m |          -0.0080 |          14.0714 |           4.6578 |
[32m[20230114 18:53:03 @agent_ppo2.py:189][0m |          -0.0089 |          13.8369 |           4.6614 |
[32m[20230114 18:53:03 @agent_ppo2.py:189][0m |          -0.0087 |          13.5864 |           4.6541 |
[32m[20230114 18:53:03 @agent_ppo2.py:189][0m |          -0.0101 |          13.3978 |           4.6583 |
[32m[20230114 18:53:03 @agent_ppo2.py:189][0m |          -0.0112 |          13.2446 |           4.6570 |
[32m[20230114 18:53:03 @agent_ppo2.py:189][0m |          -0.0112 |          13.0363 |           4.6520 |
[32m[20230114 18:53:04 @agent_ppo2.py:189][0m |          -0.0126 |          12.9192 |           4.6589 |
[32m[20230114 18:53:04 @agent_ppo2.py:134][0m Policy update time: 0.81 s
[32m[20230114 18:53:04 @agent_ppo2.py:142][0m Average TRAINING episode reward: 261.88
[32m[20230114 18:53:04 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 265.49
[32m[20230114 18:53:04 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 272.35
[32m[20230114 18:53:04 @agent_ppo2.py:147][0m Total time:      10.99 min
[32m[20230114 18:53:04 @agent_ppo2.py:149][0m 956416 total steps have happened
[32m[20230114 18:53:04 @agent_ppo2.py:125][0m #------------------------ Iteration 467 --------------------------#
[32m[20230114 18:53:04 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:53:04 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:53:04 @agent_ppo2.py:189][0m |          -0.0006 |          38.0386 |           4.6258 |
[32m[20230114 18:53:04 @agent_ppo2.py:189][0m |          -0.0053 |          33.9061 |           4.6110 |
[32m[20230114 18:53:04 @agent_ppo2.py:189][0m |          -0.0061 |          31.7135 |           4.6057 |
[32m[20230114 18:53:04 @agent_ppo2.py:189][0m |          -0.0083 |          30.3241 |           4.6062 |
[32m[20230114 18:53:04 @agent_ppo2.py:189][0m |          -0.0095 |          28.9423 |           4.6045 |
[32m[20230114 18:53:05 @agent_ppo2.py:189][0m |          -0.0083 |          28.3480 |           4.6042 |
[32m[20230114 18:53:05 @agent_ppo2.py:189][0m |          -0.0097 |          28.3172 |           4.6046 |
[32m[20230114 18:53:05 @agent_ppo2.py:189][0m |          -0.0097 |          27.1392 |           4.6016 |
[32m[20230114 18:53:05 @agent_ppo2.py:189][0m |          -0.0110 |          27.1699 |           4.6022 |
[32m[20230114 18:53:05 @agent_ppo2.py:189][0m |          -0.0111 |          26.0824 |           4.6040 |
[32m[20230114 18:53:05 @agent_ppo2.py:134][0m Policy update time: 0.82 s
[32m[20230114 18:53:05 @agent_ppo2.py:142][0m Average TRAINING episode reward: 191.73
[32m[20230114 18:53:05 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 261.60
[32m[20230114 18:53:05 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 274.50
[32m[20230114 18:53:05 @agent_ppo2.py:147][0m Total time:      11.01 min
[32m[20230114 18:53:05 @agent_ppo2.py:149][0m 958464 total steps have happened
[32m[20230114 18:53:05 @agent_ppo2.py:125][0m #------------------------ Iteration 468 --------------------------#
[32m[20230114 18:53:05 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:53:05 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:53:05 @agent_ppo2.py:189][0m |           0.0016 |          16.9284 |           4.6623 |
[32m[20230114 18:53:05 @agent_ppo2.py:189][0m |          -0.0045 |          14.6318 |           4.6576 |
[32m[20230114 18:53:06 @agent_ppo2.py:189][0m |          -0.0052 |          14.0521 |           4.6599 |
[32m[20230114 18:53:06 @agent_ppo2.py:189][0m |          -0.0039 |          13.9868 |           4.6601 |
[32m[20230114 18:53:06 @agent_ppo2.py:189][0m |          -0.0066 |          13.1860 |           4.6590 |
[32m[20230114 18:53:06 @agent_ppo2.py:189][0m |          -0.0069 |          13.2596 |           4.6606 |
[32m[20230114 18:53:06 @agent_ppo2.py:189][0m |          -0.0098 |          12.5909 |           4.6641 |
[32m[20230114 18:53:06 @agent_ppo2.py:189][0m |          -0.0109 |          12.3797 |           4.6591 |
[32m[20230114 18:53:06 @agent_ppo2.py:189][0m |          -0.0087 |          12.2723 |           4.6646 |
[32m[20230114 18:53:06 @agent_ppo2.py:189][0m |          -0.0083 |          12.2816 |           4.6618 |
[32m[20230114 18:53:06 @agent_ppo2.py:134][0m Policy update time: 0.83 s
[32m[20230114 18:53:06 @agent_ppo2.py:142][0m Average TRAINING episode reward: 259.71
[32m[20230114 18:53:06 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 262.65
[32m[20230114 18:53:06 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 273.46
[32m[20230114 18:53:06 @agent_ppo2.py:147][0m Total time:      11.04 min
[32m[20230114 18:53:06 @agent_ppo2.py:149][0m 960512 total steps have happened
[32m[20230114 18:53:06 @agent_ppo2.py:125][0m #------------------------ Iteration 469 --------------------------#
[32m[20230114 18:53:07 @agent_ppo2.py:131][0m Sampling time: 0.30 s by 4 slaves
[32m[20230114 18:53:07 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:53:07 @agent_ppo2.py:189][0m |          -0.0012 |          30.7443 |           4.7396 |
[32m[20230114 18:53:07 @agent_ppo2.py:189][0m |          -0.0040 |          25.0412 |           4.7362 |
[32m[20230114 18:53:07 @agent_ppo2.py:189][0m |          -0.0082 |          22.3114 |           4.7291 |
[32m[20230114 18:53:07 @agent_ppo2.py:189][0m |          -0.0092 |          21.4874 |           4.7299 |
[32m[20230114 18:53:07 @agent_ppo2.py:189][0m |          -0.0118 |          20.9346 |           4.7296 |
[32m[20230114 18:53:07 @agent_ppo2.py:189][0m |          -0.0102 |          20.3527 |           4.7334 |
[32m[20230114 18:53:07 @agent_ppo2.py:189][0m |          -0.0097 |          19.9414 |           4.7326 |
[32m[20230114 18:53:07 @agent_ppo2.py:189][0m |          -0.0125 |          19.4541 |           4.7317 |
[32m[20230114 18:53:07 @agent_ppo2.py:189][0m |          -0.0127 |          19.2504 |           4.7328 |
[32m[20230114 18:53:07 @agent_ppo2.py:189][0m |          -0.0102 |          19.1974 |           4.7285 |
[32m[20230114 18:53:07 @agent_ppo2.py:134][0m Policy update time: 0.87 s
[32m[20230114 18:53:08 @agent_ppo2.py:142][0m Average TRAINING episode reward: 209.33
[32m[20230114 18:53:08 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 266.00
[32m[20230114 18:53:08 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 272.14
[32m[20230114 18:53:08 @agent_ppo2.py:147][0m Total time:      11.06 min
[32m[20230114 18:53:08 @agent_ppo2.py:149][0m 962560 total steps have happened
[32m[20230114 18:53:08 @agent_ppo2.py:125][0m #------------------------ Iteration 470 --------------------------#
[32m[20230114 18:53:08 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:53:08 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:53:08 @agent_ppo2.py:189][0m |          -0.0015 |          14.4541 |           4.8242 |
[32m[20230114 18:53:08 @agent_ppo2.py:189][0m |          -0.0058 |          12.7787 |           4.8110 |
[32m[20230114 18:53:08 @agent_ppo2.py:189][0m |          -0.0049 |          12.3147 |           4.8113 |
[32m[20230114 18:53:08 @agent_ppo2.py:189][0m |           0.0096 |          14.4672 |           4.8079 |
[32m[20230114 18:53:08 @agent_ppo2.py:189][0m |          -0.0116 |          11.7804 |           4.8087 |
[32m[20230114 18:53:08 @agent_ppo2.py:189][0m |          -0.0122 |          11.6215 |           4.8086 |
[32m[20230114 18:53:08 @agent_ppo2.py:189][0m |          -0.0122 |          11.4998 |           4.8025 |
[32m[20230114 18:53:09 @agent_ppo2.py:189][0m |          -0.0126 |          11.4119 |           4.8106 |
[32m[20230114 18:53:09 @agent_ppo2.py:189][0m |          -0.0168 |          11.2811 |           4.8037 |
[32m[20230114 18:53:09 @agent_ppo2.py:189][0m |          -0.0162 |          11.1908 |           4.8096 |
[32m[20230114 18:53:09 @agent_ppo2.py:134][0m Policy update time: 0.86 s
[32m[20230114 18:53:09 @agent_ppo2.py:142][0m Average TRAINING episode reward: 259.53
[32m[20230114 18:53:09 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 261.21
[32m[20230114 18:53:09 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 270.56
[32m[20230114 18:53:09 @agent_ppo2.py:147][0m Total time:      11.08 min
[32m[20230114 18:53:09 @agent_ppo2.py:149][0m 964608 total steps have happened
[32m[20230114 18:53:09 @agent_ppo2.py:125][0m #------------------------ Iteration 471 --------------------------#
[32m[20230114 18:53:09 @agent_ppo2.py:131][0m Sampling time: 0.27 s by 4 slaves
[32m[20230114 18:53:09 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:53:09 @agent_ppo2.py:189][0m |           0.0026 |          27.9434 |           4.6455 |
[32m[20230114 18:53:09 @agent_ppo2.py:189][0m |          -0.0045 |          18.8930 |           4.6484 |
[32m[20230114 18:53:10 @agent_ppo2.py:189][0m |          -0.0067 |          16.5442 |           4.6448 |
[32m[20230114 18:53:10 @agent_ppo2.py:189][0m |          -0.0095 |          15.4397 |           4.6459 |
[32m[20230114 18:53:10 @agent_ppo2.py:189][0m |          -0.0096 |          14.9322 |           4.6431 |
[32m[20230114 18:53:10 @agent_ppo2.py:189][0m |          -0.0112 |          14.3829 |           4.6471 |
[32m[20230114 18:53:10 @agent_ppo2.py:189][0m |          -0.0127 |          13.9163 |           4.6430 |
[32m[20230114 18:53:10 @agent_ppo2.py:189][0m |          -0.0131 |          13.6205 |           4.6432 |
[32m[20230114 18:53:10 @agent_ppo2.py:189][0m |          -0.0138 |          13.3620 |           4.6461 |
[32m[20230114 18:53:10 @agent_ppo2.py:189][0m |          -0.0140 |          12.9628 |           4.6420 |
[32m[20230114 18:53:10 @agent_ppo2.py:134][0m Policy update time: 0.86 s
[32m[20230114 18:53:10 @agent_ppo2.py:142][0m Average TRAINING episode reward: 196.93
[32m[20230114 18:53:10 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 265.45
[32m[20230114 18:53:10 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 271.50
[32m[20230114 18:53:10 @agent_ppo2.py:147][0m Total time:      11.10 min
[32m[20230114 18:53:10 @agent_ppo2.py:149][0m 966656 total steps have happened
[32m[20230114 18:53:10 @agent_ppo2.py:125][0m #------------------------ Iteration 472 --------------------------#
[32m[20230114 18:53:11 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:53:11 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:53:11 @agent_ppo2.py:189][0m |          -0.0006 |          15.8003 |           4.7796 |
[32m[20230114 18:53:11 @agent_ppo2.py:189][0m |          -0.0056 |          14.3016 |           4.7599 |
[32m[20230114 18:53:11 @agent_ppo2.py:189][0m |          -0.0069 |          13.2538 |           4.7633 |
[32m[20230114 18:53:11 @agent_ppo2.py:189][0m |          -0.0079 |          12.5861 |           4.7609 |
[32m[20230114 18:53:11 @agent_ppo2.py:189][0m |          -0.0085 |          12.0194 |           4.7638 |
[32m[20230114 18:53:11 @agent_ppo2.py:189][0m |          -0.0078 |          11.6858 |           4.7613 |
[32m[20230114 18:53:11 @agent_ppo2.py:189][0m |          -0.0117 |          11.2237 |           4.7603 |
[32m[20230114 18:53:11 @agent_ppo2.py:189][0m |          -0.0107 |          11.0894 |           4.7586 |
[32m[20230114 18:53:11 @agent_ppo2.py:189][0m |          -0.0123 |          10.8179 |           4.7636 |
[32m[20230114 18:53:11 @agent_ppo2.py:189][0m |          -0.0130 |          10.6742 |           4.7672 |
[32m[20230114 18:53:11 @agent_ppo2.py:134][0m Policy update time: 0.78 s
[32m[20230114 18:53:12 @agent_ppo2.py:142][0m Average TRAINING episode reward: 262.51
[32m[20230114 18:53:12 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 266.02
[32m[20230114 18:53:12 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 268.71
[32m[20230114 18:53:12 @agent_ppo2.py:147][0m Total time:      11.12 min
[32m[20230114 18:53:12 @agent_ppo2.py:149][0m 968704 total steps have happened
[32m[20230114 18:53:12 @agent_ppo2.py:125][0m #------------------------ Iteration 473 --------------------------#
[32m[20230114 18:53:12 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:53:12 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:53:12 @agent_ppo2.py:189][0m |           0.0012 |          14.3687 |           4.7222 |
[32m[20230114 18:53:12 @agent_ppo2.py:189][0m |          -0.0022 |          13.2642 |           4.7178 |
[32m[20230114 18:53:12 @agent_ppo2.py:189][0m |          -0.0070 |          12.8493 |           4.7200 |
[32m[20230114 18:53:12 @agent_ppo2.py:189][0m |          -0.0077 |          12.5280 |           4.7177 |
[32m[20230114 18:53:12 @agent_ppo2.py:189][0m |          -0.0092 |          12.3057 |           4.7170 |
[32m[20230114 18:53:12 @agent_ppo2.py:189][0m |          -0.0097 |          12.0625 |           4.7199 |
[32m[20230114 18:53:12 @agent_ppo2.py:189][0m |          -0.0125 |          11.7600 |           4.7171 |
[32m[20230114 18:53:12 @agent_ppo2.py:189][0m |          -0.0115 |          11.4962 |           4.7219 |
[32m[20230114 18:53:12 @agent_ppo2.py:189][0m |          -0.0121 |          11.3008 |           4.7208 |
[32m[20230114 18:53:13 @agent_ppo2.py:189][0m |          -0.0133 |          11.0201 |           4.7228 |
[32m[20230114 18:53:13 @agent_ppo2.py:134][0m Policy update time: 0.79 s
[32m[20230114 18:53:13 @agent_ppo2.py:142][0m Average TRAINING episode reward: 262.28
[32m[20230114 18:53:13 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 263.20
[32m[20230114 18:53:13 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 137.53
[32m[20230114 18:53:13 @agent_ppo2.py:147][0m Total time:      11.14 min
[32m[20230114 18:53:13 @agent_ppo2.py:149][0m 970752 total steps have happened
[32m[20230114 18:53:13 @agent_ppo2.py:125][0m #------------------------ Iteration 474 --------------------------#
[32m[20230114 18:53:13 @agent_ppo2.py:131][0m Sampling time: 0.26 s by 4 slaves
[32m[20230114 18:53:13 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:53:13 @agent_ppo2.py:189][0m |           0.0042 |          14.6089 |           4.8384 |
[32m[20230114 18:53:13 @agent_ppo2.py:189][0m |          -0.0050 |          13.2243 |           4.8336 |
[32m[20230114 18:53:13 @agent_ppo2.py:189][0m |          -0.0058 |          12.7340 |           4.8343 |
[32m[20230114 18:53:13 @agent_ppo2.py:189][0m |          -0.0066 |          12.4367 |           4.8353 |
[32m[20230114 18:53:14 @agent_ppo2.py:189][0m |          -0.0079 |          12.1922 |           4.8378 |
[32m[20230114 18:53:14 @agent_ppo2.py:189][0m |          -0.0095 |          11.9695 |           4.8300 |
[32m[20230114 18:53:14 @agent_ppo2.py:189][0m |          -0.0104 |          11.7371 |           4.8367 |
[32m[20230114 18:53:14 @agent_ppo2.py:189][0m |          -0.0109 |          11.5667 |           4.8387 |
[32m[20230114 18:53:14 @agent_ppo2.py:189][0m |          -0.0106 |          11.4859 |           4.8443 |
[32m[20230114 18:53:14 @agent_ppo2.py:189][0m |          -0.0106 |          11.3525 |           4.8406 |
[32m[20230114 18:53:14 @agent_ppo2.py:134][0m Policy update time: 0.84 s
[32m[20230114 18:53:14 @agent_ppo2.py:142][0m Average TRAINING episode reward: 260.67
[32m[20230114 18:53:14 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 264.80
[32m[20230114 18:53:14 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 156.15
[32m[20230114 18:53:14 @agent_ppo2.py:147][0m Total time:      11.17 min
[32m[20230114 18:53:14 @agent_ppo2.py:149][0m 972800 total steps have happened
[32m[20230114 18:53:14 @agent_ppo2.py:125][0m #------------------------ Iteration 475 --------------------------#
[32m[20230114 18:53:14 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:53:14 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:53:14 @agent_ppo2.py:189][0m |           0.0005 |          15.2175 |           4.7885 |
[32m[20230114 18:53:15 @agent_ppo2.py:189][0m |          -0.0047 |          13.5370 |           4.7834 |
[32m[20230114 18:53:15 @agent_ppo2.py:189][0m |          -0.0080 |          12.7945 |           4.7776 |
[32m[20230114 18:53:15 @agent_ppo2.py:189][0m |          -0.0078 |          12.2714 |           4.7752 |
[32m[20230114 18:53:15 @agent_ppo2.py:189][0m |          -0.0098 |          11.8277 |           4.7774 |
[32m[20230114 18:53:15 @agent_ppo2.py:189][0m |          -0.0104 |          11.3584 |           4.7757 |
[32m[20230114 18:53:15 @agent_ppo2.py:189][0m |          -0.0071 |          11.0565 |           4.7762 |
[32m[20230114 18:53:15 @agent_ppo2.py:189][0m |          -0.0086 |          10.7606 |           4.7710 |
[32m[20230114 18:53:15 @agent_ppo2.py:189][0m |          -0.0113 |           9.9673 |           4.7724 |
[32m[20230114 18:53:15 @agent_ppo2.py:189][0m |          -0.0146 |           9.5759 |           4.7722 |
[32m[20230114 18:53:15 @agent_ppo2.py:134][0m Policy update time: 0.80 s
[32m[20230114 18:53:15 @agent_ppo2.py:142][0m Average TRAINING episode reward: 262.65
[32m[20230114 18:53:15 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 265.44
[32m[20230114 18:53:15 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 272.81
[32m[20230114 18:53:15 @agent_ppo2.py:147][0m Total time:      11.19 min
[32m[20230114 18:53:15 @agent_ppo2.py:149][0m 974848 total steps have happened
[32m[20230114 18:53:15 @agent_ppo2.py:125][0m #------------------------ Iteration 476 --------------------------#
[32m[20230114 18:53:16 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:53:16 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:53:16 @agent_ppo2.py:189][0m |           0.0005 |          22.7165 |           4.8146 |
[32m[20230114 18:53:16 @agent_ppo2.py:189][0m |          -0.0059 |          17.5478 |           4.8064 |
[32m[20230114 18:53:16 @agent_ppo2.py:189][0m |          -0.0082 |          16.9708 |           4.8035 |
[32m[20230114 18:53:16 @agent_ppo2.py:189][0m |          -0.0088 |          16.4473 |           4.8053 |
[32m[20230114 18:53:16 @agent_ppo2.py:189][0m |          -0.0107 |          15.9939 |           4.8048 |
[32m[20230114 18:53:16 @agent_ppo2.py:189][0m |          -0.0104 |          15.9538 |           4.8055 |
[32m[20230114 18:53:16 @agent_ppo2.py:189][0m |          -0.0139 |          15.5301 |           4.7996 |
[32m[20230114 18:53:16 @agent_ppo2.py:189][0m |          -0.0130 |          15.4388 |           4.7970 |
[32m[20230114 18:53:16 @agent_ppo2.py:189][0m |          -0.0151 |          15.1133 |           4.7988 |
[32m[20230114 18:53:16 @agent_ppo2.py:189][0m |          -0.0161 |          15.0323 |           4.7957 |
[32m[20230114 18:53:16 @agent_ppo2.py:134][0m Policy update time: 0.79 s
[32m[20230114 18:53:17 @agent_ppo2.py:142][0m Average TRAINING episode reward: 226.31
[32m[20230114 18:53:17 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 259.81
[32m[20230114 18:53:17 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 273.35
[32m[20230114 18:53:17 @agent_ppo2.py:147][0m Total time:      11.21 min
[32m[20230114 18:53:17 @agent_ppo2.py:149][0m 976896 total steps have happened
[32m[20230114 18:53:17 @agent_ppo2.py:125][0m #------------------------ Iteration 477 --------------------------#
[32m[20230114 18:53:17 @agent_ppo2.py:131][0m Sampling time: 0.25 s by 4 slaves
[32m[20230114 18:53:17 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:53:17 @agent_ppo2.py:189][0m |          -0.0007 |          15.2435 |           4.7557 |
[32m[20230114 18:53:17 @agent_ppo2.py:189][0m |          -0.0038 |          14.6452 |           4.7542 |
[32m[20230114 18:53:17 @agent_ppo2.py:189][0m |          -0.0067 |          14.2494 |           4.7533 |
[32m[20230114 18:53:17 @agent_ppo2.py:189][0m |          -0.0088 |          14.0089 |           4.7477 |
[32m[20230114 18:53:17 @agent_ppo2.py:189][0m |          -0.0074 |          13.8311 |           4.7487 |
[32m[20230114 18:53:17 @agent_ppo2.py:189][0m |          -0.0097 |          13.6440 |           4.7506 |
[32m[20230114 18:53:17 @agent_ppo2.py:189][0m |          -0.0093 |          13.5349 |           4.7484 |
[32m[20230114 18:53:17 @agent_ppo2.py:189][0m |          -0.0080 |          13.5686 |           4.7477 |
[32m[20230114 18:53:18 @agent_ppo2.py:189][0m |          -0.0111 |          13.2006 |           4.7498 |
[32m[20230114 18:53:18 @agent_ppo2.py:189][0m |          -0.0090 |          13.0053 |           4.7413 |
[32m[20230114 18:53:18 @agent_ppo2.py:134][0m Policy update time: 0.80 s
[32m[20230114 18:53:18 @agent_ppo2.py:142][0m Average TRAINING episode reward: 259.83
[32m[20230114 18:53:18 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 263.09
[32m[20230114 18:53:18 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 94.64
[32m[20230114 18:53:18 @agent_ppo2.py:147][0m Total time:      11.23 min
[32m[20230114 18:53:18 @agent_ppo2.py:149][0m 978944 total steps have happened
[32m[20230114 18:53:18 @agent_ppo2.py:125][0m #------------------------ Iteration 478 --------------------------#
[32m[20230114 18:53:18 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:53:18 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:53:18 @agent_ppo2.py:189][0m |           0.0012 |          18.1275 |           4.8029 |
[32m[20230114 18:53:18 @agent_ppo2.py:189][0m |           0.0078 |          17.5299 |           4.8025 |
[32m[20230114 18:53:18 @agent_ppo2.py:189][0m |          -0.0102 |          14.7653 |           4.7889 |
[32m[20230114 18:53:18 @agent_ppo2.py:189][0m |          -0.0045 |          15.3179 |           4.7984 |
[32m[20230114 18:53:18 @agent_ppo2.py:189][0m |          -0.0114 |          13.7167 |           4.7939 |
[32m[20230114 18:53:19 @agent_ppo2.py:189][0m |          -0.0122 |          13.2980 |           4.8012 |
[32m[20230114 18:53:19 @agent_ppo2.py:189][0m |          -0.0143 |          12.9680 |           4.7971 |
[32m[20230114 18:53:19 @agent_ppo2.py:189][0m |          -0.0109 |          12.9068 |           4.8007 |
[32m[20230114 18:53:19 @agent_ppo2.py:189][0m |          -0.0132 |          12.5814 |           4.7996 |
[32m[20230114 18:53:19 @agent_ppo2.py:189][0m |          -0.0188 |          12.3479 |           4.7980 |
[32m[20230114 18:53:19 @agent_ppo2.py:134][0m Policy update time: 0.83 s
[32m[20230114 18:53:19 @agent_ppo2.py:142][0m Average TRAINING episode reward: 259.47
[32m[20230114 18:53:19 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 261.32
[32m[20230114 18:53:19 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 106.65
[32m[20230114 18:53:19 @agent_ppo2.py:147][0m Total time:      11.25 min
[32m[20230114 18:53:19 @agent_ppo2.py:149][0m 980992 total steps have happened
[32m[20230114 18:53:19 @agent_ppo2.py:125][0m #------------------------ Iteration 479 --------------------------#
[32m[20230114 18:53:19 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:53:19 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:53:19 @agent_ppo2.py:189][0m |           0.0002 |          19.6658 |           4.8514 |
[32m[20230114 18:53:20 @agent_ppo2.py:189][0m |          -0.0074 |          13.5023 |           4.8483 |
[32m[20230114 18:53:20 @agent_ppo2.py:189][0m |          -0.0094 |          12.2998 |           4.8477 |
[32m[20230114 18:53:20 @agent_ppo2.py:189][0m |          -0.0109 |          11.6391 |           4.8432 |
[32m[20230114 18:53:20 @agent_ppo2.py:189][0m |          -0.0119 |          11.4020 |           4.8499 |
[32m[20230114 18:53:20 @agent_ppo2.py:189][0m |          -0.0136 |          10.8121 |           4.8372 |
[32m[20230114 18:53:20 @agent_ppo2.py:189][0m |          -0.0133 |          10.5354 |           4.8425 |
[32m[20230114 18:53:20 @agent_ppo2.py:189][0m |          -0.0149 |          10.0985 |           4.8433 |
[32m[20230114 18:53:20 @agent_ppo2.py:189][0m |          -0.0168 |           9.7564 |           4.8434 |
[32m[20230114 18:53:20 @agent_ppo2.py:189][0m |          -0.0175 |           9.4218 |           4.8351 |
[32m[20230114 18:53:20 @agent_ppo2.py:134][0m Policy update time: 0.89 s
[32m[20230114 18:53:20 @agent_ppo2.py:142][0m Average TRAINING episode reward: 219.77
[32m[20230114 18:53:20 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 264.80
[32m[20230114 18:53:20 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 272.75
[32m[20230114 18:53:20 @agent_ppo2.py:147][0m Total time:      11.27 min
[32m[20230114 18:53:20 @agent_ppo2.py:149][0m 983040 total steps have happened
[32m[20230114 18:53:20 @agent_ppo2.py:125][0m #------------------------ Iteration 480 --------------------------#
[32m[20230114 18:53:21 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:53:21 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:53:21 @agent_ppo2.py:189][0m |           0.0037 |          13.7829 |           4.8423 |
[32m[20230114 18:53:21 @agent_ppo2.py:189][0m |          -0.0008 |          11.8867 |           4.8334 |
[32m[20230114 18:53:21 @agent_ppo2.py:189][0m |          -0.0049 |          10.8960 |           4.8329 |
[32m[20230114 18:53:21 @agent_ppo2.py:189][0m |          -0.0059 |          10.2992 |           4.8288 |
[32m[20230114 18:53:21 @agent_ppo2.py:189][0m |          -0.0075 |           9.8729 |           4.8299 |
[32m[20230114 18:53:21 @agent_ppo2.py:189][0m |          -0.0094 |           9.5540 |           4.8301 |
[32m[20230114 18:53:21 @agent_ppo2.py:189][0m |          -0.0098 |           9.1549 |           4.8262 |
[32m[20230114 18:53:21 @agent_ppo2.py:189][0m |          -0.0098 |           9.0128 |           4.8239 |
[32m[20230114 18:53:21 @agent_ppo2.py:189][0m |          -0.0111 |           8.7749 |           4.8192 |
[32m[20230114 18:53:21 @agent_ppo2.py:189][0m |          -0.0107 |           8.6065 |           4.8217 |
[32m[20230114 18:53:21 @agent_ppo2.py:134][0m Policy update time: 0.82 s
[32m[20230114 18:53:22 @agent_ppo2.py:142][0m Average TRAINING episode reward: 260.04
[32m[20230114 18:53:22 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 263.75
[32m[20230114 18:53:22 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 272.35
[32m[20230114 18:53:22 @agent_ppo2.py:147][0m Total time:      11.29 min
[32m[20230114 18:53:22 @agent_ppo2.py:149][0m 985088 total steps have happened
[32m[20230114 18:53:22 @agent_ppo2.py:125][0m #------------------------ Iteration 481 --------------------------#
[32m[20230114 18:53:22 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:53:22 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:53:22 @agent_ppo2.py:189][0m |           0.0006 |          15.7899 |           4.9006 |
[32m[20230114 18:53:22 @agent_ppo2.py:189][0m |          -0.0033 |          14.4166 |           4.8975 |
[32m[20230114 18:53:22 @agent_ppo2.py:189][0m |          -0.0050 |          14.0474 |           4.8939 |
[32m[20230114 18:53:22 @agent_ppo2.py:189][0m |          -0.0061 |          13.7314 |           4.8953 |
[32m[20230114 18:53:22 @agent_ppo2.py:189][0m |          -0.0083 |          13.5365 |           4.8923 |
[32m[20230114 18:53:22 @agent_ppo2.py:189][0m |          -0.0079 |          13.4764 |           4.8909 |
[32m[20230114 18:53:22 @agent_ppo2.py:189][0m |          -0.0098 |          13.2149 |           4.8915 |
[32m[20230114 18:53:23 @agent_ppo2.py:189][0m |          -0.0091 |          13.1068 |           4.8909 |
[32m[20230114 18:53:23 @agent_ppo2.py:189][0m |          -0.0110 |          13.0639 |           4.8885 |
[32m[20230114 18:53:23 @agent_ppo2.py:189][0m |          -0.0116 |          12.9116 |           4.8889 |
[32m[20230114 18:53:23 @agent_ppo2.py:134][0m Policy update time: 0.80 s
[32m[20230114 18:53:23 @agent_ppo2.py:142][0m Average TRAINING episode reward: 260.37
[32m[20230114 18:53:23 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 261.92
[32m[20230114 18:53:23 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 274.15
[32m[20230114 18:53:23 @agent_ppo2.py:147][0m Total time:      11.31 min
[32m[20230114 18:53:23 @agent_ppo2.py:149][0m 987136 total steps have happened
[32m[20230114 18:53:23 @agent_ppo2.py:125][0m #------------------------ Iteration 482 --------------------------#
[32m[20230114 18:53:23 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:53:23 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:53:23 @agent_ppo2.py:189][0m |          -0.0024 |          23.3551 |           4.7649 |
[32m[20230114 18:53:23 @agent_ppo2.py:189][0m |          -0.0109 |          19.0603 |           4.7598 |
[32m[20230114 18:53:23 @agent_ppo2.py:189][0m |          -0.0107 |          17.9737 |           4.7561 |
[32m[20230114 18:53:23 @agent_ppo2.py:189][0m |          -0.0016 |          17.8018 |           4.7569 |
[32m[20230114 18:53:24 @agent_ppo2.py:189][0m |           0.0073 |          18.7104 |           4.7524 |
[32m[20230114 18:53:24 @agent_ppo2.py:189][0m |          -0.0114 |          15.5872 |           4.7422 |
[32m[20230114 18:53:24 @agent_ppo2.py:189][0m |          -0.0147 |          15.0112 |           4.7459 |
[32m[20230114 18:53:24 @agent_ppo2.py:189][0m |          -0.0172 |          15.0544 |           4.7453 |
[32m[20230114 18:53:24 @agent_ppo2.py:189][0m |          -0.0119 |          15.0350 |           4.7435 |
[32m[20230114 18:53:24 @agent_ppo2.py:189][0m |          -0.0180 |          14.3788 |           4.7430 |
[32m[20230114 18:53:24 @agent_ppo2.py:134][0m Policy update time: 0.78 s
[32m[20230114 18:53:24 @agent_ppo2.py:142][0m Average TRAINING episode reward: 206.80
[32m[20230114 18:53:24 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 261.77
[32m[20230114 18:53:24 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 266.44
[32m[20230114 18:53:24 @agent_ppo2.py:147][0m Total time:      11.33 min
[32m[20230114 18:53:24 @agent_ppo2.py:149][0m 989184 total steps have happened
[32m[20230114 18:53:24 @agent_ppo2.py:125][0m #------------------------ Iteration 483 --------------------------#
[32m[20230114 18:53:24 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:53:24 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:53:24 @agent_ppo2.py:189][0m |          -0.0009 |          11.6542 |           4.9040 |
[32m[20230114 18:53:25 @agent_ppo2.py:189][0m |          -0.0057 |          10.3158 |           4.8959 |
[32m[20230114 18:53:25 @agent_ppo2.py:189][0m |          -0.0060 |           9.5852 |           4.8895 |
[32m[20230114 18:53:25 @agent_ppo2.py:189][0m |          -0.0068 |           9.2367 |           4.8898 |
[32m[20230114 18:53:25 @agent_ppo2.py:189][0m |          -0.0088 |           8.7093 |           4.8871 |
[32m[20230114 18:53:25 @agent_ppo2.py:189][0m |          -0.0108 |           8.2556 |           4.8855 |
[32m[20230114 18:53:25 @agent_ppo2.py:189][0m |          -0.0108 |           7.9835 |           4.8805 |
[32m[20230114 18:53:25 @agent_ppo2.py:189][0m |          -0.0101 |           7.7095 |           4.8809 |
[32m[20230114 18:53:25 @agent_ppo2.py:189][0m |          -0.0129 |           7.4138 |           4.8793 |
[32m[20230114 18:53:25 @agent_ppo2.py:189][0m |          -0.0126 |           7.2250 |           4.8795 |
[32m[20230114 18:53:25 @agent_ppo2.py:134][0m Policy update time: 0.80 s
[32m[20230114 18:53:25 @agent_ppo2.py:142][0m Average TRAINING episode reward: 259.48
[32m[20230114 18:53:25 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 262.58
[32m[20230114 18:53:25 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 273.88
[32m[20230114 18:53:25 @agent_ppo2.py:147][0m Total time:      11.35 min
[32m[20230114 18:53:25 @agent_ppo2.py:149][0m 991232 total steps have happened
[32m[20230114 18:53:25 @agent_ppo2.py:125][0m #------------------------ Iteration 484 --------------------------#
[32m[20230114 18:53:26 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:53:26 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:53:26 @agent_ppo2.py:189][0m |           0.0012 |          24.7552 |           4.8240 |
[32m[20230114 18:53:26 @agent_ppo2.py:189][0m |          -0.0029 |          15.7005 |           4.8245 |
[32m[20230114 18:53:26 @agent_ppo2.py:189][0m |          -0.0077 |          14.0506 |           4.8270 |
[32m[20230114 18:53:26 @agent_ppo2.py:189][0m |          -0.0080 |          13.1715 |           4.8285 |
[32m[20230114 18:53:26 @agent_ppo2.py:189][0m |          -0.0110 |          12.4390 |           4.8284 |
[32m[20230114 18:53:26 @agent_ppo2.py:189][0m |          -0.0126 |          12.0255 |           4.8296 |
[32m[20230114 18:53:26 @agent_ppo2.py:189][0m |          -0.0122 |          11.7545 |           4.8250 |
[32m[20230114 18:53:26 @agent_ppo2.py:189][0m |          -0.0141 |          11.5498 |           4.8308 |
[32m[20230114 18:53:26 @agent_ppo2.py:189][0m |          -0.0147 |          11.3456 |           4.8290 |
[32m[20230114 18:53:26 @agent_ppo2.py:189][0m |          -0.0147 |          11.2018 |           4.8263 |
[32m[20230114 18:53:26 @agent_ppo2.py:134][0m Policy update time: 0.78 s
[32m[20230114 18:53:27 @agent_ppo2.py:142][0m Average TRAINING episode reward: 229.63
[32m[20230114 18:53:27 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 263.75
[32m[20230114 18:53:27 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 265.82
[32m[20230114 18:53:27 @agent_ppo2.py:147][0m Total time:      11.37 min
[32m[20230114 18:53:27 @agent_ppo2.py:149][0m 993280 total steps have happened
[32m[20230114 18:53:27 @agent_ppo2.py:125][0m #------------------------ Iteration 485 --------------------------#
[32m[20230114 18:53:27 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:53:27 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:53:27 @agent_ppo2.py:189][0m |           0.0018 |          24.7067 |           4.8657 |
[32m[20230114 18:53:27 @agent_ppo2.py:189][0m |          -0.0025 |          18.9931 |           4.8641 |
[32m[20230114 18:53:27 @agent_ppo2.py:189][0m |          -0.0077 |          17.5481 |           4.8640 |
[32m[20230114 18:53:27 @agent_ppo2.py:189][0m |          -0.0091 |          16.6881 |           4.8556 |
[32m[20230114 18:53:27 @agent_ppo2.py:189][0m |          -0.0119 |          16.3645 |           4.8430 |
[32m[20230114 18:53:27 @agent_ppo2.py:189][0m |          -0.0118 |          15.7787 |           4.8493 |
[32m[20230114 18:53:27 @agent_ppo2.py:189][0m |          -0.0128 |          15.4259 |           4.8469 |
[32m[20230114 18:53:27 @agent_ppo2.py:189][0m |          -0.0134 |          15.2080 |           4.8487 |
[32m[20230114 18:53:27 @agent_ppo2.py:189][0m |          -0.0136 |          14.9448 |           4.8445 |
[32m[20230114 18:53:28 @agent_ppo2.py:189][0m |          -0.0141 |          14.6663 |           4.8390 |
[32m[20230114 18:53:28 @agent_ppo2.py:134][0m Policy update time: 0.74 s
[32m[20230114 18:53:28 @agent_ppo2.py:142][0m Average TRAINING episode reward: 217.23
[32m[20230114 18:53:28 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 263.42
[32m[20230114 18:53:28 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 270.73
[32m[20230114 18:53:28 @agent_ppo2.py:147][0m Total time:      11.39 min
[32m[20230114 18:53:28 @agent_ppo2.py:149][0m 995328 total steps have happened
[32m[20230114 18:53:28 @agent_ppo2.py:125][0m #------------------------ Iteration 486 --------------------------#
[32m[20230114 18:53:28 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:53:28 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:53:28 @agent_ppo2.py:189][0m |           0.0015 |          15.0386 |           4.7868 |
[32m[20230114 18:53:28 @agent_ppo2.py:189][0m |          -0.0054 |          14.0471 |           4.7767 |
[32m[20230114 18:53:28 @agent_ppo2.py:189][0m |          -0.0081 |          13.7295 |           4.7694 |
[32m[20230114 18:53:28 @agent_ppo2.py:189][0m |          -0.0100 |          13.4665 |           4.7759 |
[32m[20230114 18:53:28 @agent_ppo2.py:189][0m |          -0.0092 |          13.2643 |           4.7828 |
[32m[20230114 18:53:28 @agent_ppo2.py:189][0m |          -0.0105 |          13.1016 |           4.7788 |
[32m[20230114 18:53:29 @agent_ppo2.py:189][0m |          -0.0080 |          13.3149 |           4.7760 |
[32m[20230114 18:53:29 @agent_ppo2.py:189][0m |          -0.0082 |          12.9489 |           4.7834 |
[32m[20230114 18:53:29 @agent_ppo2.py:189][0m |          -0.0098 |          12.7594 |           4.7803 |
[32m[20230114 18:53:29 @agent_ppo2.py:189][0m |          -0.0133 |          12.4236 |           4.7859 |
[32m[20230114 18:53:29 @agent_ppo2.py:134][0m Policy update time: 0.80 s
[32m[20230114 18:53:29 @agent_ppo2.py:142][0m Average TRAINING episode reward: 260.83
[32m[20230114 18:53:29 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 261.60
[32m[20230114 18:53:29 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 272.37
[32m[20230114 18:53:29 @agent_ppo2.py:147][0m Total time:      11.41 min
[32m[20230114 18:53:29 @agent_ppo2.py:149][0m 997376 total steps have happened
[32m[20230114 18:53:29 @agent_ppo2.py:125][0m #------------------------ Iteration 487 --------------------------#
[32m[20230114 18:53:29 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:53:29 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:53:29 @agent_ppo2.py:189][0m |           0.0032 |          15.2921 |           4.7727 |
[32m[20230114 18:53:29 @agent_ppo2.py:189][0m |          -0.0004 |          13.7562 |           4.7666 |
[32m[20230114 18:53:29 @agent_ppo2.py:189][0m |          -0.0051 |          12.9111 |           4.7658 |
[32m[20230114 18:53:30 @agent_ppo2.py:189][0m |          -0.0057 |          12.5189 |           4.7639 |
[32m[20230114 18:53:30 @agent_ppo2.py:189][0m |          -0.0073 |          12.4235 |           4.7607 |
[32m[20230114 18:53:30 @agent_ppo2.py:189][0m |          -0.0072 |          12.0012 |           4.7592 |
[32m[20230114 18:53:30 @agent_ppo2.py:189][0m |          -0.0105 |          11.7746 |           4.7610 |
[32m[20230114 18:53:30 @agent_ppo2.py:189][0m |          -0.0115 |          11.5973 |           4.7564 |
[32m[20230114 18:53:30 @agent_ppo2.py:189][0m |          -0.0116 |          11.3660 |           4.7550 |
[32m[20230114 18:53:30 @agent_ppo2.py:189][0m |          -0.0109 |          11.2346 |           4.7554 |
[32m[20230114 18:53:30 @agent_ppo2.py:134][0m Policy update time: 0.80 s
[32m[20230114 18:53:30 @agent_ppo2.py:142][0m Average TRAINING episode reward: 258.96
[32m[20230114 18:53:30 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 261.71
[32m[20230114 18:53:30 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 272.53
[32m[20230114 18:53:30 @agent_ppo2.py:147][0m Total time:      11.43 min
[32m[20230114 18:53:30 @agent_ppo2.py:149][0m 999424 total steps have happened
[32m[20230114 18:53:30 @agent_ppo2.py:125][0m #------------------------ Iteration 488 --------------------------#
[32m[20230114 18:53:30 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:53:30 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:53:31 @agent_ppo2.py:189][0m |           0.0018 |          25.2233 |           4.7910 |
[32m[20230114 18:53:31 @agent_ppo2.py:189][0m |          -0.0022 |          19.1372 |           4.7849 |
[32m[20230114 18:53:31 @agent_ppo2.py:189][0m |           0.0047 |          16.6054 |           4.7851 |
[32m[20230114 18:53:31 @agent_ppo2.py:189][0m |          -0.0050 |          15.3881 |           4.7720 |
[32m[20230114 18:53:31 @agent_ppo2.py:189][0m |          -0.0101 |          14.8740 |           4.7838 |
[32m[20230114 18:53:31 @agent_ppo2.py:189][0m |          -0.0093 |          14.2691 |           4.7822 |
[32m[20230114 18:53:31 @agent_ppo2.py:189][0m |          -0.0066 |          13.7789 |           4.7786 |
[32m[20230114 18:53:31 @agent_ppo2.py:189][0m |          -0.0106 |          13.4449 |           4.7784 |
[32m[20230114 18:53:31 @agent_ppo2.py:189][0m |          -0.0087 |          13.5752 |           4.7790 |
[32m[20230114 18:53:31 @agent_ppo2.py:189][0m |          -0.0118 |          12.7668 |           4.7803 |
[32m[20230114 18:53:31 @agent_ppo2.py:134][0m Policy update time: 0.73 s
[32m[20230114 18:53:31 @agent_ppo2.py:142][0m Average TRAINING episode reward: 213.66
[32m[20230114 18:53:31 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 262.40
[32m[20230114 18:53:31 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 274.24
[32m[20230114 18:53:31 @agent_ppo2.py:147][0m Total time:      11.45 min
[32m[20230114 18:53:31 @agent_ppo2.py:149][0m 1001472 total steps have happened
[32m[20230114 18:53:31 @agent_ppo2.py:125][0m #------------------------ Iteration 489 --------------------------#
[32m[20230114 18:53:32 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:53:32 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:53:32 @agent_ppo2.py:189][0m |          -0.0007 |          36.1171 |           4.7916 |
[32m[20230114 18:53:32 @agent_ppo2.py:189][0m |          -0.0055 |          27.6824 |           4.7834 |
[32m[20230114 18:53:32 @agent_ppo2.py:189][0m |          -0.0033 |          25.8729 |           4.7824 |
[32m[20230114 18:53:32 @agent_ppo2.py:189][0m |          -0.0086 |          23.8919 |           4.7793 |
[32m[20230114 18:53:32 @agent_ppo2.py:189][0m |          -0.0098 |          23.1684 |           4.7739 |
[32m[20230114 18:53:32 @agent_ppo2.py:189][0m |          -0.0101 |          22.9992 |           4.7775 |
[32m[20230114 18:53:32 @agent_ppo2.py:189][0m |          -0.0100 |          22.3716 |           4.7754 |
[32m[20230114 18:53:32 @agent_ppo2.py:189][0m |          -0.0103 |          21.7961 |           4.7763 |
[32m[20230114 18:53:32 @agent_ppo2.py:189][0m |          -0.0116 |          21.5954 |           4.7753 |
[32m[20230114 18:53:32 @agent_ppo2.py:189][0m |          -0.0128 |          21.1064 |           4.7768 |
[32m[20230114 18:53:32 @agent_ppo2.py:134][0m Policy update time: 0.72 s
[32m[20230114 18:53:32 @agent_ppo2.py:142][0m Average TRAINING episode reward: 214.98
[32m[20230114 18:53:32 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 262.76
[32m[20230114 18:53:32 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 273.48
[32m[20230114 18:53:32 @agent_ppo2.py:147][0m Total time:      11.47 min
[32m[20230114 18:53:32 @agent_ppo2.py:149][0m 1003520 total steps have happened
[32m[20230114 18:53:32 @agent_ppo2.py:125][0m #------------------------ Iteration 490 --------------------------#
[32m[20230114 18:53:33 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:53:33 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:53:33 @agent_ppo2.py:189][0m |          -0.0000 |          16.6696 |           4.8837 |
[32m[20230114 18:53:33 @agent_ppo2.py:189][0m |          -0.0051 |          13.7670 |           4.8790 |
[32m[20230114 18:53:33 @agent_ppo2.py:189][0m |          -0.0076 |          13.0829 |           4.8800 |
[32m[20230114 18:53:33 @agent_ppo2.py:189][0m |          -0.0112 |          12.5963 |           4.8803 |
[32m[20230114 18:53:33 @agent_ppo2.py:189][0m |          -0.0107 |          12.3609 |           4.8824 |
[32m[20230114 18:53:33 @agent_ppo2.py:189][0m |          -0.0102 |          12.1298 |           4.8795 |
[32m[20230114 18:53:33 @agent_ppo2.py:189][0m |          -0.0098 |          12.0006 |           4.8802 |
[32m[20230114 18:53:33 @agent_ppo2.py:189][0m |          -0.0121 |          11.7670 |           4.8811 |
[32m[20230114 18:53:33 @agent_ppo2.py:189][0m |          -0.0125 |          11.6581 |           4.8783 |
[32m[20230114 18:53:33 @agent_ppo2.py:189][0m |          -0.0137 |          11.5845 |           4.8762 |
[32m[20230114 18:53:33 @agent_ppo2.py:134][0m Policy update time: 0.77 s
[32m[20230114 18:53:34 @agent_ppo2.py:142][0m Average TRAINING episode reward: 261.14
[32m[20230114 18:53:34 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 263.07
[32m[20230114 18:53:34 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 275.43
[32m[20230114 18:53:34 @agent_ppo2.py:147][0m Total time:      11.49 min
[32m[20230114 18:53:34 @agent_ppo2.py:149][0m 1005568 total steps have happened
[32m[20230114 18:53:34 @agent_ppo2.py:125][0m #------------------------ Iteration 491 --------------------------#
[32m[20230114 18:53:34 @agent_ppo2.py:131][0m Sampling time: 0.24 s by 4 slaves
[32m[20230114 18:53:34 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:53:34 @agent_ppo2.py:189][0m |           0.0017 |          13.7889 |           4.8767 |
[32m[20230114 18:53:34 @agent_ppo2.py:189][0m |          -0.0042 |          11.9321 |           4.8643 |
[32m[20230114 18:53:34 @agent_ppo2.py:189][0m |          -0.0042 |          11.1238 |           4.8698 |
[32m[20230114 18:53:34 @agent_ppo2.py:189][0m |          -0.0062 |          10.1788 |           4.8651 |
[32m[20230114 18:53:34 @agent_ppo2.py:189][0m |          -0.0072 |           9.4791 |           4.8619 |
[32m[20230114 18:53:34 @agent_ppo2.py:189][0m |          -0.0069 |           9.0930 |           4.8656 |
[32m[20230114 18:53:34 @agent_ppo2.py:189][0m |          -0.0097 |           8.4534 |           4.8656 |
[32m[20230114 18:53:35 @agent_ppo2.py:189][0m |          -0.0088 |           8.0068 |           4.8662 |
[32m[20230114 18:53:35 @agent_ppo2.py:189][0m |          -0.0106 |           7.6405 |           4.8694 |
[32m[20230114 18:53:35 @agent_ppo2.py:189][0m |          -0.0115 |           7.3048 |           4.8645 |
[32m[20230114 18:53:35 @agent_ppo2.py:134][0m Policy update time: 0.78 s
[32m[20230114 18:53:35 @agent_ppo2.py:142][0m Average TRAINING episode reward: 261.82
[32m[20230114 18:53:35 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 263.47
[32m[20230114 18:53:35 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 270.16
[32m[20230114 18:53:35 @agent_ppo2.py:147][0m Total time:      11.51 min
[32m[20230114 18:53:35 @agent_ppo2.py:149][0m 1007616 total steps have happened
[32m[20230114 18:53:35 @agent_ppo2.py:125][0m #------------------------ Iteration 492 --------------------------#
[32m[20230114 18:53:35 @agent_ppo2.py:131][0m Sampling time: 0.21 s by 4 slaves
[32m[20230114 18:53:35 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:53:35 @agent_ppo2.py:189][0m |           0.0081 |          15.1038 |           4.7606 |
[32m[20230114 18:53:35 @agent_ppo2.py:189][0m |          -0.0056 |          13.5982 |           4.7531 |
[32m[20230114 18:53:35 @agent_ppo2.py:189][0m |          -0.0042 |          12.9727 |           4.7510 |
[32m[20230114 18:53:35 @agent_ppo2.py:189][0m |          -0.0059 |          12.5763 |           4.7516 |
[32m[20230114 18:53:36 @agent_ppo2.py:189][0m |          -0.0065 |          12.0921 |           4.7562 |
[32m[20230114 18:53:36 @agent_ppo2.py:189][0m |          -0.0091 |          11.7641 |           4.7558 |
[32m[20230114 18:53:36 @agent_ppo2.py:189][0m |          -0.0134 |          11.5038 |           4.7578 |
[32m[20230114 18:53:36 @agent_ppo2.py:189][0m |          -0.0243 |          11.1739 |           4.7586 |
[32m[20230114 18:53:36 @agent_ppo2.py:189][0m |          -0.0147 |          10.9065 |           4.7569 |
[32m[20230114 18:53:36 @agent_ppo2.py:189][0m |          -0.0115 |          10.6015 |           4.7576 |
[32m[20230114 18:53:36 @agent_ppo2.py:134][0m Policy update time: 0.76 s
[32m[20230114 18:53:36 @agent_ppo2.py:142][0m Average TRAINING episode reward: 263.19
[32m[20230114 18:53:36 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 265.45
[32m[20230114 18:53:36 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 273.32
[32m[20230114 18:53:36 @agent_ppo2.py:147][0m Total time:      11.53 min
[32m[20230114 18:53:36 @agent_ppo2.py:149][0m 1009664 total steps have happened
[32m[20230114 18:53:36 @agent_ppo2.py:125][0m #------------------------ Iteration 493 --------------------------#
[32m[20230114 18:53:36 @agent_ppo2.py:131][0m Sampling time: 0.28 s by 4 slaves
[32m[20230114 18:53:36 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:53:37 @agent_ppo2.py:189][0m |           0.0030 |          26.8068 |           4.9170 |
[32m[20230114 18:53:37 @agent_ppo2.py:189][0m |          -0.0062 |          17.0677 |           4.9097 |
[32m[20230114 18:53:37 @agent_ppo2.py:189][0m |          -0.0102 |          15.2483 |           4.9101 |
[32m[20230114 18:53:37 @agent_ppo2.py:189][0m |          -0.0095 |          14.7426 |           4.9067 |
[32m[20230114 18:53:37 @agent_ppo2.py:189][0m |          -0.0112 |          14.4800 |           4.9062 |
[32m[20230114 18:53:37 @agent_ppo2.py:189][0m |          -0.0133 |          14.1472 |           4.9033 |
[32m[20230114 18:53:37 @agent_ppo2.py:189][0m |          -0.0140 |          14.1959 |           4.9040 |
[32m[20230114 18:53:37 @agent_ppo2.py:189][0m |          -0.0165 |          13.7657 |           4.9050 |
[32m[20230114 18:53:37 @agent_ppo2.py:189][0m |          -0.0150 |          13.6610 |           4.9059 |
[32m[20230114 18:53:37 @agent_ppo2.py:189][0m |          -0.0172 |          13.5589 |           4.9049 |
[32m[20230114 18:53:37 @agent_ppo2.py:134][0m Policy update time: 0.84 s
[32m[20230114 18:53:37 @agent_ppo2.py:142][0m Average TRAINING episode reward: 193.64
[32m[20230114 18:53:37 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 261.55
[32m[20230114 18:53:37 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 269.66
[32m[20230114 18:53:37 @agent_ppo2.py:147][0m Total time:      11.55 min
[32m[20230114 18:53:37 @agent_ppo2.py:149][0m 1011712 total steps have happened
[32m[20230114 18:53:37 @agent_ppo2.py:125][0m #------------------------ Iteration 494 --------------------------#
[32m[20230114 18:53:38 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:53:38 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:53:38 @agent_ppo2.py:189][0m |           0.0010 |          14.0697 |           4.9956 |
[32m[20230114 18:53:38 @agent_ppo2.py:189][0m |          -0.0050 |          11.2771 |           4.9873 |
[32m[20230114 18:53:38 @agent_ppo2.py:189][0m |          -0.0063 |          10.4759 |           4.9905 |
[32m[20230114 18:53:38 @agent_ppo2.py:189][0m |          -0.0060 |           9.8858 |           4.9867 |
[32m[20230114 18:53:38 @agent_ppo2.py:189][0m |          -0.0105 |           9.4352 |           4.9894 |
[32m[20230114 18:53:38 @agent_ppo2.py:189][0m |          -0.0106 |           8.9991 |           4.9893 |
[32m[20230114 18:53:38 @agent_ppo2.py:189][0m |          -0.0144 |           8.6297 |           4.9908 |
[32m[20230114 18:53:38 @agent_ppo2.py:189][0m |          -0.0165 |           8.3187 |           4.9958 |
[32m[20230114 18:53:38 @agent_ppo2.py:189][0m |          -0.0124 |           8.0451 |           4.9960 |
[32m[20230114 18:53:38 @agent_ppo2.py:189][0m |          -0.0135 |           7.8022 |           4.9955 |
[32m[20230114 18:53:38 @agent_ppo2.py:134][0m Policy update time: 0.80 s
[32m[20230114 18:53:39 @agent_ppo2.py:142][0m Average TRAINING episode reward: 258.85
[32m[20230114 18:53:39 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 260.14
[32m[20230114 18:53:39 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 273.14
[32m[20230114 18:53:39 @agent_ppo2.py:147][0m Total time:      11.57 min
[32m[20230114 18:53:39 @agent_ppo2.py:149][0m 1013760 total steps have happened
[32m[20230114 18:53:39 @agent_ppo2.py:125][0m #------------------------ Iteration 495 --------------------------#
[32m[20230114 18:53:39 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:53:39 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:53:39 @agent_ppo2.py:189][0m |           0.0011 |          11.9273 |           5.0476 |
[32m[20230114 18:53:39 @agent_ppo2.py:189][0m |          -0.0050 |          10.8849 |           5.0424 |
[32m[20230114 18:53:39 @agent_ppo2.py:189][0m |          -0.0075 |          10.3703 |           5.0437 |
[32m[20230114 18:53:39 @agent_ppo2.py:189][0m |          -0.0089 |          10.0020 |           5.0413 |
[32m[20230114 18:53:39 @agent_ppo2.py:189][0m |          -0.0105 |           9.7475 |           5.0424 |
[32m[20230114 18:53:39 @agent_ppo2.py:189][0m |          -0.0105 |           9.5784 |           5.0350 |
[32m[20230114 18:53:39 @agent_ppo2.py:189][0m |          -0.0103 |           9.4614 |           5.0374 |
[32m[20230114 18:53:39 @agent_ppo2.py:189][0m |          -0.0121 |           9.1909 |           5.0362 |
[32m[20230114 18:53:40 @agent_ppo2.py:189][0m |          -0.0134 |           8.9810 |           5.0326 |
[32m[20230114 18:53:40 @agent_ppo2.py:189][0m |          -0.0131 |           8.7627 |           5.0341 |
[32m[20230114 18:53:40 @agent_ppo2.py:134][0m Policy update time: 0.77 s
[32m[20230114 18:53:40 @agent_ppo2.py:142][0m Average TRAINING episode reward: 261.50
[32m[20230114 18:53:40 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 262.72
[32m[20230114 18:53:40 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 274.75
[32m[20230114 18:53:40 @agent_ppo2.py:147][0m Total time:      11.59 min
[32m[20230114 18:53:40 @agent_ppo2.py:149][0m 1015808 total steps have happened
[32m[20230114 18:53:40 @agent_ppo2.py:125][0m #------------------------ Iteration 496 --------------------------#
[32m[20230114 18:53:40 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:53:40 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:53:40 @agent_ppo2.py:189][0m |          -0.0017 |          13.9524 |           4.9902 |
[32m[20230114 18:53:40 @agent_ppo2.py:189][0m |          -0.0048 |          13.5969 |           4.9883 |
[32m[20230114 18:53:40 @agent_ppo2.py:189][0m |          -0.0083 |          13.0942 |           4.9832 |
[32m[20230114 18:53:40 @agent_ppo2.py:189][0m |          -0.0080 |          12.7460 |           4.9766 |
[32m[20230114 18:53:40 @agent_ppo2.py:189][0m |          -0.0078 |          12.5150 |           4.9765 |
[32m[20230114 18:53:41 @agent_ppo2.py:189][0m |          -0.0101 |          12.2226 |           4.9806 |
[32m[20230114 18:53:41 @agent_ppo2.py:189][0m |          -0.0120 |          11.7013 |           4.9796 |
[32m[20230114 18:53:41 @agent_ppo2.py:189][0m |          -0.0127 |          11.4244 |           4.9798 |
[32m[20230114 18:53:41 @agent_ppo2.py:189][0m |          -0.0122 |          11.3109 |           4.9775 |
[32m[20230114 18:53:41 @agent_ppo2.py:189][0m |          -0.0129 |          10.9913 |           4.9748 |
[32m[20230114 18:53:41 @agent_ppo2.py:134][0m Policy update time: 0.78 s
[32m[20230114 18:53:41 @agent_ppo2.py:142][0m Average TRAINING episode reward: 260.82
[32m[20230114 18:53:41 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 262.26
[32m[20230114 18:53:41 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 272.93
[32m[20230114 18:53:41 @agent_ppo2.py:147][0m Total time:      11.61 min
[32m[20230114 18:53:41 @agent_ppo2.py:149][0m 1017856 total steps have happened
[32m[20230114 18:53:41 @agent_ppo2.py:125][0m #------------------------ Iteration 497 --------------------------#
[32m[20230114 18:53:41 @agent_ppo2.py:131][0m Sampling time: 0.22 s by 4 slaves
[32m[20230114 18:53:41 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:53:41 @agent_ppo2.py:189][0m |           0.0010 |          17.3240 |           4.8803 |
[32m[20230114 18:53:41 @agent_ppo2.py:189][0m |          -0.0035 |          15.7190 |           4.8666 |
[32m[20230114 18:53:42 @agent_ppo2.py:189][0m |          -0.0068 |          15.1423 |           4.8732 |
[32m[20230114 18:53:42 @agent_ppo2.py:189][0m |          -0.0075 |          14.8024 |           4.8683 |
[32m[20230114 18:53:42 @agent_ppo2.py:189][0m |          -0.0080 |          14.4221 |           4.8624 |
[32m[20230114 18:53:42 @agent_ppo2.py:189][0m |          -0.0102 |          14.1595 |           4.8625 |
[32m[20230114 18:53:42 @agent_ppo2.py:189][0m |          -0.0091 |          14.0323 |           4.8587 |
[32m[20230114 18:53:42 @agent_ppo2.py:189][0m |          -0.0118 |          13.7217 |           4.8612 |
[32m[20230114 18:53:42 @agent_ppo2.py:189][0m |          -0.0124 |          13.4805 |           4.8617 |
[32m[20230114 18:53:42 @agent_ppo2.py:189][0m |          -0.0130 |          13.3345 |           4.8615 |
[32m[20230114 18:53:42 @agent_ppo2.py:134][0m Policy update time: 0.76 s
[32m[20230114 18:53:42 @agent_ppo2.py:142][0m Average TRAINING episode reward: 261.67
[32m[20230114 18:53:42 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 263.24
[32m[20230114 18:53:42 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 272.75
[32m[20230114 18:53:42 @agent_ppo2.py:147][0m Total time:      11.63 min
[32m[20230114 18:53:42 @agent_ppo2.py:149][0m 1019904 total steps have happened
[32m[20230114 18:53:42 @agent_ppo2.py:125][0m #------------------------ Iteration 498 --------------------------#
[32m[20230114 18:53:42 @agent_ppo2.py:131][0m Sampling time: 0.23 s by 4 slaves
[32m[20230114 18:53:43 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:53:43 @agent_ppo2.py:189][0m |          -0.0018 |          16.1594 |           4.9341 |
[32m[20230114 18:53:43 @agent_ppo2.py:189][0m |          -0.0065 |          14.1373 |           4.9346 |
[32m[20230114 18:53:43 @agent_ppo2.py:189][0m |          -0.0068 |          13.2910 |           4.9389 |
[32m[20230114 18:53:43 @agent_ppo2.py:189][0m |          -0.0127 |          12.8745 |           4.9388 |
[32m[20230114 18:53:43 @agent_ppo2.py:189][0m |          -0.0123 |          12.8135 |           4.9393 |
[32m[20230114 18:53:43 @agent_ppo2.py:189][0m |          -0.0083 |          12.7804 |           4.9407 |
[32m[20230114 18:53:43 @agent_ppo2.py:189][0m |          -0.0155 |          12.3250 |           4.9408 |
[32m[20230114 18:53:43 @agent_ppo2.py:189][0m |          -0.0149 |          12.1805 |           4.9430 |
[32m[20230114 18:53:43 @agent_ppo2.py:189][0m |          -0.0163 |          12.0180 |           4.9394 |
[32m[20230114 18:53:43 @agent_ppo2.py:189][0m |          -0.0149 |          11.9579 |           4.9420 |
[32m[20230114 18:53:43 @agent_ppo2.py:134][0m Policy update time: 0.77 s
[32m[20230114 18:53:43 @agent_ppo2.py:142][0m Average TRAINING episode reward: 260.56
[32m[20230114 18:53:43 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 261.94
[32m[20230114 18:53:43 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 139.97
[32m[20230114 18:53:43 @agent_ppo2.py:147][0m Total time:      11.65 min
[32m[20230114 18:53:43 @agent_ppo2.py:149][0m 1021952 total steps have happened
[32m[20230114 18:53:43 @agent_ppo2.py:125][0m #------------------------ Iteration 499 --------------------------#
[32m[20230114 18:53:44 @agent_ppo2.py:131][0m Sampling time: 0.33 s by 4 slaves
[32m[20230114 18:53:44 @agent_ppo2.py:165][0m |      policy_loss |       value_loss |          entropy |
[32m[20230114 18:53:44 @agent_ppo2.py:189][0m |          -0.0001 |          22.4521 |           5.0434 |
[32m[20230114 18:53:44 @agent_ppo2.py:189][0m |          -0.0076 |          19.8519 |           5.0280 |
[32m[20230114 18:53:44 @agent_ppo2.py:189][0m |          -0.0096 |          19.0671 |           5.0293 |
[32m[20230114 18:53:44 @agent_ppo2.py:189][0m |          -0.0106 |          18.7224 |           5.0310 |
[32m[20230114 18:53:44 @agent_ppo2.py:189][0m |          -0.0116 |          18.4601 |           5.0288 |
[32m[20230114 18:53:44 @agent_ppo2.py:189][0m |          -0.0129 |          17.9600 |           5.0275 |
[32m[20230114 18:53:44 @agent_ppo2.py:189][0m |          -0.0130 |          17.7405 |           5.0250 |
[32m[20230114 18:53:44 @agent_ppo2.py:189][0m |          -0.0135 |          17.5943 |           5.0301 |
[32m[20230114 18:53:44 @agent_ppo2.py:189][0m |          -0.0147 |          17.2134 |           5.0266 |
[32m[20230114 18:53:45 @agent_ppo2.py:189][0m |          -0.0148 |          16.9760 |           5.0247 |
[32m[20230114 18:53:45 @agent_ppo2.py:134][0m Policy update time: 0.85 s
[32m[20230114 18:53:45 @agent_ppo2.py:142][0m Average TRAINING episode reward: 211.17
[32m[20230114 18:53:45 @agent_ppo2.py:143][0m Maximum TRAINING episode reward: 263.39
[32m[20230114 18:53:45 @agent_ppo2.py:144][0m Average EVALUATION episode reward: 125.25
[32m[20230114 18:53:45 @agent_ppo2.py:104][0m [4m[34mCRITICAL[0m Saving the interval checkpoint with rewards 282.53
[32m[20230114 18:53:45 @agent_ppo2.py:147][0m Total time:      11.68 min
[32m[20230114 18:53:45 @agent_ppo2.py:149][0m 1024000 total steps have happened
[32m[20230114 18:53:45 @train.py:54][0m [4m[34mCRITICAL[0m Training completed!
